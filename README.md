# 2024-Arxiv-Paper-List-quadrupedal-robot
Arxiv 2024 Paper List about `quadrupedal robot`. <br>
Crawled with [Arxiv-Crawler-Py](https://github.com/mjlee111/Arxiv-Crawler-Py). 

## Disclaimer

This repository is based on papers collected through the public Arxiv API and includes papers related to the topic of `quadrupedal robot` published on Arxiv in 2024. The list is provided solely for research reference purposes. Copyrights for each paper belong to the respective authors and publishers, and the list complies with Arxiv’s terms of use.

Please refer to the [Arxiv](https://arxiv.org/) site directly to check for the latest updates, revisions, or original links for each paper.

## Paper List
| title | authors | published | summary | link |
| --- | --- | --- | --- | --- |
| Adaptive Length Image Tokenization via Recurrent Allocation | Shivam Duggal; Phillip Isola; Antonio Torralba; William T. Freeman | 2024-11-04 | <details><summary>Click to expand</summary>Current vision systems typically assign fixed-length representations to images, regardless of the information content. This contrasts with human intelligence - and even large language models - which allocate varying representational capacities based on entropy, context and familiarity. Inspired by this, we propose an approach to learn variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token count aligns with image entropy, familiarity and downstream task requirements. Recurrent token processing with increasing representational capacity in each iteration shows signs of token specialization, revealing potential for object / part discovery.</details> | http://arxiv.org/abs/2411.02393v1 |
| DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for   Efficient Robot Execution | Yang Yue; Yulin Wang; Bingyi Kang; Yizeng Han; Shenzhi Wang; Shiji Song; Jiashi Feng; Gao Huang | 2024-11-04 | <details><summary>Click to expand</summary>MLLMs have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.</details> | http://arxiv.org/abs/2411.02359v1 |
| Simulation of Nanorobots with Artificial Intelligence and Reinforcement   Learning for Advanced Cancer Cell Detection and Tracking | Shahab Kavousinejad | 2024-11-04 | <details><summary>Click to expand</summary>Nanorobots are a promising development in targeted drug delivery and the treatment of neurological disorders, with potential for crossing the blood-brain barrier (BBB). These small devices leverage advancements in nanotechnology and bioengineering for precise navigation and targeted payload delivery, particularly for conditions like brain tumors, Alzheimer's disease, and Parkinson's disease. Recent progress in artificial intelligence (AI) and machine learning (ML) has improved the navigation and effectiveness of nanorobots, allowing them to detect and interact with cancer cells through biomarker analysis. This study presents a new reinforcement learning (RL) framework for optimizing nanorobot navigation in complex biological environments, focusing on cancer cell detection by analyzing the concentration gradients of surrounding biomarkers. We utilize a computer simulation model to explore the behavior of nanorobots in a three-dimensional space with cancer cells and biological barriers. The proposed method uses Q-learning to refine movement strategies based on real-time biomarker concentration data, enabling nanorobots to autonomously navigate to cancerous tissues for targeted drug delivery. This research lays the groundwork for future laboratory experiments and clinical applications, with implications for personalized medicine and less invasive cancer treatments. The integration of intelligent nanorobots could revolutionize therapeutic strategies, reducing side effects and enhancing treatment effectiveness for cancer patients. Further research will investigate the practical deployment of these technologies in medical settings, aiming to unlock the full potential of nanorobotics in healthcare.</details> | http://arxiv.org/abs/2411.02345v1 |
| Microscale velocity-dependent unbinding generates a macroscale   performance-efficiency tradeoff in actomyosin systems | Jake McGrath; Brian Kent; Colin Johnson; José Alvarado | 2024-11-04 | <details><summary>Click to expand</summary>Myosin motors are fundamental biological actuators, powering diverse mechanical tasks in eukaryotic cells via ATP hydrolysis. Recent work revealed that myosin's velocity-dependent detachment rate can bridge actomyosin dynamics to macroscale Hill muscle predictions. However, the influence of this microscale unbinding, which we characterize by a dimensionless parameter $\alpha$, on macroscale energetic flows-such as power consumption, output and efficiency-remains elusive. Here we develop an analytical model of myosin dynamics that relates unbinding rates $\alpha$ to energetics. Our model agrees with published in-vivo muscle data and, furthermore, uncovers a performance-efficiency tradeoff governed by $\alpha$. To experimentally validate the tradeoff, we build HillBot, a robophysical model of Hill's muscle that mimics nonlinearity. Through HillBot, we decouple $\alpha$'s concurrent effect on performance and efficiency, demonstrating that nonlinearity drives efficiency. We compile 136 published measurements of $\alpha$ in muscle and myoblasts to reveal a distribution centered at $\alpha^* = 3.85 \pm 2.32$. Synthesizing data from our model and HillBot, we quantitatively show that $\alpha^*$ corresponds to a class of generalist actuators that are both relatively powerful and efficient, suggesting that the performance-efficiency tradeoff underpins the prevalence of $\alpha^*$ in nature. We leverage these insights and propose a nonlinear variable-impedance protocol to shift along a performance-efficiency axis in robotic applications.</details> | http://arxiv.org/abs/2411.02340v1 |
| Defining and Evaluating Physical Safety for Large Language Models | Yung-Chen Tang; Pin-Yu Chen; Tsung-Yi Ho | 2024-11-04 | <details><summary>Click to expand</summary>Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating LLM physical safety by developing a comprehensive benchmark for drone control. We classify the physical safety risks of drones into four categories: (1) human-targeted threats, (2) object-targeted threats, (3) infrastructure attacks, and (4) regulatory violations. Our evaluation of mainstream LLMs reveals an undesirable trade-off between utility and safety, with models that excel in code generation often performing poorly in crucial safety aspects. Furthermore, while incorporating advanced prompt engineering techniques such as In-Context Learning and Chain-of-Thought can improve safety, these methods still struggle to identify unintentional attacks. In addition, larger models demonstrate better safety capabilities, particularly in refusing dangerous commands. Our findings and benchmark can facilitate the design and evaluation of physical safety for LLMs. The project page is available at huggingface.co/spaces/TrustSafeAI/LLM-physical-safety.</details> | http://arxiv.org/abs/2411.02317v1 |
| Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation   With Fluidic Heating | Di Ni; Ved Gund; Landon Ivy; Amit Lal | 2024-11-04 | <details><summary>Click to expand</summary>Integrated micro power generators are crucial components for micro robotic platforms to demonstrate untethered operation and to achieve autonomy. Current micro robotic electrostatic actuators typically require hundreds to thousands of voltages to output sufficient work. Pyroelectricity is one such source of high voltages that can be scaled to small form factors. This paper demonstrates a distributed pyroelectric high voltage generation mechanism to power kV actuators using alternating exposure of crystals to hot and cold water (300C to 900C water temperature). Using this fluidic temperature control, a pyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage capacitor yielding a 6.10 {\mu}J stored energy. A maximum energy of 17.46 {\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can be used to heat a distributed array of converters to generate electricity in distant robotic actuator sections. The development of this distributed system would enable untethered micro-robot to be operated with a flexible body and free of battery recharging, which advances its applications in the real world.</details> | http://arxiv.org/abs/2411.02295v1 |
| AI Should Challenge, Not Obey | Advait Sarkar | 2024-11-04 | <details><summary>Click to expand</summary>Let's transform our robot secretaries into Socratic gadflies.</details> | http://arxiv.org/abs/2411.02263v1 |
| 3D Audio-Visual Segmentation | Artem Sokolov; Swapnil Bhosale; Xiatian Zhu | 2024-11-04 | <details><summary>Click to expand</summary>Recognizing the sounding objects in scenes is a longstanding objective in embodied AI, with diverse applications in robotics and AR/VR/MR. To that end, Audio-Visual Segmentation (AVS), taking as condition an audio signal to identify the masks of the target sounding objects in an input image with synchronous camera and microphone sensors, has been recently advanced. However, this paradigm is still insufficient for real-world operation, as the mapping from 2D images to 3D scenes is missing. To address this fundamental limitation, we introduce a novel research problem, 3D Audio-Visual Segmentation, extending the existing AVS to the 3D output space. This problem poses more challenges due to variations in camera extrinsics, audio scattering, occlusions, and diverse acoustics across sounding object categories. To facilitate this research, we create the very first simulation based benchmark, 3DAVS-S34-O7, providing photorealistic 3D scene environments with grounded spatial audio under single-instance and multi-instance settings, across 34 scenes and 7 object categories. This is made possible by re-purposing the Habitat simulator to generate comprehensive annotations of sounding object locations and corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet, characterized by integrating the ready-to-use knowledge from pretrained 2D audio-visual foundation models synergistically with 3D visual scene representation through spatial audio-aware mask alignment and refinement. Extensive experiments demonstrate that EchoSegnet can effectively segment sounding objects in 3D space on our new benchmark, representing a significant advancement in the field of embodied AI. Project page: https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/</details> | http://arxiv.org/abs/2411.02236v1 |
| Energy-Aware Coverage Planning for Heterogeneous Multi-Robot System | Aiman Munir; Ayan Dutta; Ramviyas Parasuraman | 2024-11-04 | <details><summary>Click to expand</summary>We propose a distributed control law for a heterogeneous multi-robot coverage problem, where the robots could have different energy characteristics, such as capacity and depletion rates, due to their varying sizes, speeds, capabilities, and payloads. Existing energy-aware coverage control laws consider capacity differences but assume the battery depletion rate to be the same for all robots. In realistic scenarios, however, some robots can consume energy much faster than other robots; for instance, UAVs hover at different altitudes, and these changes could be dynamically updated based on their assigned tasks. Robots' energy capacities and depletion rates need to be considered to maximize the performance of a multi-robot system. To this end, we propose a new energy-aware controller based on Lloyd's algorithm to adapt the weights of the robots based on their energy dynamics and divide the area of interest among the robots accordingly. The controller is theoretically analyzed and extensively evaluated through simulations and real-world demonstrations in multiple realistic scenarios and compared with three baseline control laws to validate its performance and efficacy.</details> | http://arxiv.org/abs/2411.02230v1 |
| DexHub and DART: Towards Internet Scale Robot Data Collection | Younghyo Park; Jagdeep Singh Bhatia; Lars Ankile; Pulkit Agrawal | 2024-11-04 | <details><summary>Click to expand</summary>The quest to build a generalist robotic system is impeded by the scarcity of diverse and high-quality data. While real-world data collection effort exist, requirements for robot hardware, physical environment setups, and frequent resets significantly impede the scalability needed for modern learning frameworks. We introduce DART, a teleoperation platform designed for crowdsourcing that reimagines robotic data collection by leveraging cloud-based simulation and augmented reality (AR) to address many limitations of prior data collection efforts. Our user studies highlight that DART enables higher data collection throughput and lower physical fatigue compared to real-world teleoperation. We also demonstrate that policies trained using DART-collected datasets successfully transfer to reality and are robust to unseen visual disturbances. All data collected through DART is automatically stored in our cloud-hosted database, DexHub, which will be made publicly available upon curation, paving the path for DexHub to become an ever-growing data hub for robot learning. Videos are available at: https://dexhub.ai/project</details> | http://arxiv.org/abs/2411.02214v1 |
| DiffSim2Real: Deploying Quadrupedal Locomotion Policies Purely Trained   in Differentiable Simulation | Joshua Bagajo; Clemens Schwarke; Victor Klemm; Ignat Georgiev; Jean-Pierre Sleiman; Jesus Tordesillas; Animesh Garg; Marco Hutter | 2024-11-04 | <details><summary>Click to expand</summary>Differentiable simulators provide analytic gradients, enabling more sample-efficient learning algorithms and paving the way for data intensive learning tasks such as learning from images. In this work, we demonstrate that locomotion policies trained with analytic gradients from a differentiable simulator can be successfully transferred to the real world. Typically, simulators that offer informative gradients lack the physical accuracy needed for sim-to-real transfer, and vice-versa. A key factor in our success is a smooth contact model that combines informative gradients with physical accuracy, ensuring effective transfer of learned behaviors. To the best of our knowledge, this is the first time a real quadrupedal robot is able to locomote after training exclusively in a differentiable simulation.</details> | http://arxiv.org/abs/2411.02189v1 |
| Touch-to-Touch Translation -- Learning the Mapping Between Heterogeneous   Tactile Sensing Technologies | Francesco Grella; Alessandro Albini; Giorgio Cannata; Perla Maiolino | 2024-11-04 | <details><summary>Click to expand</summary>The use of data-driven techniques for tactile data processing and classification has recently increased. However, collecting tactile data is a time-expensive and sensor-specific procedure. Indeed, due to the lack of hardware standards in tactile sensing, data is required to be collected for each different sensor. This paper considers the problem of learning the mapping between two tactile sensor outputs with respect to the same physical stimulus -- we refer to this problem as touch-to-touch translation. In this respect, we proposed two data-driven approaches to address this task and we compared their performance. The first one exploits a generative model developed for image-to-image translation and adapted for this context. The second one uses a ResNet model trained to perform a regression task. We validated both methods using two completely different tactile sensors -- a camera-based, Digit and a capacitance-based, CySkin. In particular, we used Digit images to generate the corresponding CySkin data. We trained the models on a set of tactile features that can be found in common larger objects and we performed the testing on a previously unseen set of data. Experimental results show the possibility of translating Digit images into the CySkin output by preserving the contact shape and with an error of 15.18% in the magnitude of the sensor responses.</details> | http://arxiv.org/abs/2411.02187v1 |
| Limiting Kinetic Energy through Control Barrier Functions: Analysis and   Experimental Validation | Federico Califano; Daniel Logmans; Wesley Roozing | 2024-11-04 | <details><summary>Click to expand</summary>In the context of safety-critical control, we propose and analyse the use of Control Barrier Functions (CBFs) to limit the kinetic energy of torque-controlled robots. The proposed scheme is able to modify a nominal control action in a minimally invasive manner to achieve the desired kinetic energy limit. We show how this safety condition is achieved by appropriately injecting damping in the underlying robot dynamics independently of the nominal controller structure. We present an extensive experimental validation of the approach on a 7-Degree of Freedom (DoF) Franka Emika Panda robot. The results demonstrate that this approach provides an effective, minimally invasive safety layer that is straightforward to implement and is robust in real experiments.</details> | http://arxiv.org/abs/2411.02186v1 |
| Diffusion-based Virtual Fixtures | Cem Bilaloglu; Tobias Löw; Sylvain Calinon | 2024-11-04 | <details><summary>Click to expand</summary>Virtual fixtures assist human operators in teleoperation settings by constraining their actions. This extended abstract introduces a novel virtual fixture formulation \emph{on surfaces} for tactile robotics tasks. Unlike existing methods, our approach constrains the behavior based on the position on the surface and generalizes it over the surface by considering the distance (metric) on the surface. Our method works directly on possibly noisy and partial point clouds collected via a camera. Given a set of regions on the surface together with their desired behaviors, our method diffuses the behaviors across the entire surface by taking into account the surface geometry. We demonstrate our method's ability in two simulated experiments (i) to regulate contact force magnitude or tangential speed based on surface position and (ii) to guide the robot to targets while avoiding restricted regions defined on the surface. All source codes, experimental data, and videos are available as open access at https://sites.google.com/view/diffusion-virtual-fixtures</details> | http://arxiv.org/abs/2411.02169v1 |
| Learning Multiple Initial Solutions to Optimization Problems | Elad Sharony; Heng Yang; Tong Che; Marco Pavone; Shie Mannor; Peter Karkus | 2024-11-04 | <details><summary>Click to expand</summary>Sequentially solving similar optimization problems under strict runtime constraints is essential for many applications, such as robot control, autonomous driving, and portfolio management. The performance of local optimization methods in these settings is sensitive to the initial solution: poor initialization can lead to slow convergence or suboptimal solutions. To address this challenge, we propose learning to predict \emph{multiple} diverse initial solutions given parameters that define the problem instance. We introduce two strategies for utilizing multiple initial solutions: (i) a single-optimizer approach, where the most promising initial solution is chosen using a selection function, and (ii) a multiple-optimizers approach, where several optimizers, potentially run in parallel, are each initialized with a different solution, with the best solution chosen afterward. We validate our method on three optimal control benchmark tasks: cart-pole, reacher, and autonomous driving, using different optimizers: DDP, MPPI, and iLQR. We find significant and consistent improvement with our method across all evaluation settings and demonstrate that it efficiently scales with the number of initial solutions required. The code is available at $\href{https://github.com/EladSharony/miso}{\tt{https://github.com/EladSharony/miso}}$.</details> | http://arxiv.org/abs/2411.02158v1 |
| Toward Realistic Cinema: The State of the Art in Mechatronics for Modern   Animatronic | Riham M. Hilal; Haitham El-Hussieny; Ayman A. Nada | 2024-11-04 | <details><summary>Click to expand</summary>The pursuit of realism in cinema has driven significant advancements in animatronics, where the integration of mechatronics, a multidisciplinary field that combines mechanical engineering, electronics, and computer science, plays a pivotal role in enhancing the functionality and realism of animatronics. This interdisciplinary approach facilitates smoother characters movements and enhances the sophistication of behaviors in animatronic creatures, thereby increasing their realism. This article examines the most recent developments in mechatronic technology and their significant impact on the art and engineering of animatronics in the filmmaking. It explores the sophisticated integration of system components and analyzes how these enhancements foster complexity and integration, crucial for achieving unprecedented levels of realism in modern cinema. Further, the article delves into in-depth case studies of well-known movie characters, demonstrating the practical applicability of these state-of-the-art mechatronic solutions in creating compelling, lifelike cinematic experiences. This paper aims to bridge the gap between the technical aspects of mechatronics and the creative demands of the film industry, ultimately contributing to the ongoing evolution of cinematic realism.</details> | http://arxiv.org/abs/2411.02102v1 |
| Heterogeneous Multi-robot Task Allocation for Long-Endurance Missions in   Dynamic Scenarios | Alvaro Calvo; Jesus Capitan | 2024-11-04 | <details><summary>Click to expand</summary>We present a framework for Multi-Robot Task Allocation (MRTA) in heterogeneous teams performing long-endurance missions in dynamic scenarios. Given the limited battery of robots, especially in the case of aerial vehicles, we allow for robot recharges and the possibility of fragmenting and/or relaying certain tasks. We also address tasks that must be performed by a coalition of robots in a coordinated manner. Given these features, we introduce a new class of heterogeneous MRTA problems which we analyze theoretically and optimally formulate as a Mixed-Integer Linear Program. We then contribute a heuristic algorithm to compute approximate solutions and integrate it into a mission planning and execution architecture capable of reacting to unexpected events by repairing or recomputing plans online. Our experimental results show the relevance of our newly formulated problem in a realistic use case for inspection with aerial robots. We assess the performance of our heuristic solver in comparison with other variants and with exact optimal solutions in small-scale scenarios. In addition, we evaluate the ability of our replanning framework to repair plans online.</details> | http://arxiv.org/abs/2411.02062v1 |
| An Immediate Update Strategy of Multi-State Constraint Kalman Filter | Qingchao Zhang; Wei Ouyang; Jiale Han; Qi Cai; Maoran Zhu; Yuanxin Wu | 2024-11-04 | <details><summary>Click to expand</summary>The lightweight Multi-state Constraint Kalman Filter (MSCKF) has been well-known for its high efficiency, in which the delayed update has been usually adopted since its proposal. This work investigates the immediate update strategy of MSCKF based on timely reconstructed 3D feature points and measurement constraints. The differences between the delayed update and the immediate update are theoretically analyzed in detail. It is found that the immediate update helps construct more observation constraints and employ more filtering updates than the delayed update, which improves the linearization point of the measurement model and therefore enhances the estimation accuracy. Numerical simulations and experiments show that the immediate update strategy significantly enhances MSCKF even with a small amount of feature observations.</details> | http://arxiv.org/abs/2411.02028v1 |
| Reshaping UAV-Enabled Communications with Omnidirectional Multi-Rotor   Aerial Vehicles | Daniel Bonilla Licea; Giuseppe Silano; Hajar El Hammouti; Mounir Ghogho; Martin Saska | 2024-11-04 | <details><summary>Click to expand</summary>A new class of Multi-Rotor Aerial Vehicles (MRAVs), known as omnidirectional MRAVs (o-MRAVs), has attracted significant interest in the robotics community. These MRAVs have the unique capability of independently controlling their 3D position and 3D orientation. In the context of aerial communication networks, this translates into the ability to control the position and orientation of the antenna mounted on the MRAV without any additional devices tasked for antenna orientation. This additional Degrees of Freedom (DoF) adds a new dimension to aerial communication systems, creating various research opportunities in communications-aware trajectory planning and positioning. This paper presents this new class of MRAVs and discusses use cases in areas such as physical layer security and optical communications. Furthermore, the benefits of these MRAVs are illustrated with realistic simulation scenarios. Finally, new research problems and opportunities introduced by this advanced robotics technology are discussed.</details> | http://arxiv.org/abs/2411.01985v1 |
| Learning Controlled Stochastic Differential Equations | Luc Brogat-Motte; Riccardo Bonalli; Alessandro Rudi | 2024-11-04 | <details><summary>Click to expand</summary>Identification of nonlinear dynamical systems is crucial across various fields, facilitating tasks such as control, prediction, optimization, and fault detection. Many applications require methods capable of handling complex systems while providing strong learning guarantees for safe and reliable performance. However, existing approaches often focus on simplified scenarios, such as deterministic models, known diffusion, discrete systems, one-dimensional dynamics, or systems constrained by strong structural assumptions such as linearity. This work proposes a novel method for estimating both drift and diffusion coefficients of continuous, multidimensional, nonlinear controlled stochastic differential equations with non-uniform diffusion. We assume regularity of the coefficients within a Sobolev space, allowing for broad applicability to various dynamical systems in robotics, finance, climate modeling, and biology. Leveraging the Fokker-Planck equation, we split the estimation into two tasks: (a) estimating system dynamics for a finite set of controls, and (b) estimating coefficients that govern those dynamics. We provide strong theoretical guarantees, including finite-sample bounds for \(L^2\), \(L^\infty\), and risk metrics, with learning rates adaptive to coefficients' regularity, similar to those in nonparametric least-squares regression literature. The practical effectiveness of our approach is demonstrated through extensive numerical experiments. Our method is available as an open-source Python library.</details> | http://arxiv.org/abs/2411.01982v1 |
| Approaches to critical point theory via sequential and parametrized   topological complexity | Stephan Mescher; Maximilian Stegemeyer | 2024-11-04 | <details><summary>Click to expand</summary>The Lusternik-Schnirelmann category of a space was introduced to obtain a lower bound on the number of critical points of a $C^1$-function on a given manifold. Related to Lusternik-Schnirelmann category and motivated by topological robotics, the topological complexity (TC) of a space is a numerical homotopy invariant whose topological properties are an active field of research. The notions of sequential and parametrized topological complexity extend the ideas of topological complexity. While the definition of TC is closely related to Lusternik-Schnirelmann category, the connections of sequential and parametrized TC to critical point theory have not been fully explored yet. In this article we apply methods from Lusternik-Schnirelmann theory to establish various lower bounds on numbers of critical points of functions in terms of sequential and parametrized TCs. We carry out several consequences and applications of these bounds, among them a computation of the parametrized TC of the unit tangent bundles of $(4m-1)$-spheres.</details> | http://arxiv.org/abs/2411.01980v1 |
| V-CAS: A Realtime Vehicle Anti Collision System Using Vision Transformer   on Multi-Camera Streams | Muhammad Waqas Ashraf; Ali Hassan; Imad Ali Shah | 2024-11-04 | <details><summary>Click to expand</summary>This paper introduces a real-time Vehicle Collision Avoidance System (V-CAS) designed to enhance vehicle safety through adaptive braking based on environmental perception. V-CAS leverages the advanced vision-based transformer model RT-DETR, DeepSORT tracking, speed estimation, brake light detection, and an adaptive braking mechanism. It computes a composite collision risk score based on vehicles' relative accelerations, distances, and detected braking actions, using brake light signals and trajectory data from multiple camera streams to improve scene perception. Implemented on the Jetson Orin Nano, V-CAS enables real-time collision risk assessment and proactive mitigation through adaptive braking. A comprehensive training process was conducted on various datasets for comparative analysis, followed by fine-tuning the selected object detection model using transfer learning. The system's effectiveness was rigorously evaluated on the Car Crash Dataset (CCD) from YouTube and through real-time experiments, achieving over 98% accuracy with an average proactive alert time of 1.13 seconds. Results indicate significant improvements in object detection and tracking, enhancing collision avoidance compared to traditional single-camera methods. This research demonstrates the potential of low-cost, multi-camera embedded vision transformer systems to advance automotive safety through enhanced environmental perception and proactive collision avoidance mechanisms.</details> | http://arxiv.org/abs/2411.01963v1 |
| Brainbots as smart autonomous active particles with programmable motion | M. Noirhomme; I. Mammadli; N. Vanesse; J. Pande; A. -S. Smith; N. Vandewalle | 2024-11-04 | <details><summary>Click to expand</summary>We present an innovative robotic device designed to provide controlled motion for studying active matter. Motion is driven by an internal vibrator powered by a small rechargeable battery. The system integrates acoustic and magnetic sensors along with a programmable microcontroller. Unlike conventional vibrobots, the motor induces horizontal vibrations, resulting in cycloidal trajectories that have been characterized and optimized. Portions of these orbits can be utilized to create specific motion patterns. As a proof of concept, we demonstrate how this versatile system can be exploited to develop active particles with varying dynamics, ranging from ballistic motion to run-and-tumble diffusive behavior.</details> | http://arxiv.org/abs/2411.01943v1 |
| Real-Time Polygonal Semantic Mapping for Humanoid Robot Stair Climbing | Teng Bin; Jianming Yao; Tin Lun Lam; Tianwei Zhang | 2024-11-04 | <details><summary>Click to expand</summary>We present a novel algorithm for real-time planar semantic mapping tailored for humanoid robots navigating complex terrains such as staircases. Our method is adaptable to any odometry input and leverages GPU-accelerated processes for planar extraction, enabling the rapid generation of globally consistent semantic maps. We utilize an anisotropic diffusion filter on depth images to effectively minimize noise from gradient jumps while preserving essential edge details, enhancing normal vector images' accuracy and smoothness. Both the anisotropic diffusion and the RANSAC-based plane extraction processes are optimized for parallel processing on GPUs, significantly enhancing computational efficiency. Our approach achieves real-time performance, processing single frames at rates exceeding $30~Hz$, which facilitates detailed plane extraction and map management swiftly and efficiently. Extensive testing underscores the algorithm's capabilities in real-time scenarios and demonstrates its practical application in humanoid robot gait planning, significantly improving its ability to navigate dynamic environments.</details> | http://arxiv.org/abs/2411.01919v1 |
| Preemptive Holistic Collaborative System and Its Application in Road   Transportation | Ting Peng; Yuan Li; Tao Li; Xiaoxue Xu; Xiang Dong; Yincai Cai | 2024-11-04 | <details><summary>Click to expand</summary>Numerous real-world systems, including manufacturing processes, supply chains, and robotic systems, involve multiple independent entities with diverse objectives. The potential for conflicts arises from the inability of these entities to accurately predict and anticipate each other's actions. To address this challenge, we propose the Preemptive Holistic Collaborative System (PHCS) framework. By enabling information sharing and collaborative planning among independent entities, the PHCS facilitates the preemptive resolution of potential conflicts. We apply the PHCS framework to the specific context of road transportation, resulting in the Preemptive Holistic Collaborative Road Transportation System (PHCRTS). This system leverages shared driving intentions and pre-planned trajectories to optimize traffic flow and enhance safety. Simulation experiments in a two-lane merging scenario demonstrate the effectiveness of PHCRTS, reducing vehicle time delays by 90%, increasing traffic capacity by 300%, and eliminating accidents. The PHCS framework offers a promising approach to optimize the performance and safety of complex systems with multiple independent entities.</details> | http://arxiv.org/abs/2411.01918v1 |
| RoboCrowd: Scaling Robot Data Collection through Crowdsourcing | Suvir Mirchandani; David D. Yuan; Kaylee Burns; Md Sazzad Islam; Tony Z. Zhao; Chelsea Finn; Dorsa Sadigh | 2024-11-04 | <details><summary>Click to expand</summary>In recent years, imitation learning from large-scale human demonstrations has emerged as a promising paradigm for training robot policies. However, the burden of collecting large quantities of human demonstrations is significant in terms of collection time and the need for access to expert operators. We introduce a new data collection paradigm, RoboCrowd, which distributes the workload by utilizing crowdsourcing principles and incentive design. RoboCrowd helps enable scalable data collection and facilitates more efficient learning of robot policies. We build RoboCrowd on top of ALOHA (Zhao et al. 2023) -- a bimanual platform that supports data collection via puppeteering -- to explore the design space for crowdsourcing in-person demonstrations in a public environment. We propose three classes of incentive mechanisms to appeal to users' varying sources of motivation for interacting with the system: material rewards, intrinsic interest, and social comparison. We instantiate these incentives through tasks that include physical rewards, engaging or challenging manipulations, as well as gamification elements such as a leaderboard. We conduct a large-scale, two-week field experiment in which the platform is situated in a university cafe. We observe significant engagement with the system -- over 200 individuals independently volunteered to provide a total of over 800 interaction episodes. Our findings validate the proposed incentives as mechanisms for shaping users' data quantity and quality. Further, we demonstrate that the crowdsourced data can serve as useful pre-training data for policies fine-tuned on expert demonstrations -- boosting performance up to 20% compared to when this data is not available. These results suggest the potential for RoboCrowd to reduce the burden of robot data collection by carefully implementing crowdsourcing and incentive design principles.</details> | http://arxiv.org/abs/2411.01915v1 |
| Traffic and Safety Rule Compliance of Humans in Diverse Driving   Situations | Michael Kurenkov; Sajad Marvi; Julian Schmidt; Christoph B. Rist; Alessandro Canevaro; Hang Yu; Julian Jordan; Georg Schildbach; Abhinav Valada | 2024-11-04 | <details><summary>Click to expand</summary>The increasing interest in autonomous driving systems has highlighted the need for an in-depth analysis of human driving behavior in diverse scenarios. Analyzing human data is crucial for developing autonomous systems that replicate safe driving practices and ensure seamless integration into human-dominated environments. This paper presents a comparative evaluation of human compliance with traffic and safety rules across multiple trajectory prediction datasets, including Argoverse 2, nuPlan, Lyft, and DeepUrban. By defining and leveraging existing safety and behavior-related metrics, such as time to collision, adherence to speed limits, and interactions with other traffic participants, we aim to provide a comprehensive understanding of each datasets strengths and limitations. Our analysis focuses on the distribution of data samples, identifying noise, outliers, and undesirable behaviors exhibited by human drivers in both the training and validation sets. The results underscore the need for applying robust filtering techniques to certain datasets due to high levels of noise and the presence of such undesirable behaviors.</details> | http://arxiv.org/abs/2411.01909v1 |
| Efficient Active Imitation Learning with Random Network Distillation | Emilien Biré; Anthony Kobanda; Ludovic Denoyer; Rémy Portelas | 2024-11-04 | <details><summary>Click to expand</summary>Developing agents for complex and underspecified tasks, where no clear objective exists, remains challenging but offers many opportunities. This is especially true in video games, where simulated players (bots) need to play realistically, and there is no clear reward to evaluate them. While imitation learning has shown promise in such domains, these methods often fail when agents encounter out-of-distribution scenarios during deployment. Expanding the training dataset is a common solution, but it becomes impractical or costly when relying on human demonstrations. This article addresses active imitation learning, aiming to trigger expert intervention only when necessary, reducing the need for constant expert input along training. We introduce Random Network Distillation DAgger (RND-DAgger), a new active imitation learning method that limits expert querying by using a learned state-based out-of-distribution measure to trigger interventions. This approach avoids frequent expert-agent action comparisons, thus making the expert intervene only when it is useful. We evaluate RND-DAgger against traditional imitation learning and other active approaches in 3D video games (racing and third-person navigation) and in a robotic locomotion task and show that RND-DAgger surpasses previous methods by reducing expert queries. https://sites.google.com/view/rnd-dagger</details> | http://arxiv.org/abs/2411.01894v1 |
| Improving Trust Estimation in Human-Robot Collaboration Using Beta   Reputation at Fine-grained Timescales | Resul Dagdanov; Milan Andrejevic; Dikai Liu; Chin-Teng Lin | 2024-11-04 | <details><summary>Click to expand</summary>When interacting with each other, humans adjust their behavior based on perceived trust. However, to achieve similar adaptability, robots must accurately estimate human trust at sufficiently granular timescales during the human-robot collaboration task. A beta reputation is a popular way to formalize a mathematical estimation of human trust. However, it relies on binary performance, which updates trust estimations only after each task concludes. Additionally, manually crafting a reward function is the usual method of building a performance indicator, which is labor-intensive and time-consuming. These limitations prevent efficiently capturing continuous changes in trust at more granular timescales throughout the collaboration task. Therefore, this paper presents a new framework for the estimation of human trust using a beta reputation at fine-grained timescales. To achieve granularity in beta reputation, we utilize continuous reward values to update trust estimations at each timestep of a task. We construct a continuous reward function using maximum entropy optimization to eliminate the need for the laborious specification of a performance indicator. The proposed framework improves trust estimations by increasing accuracy, eliminating the need for manually crafting a reward function, and advancing toward developing more intelligent robots. The source code is publicly available. https://github.com/resuldagdanov/robot-learning-human-trust</details> | http://arxiv.org/abs/2411.01866v1 |
| ManiBox: Enhancing Spatial Grasping Generalization via Scalable   Simulation Data Generation | Hengkai Tan; Xuezhou Xu; Chengyang Ying; Xinyi Mao; Songming Liu; Xingxing Zhang; Hang Su; Jun Zhu | 2024-11-04 | <details><summary>Click to expand</summary>Learning a precise robotic grasping policy is crucial for embodied agents operating in complex real-world manipulation tasks. Despite significant advancements, most models still struggle with accurate spatial positioning of objects to be grasped. We first show that this spatial generalization challenge stems primarily from the extensive data requirements for adequate spatial understanding. However, collecting such data with real robots is prohibitively expensive, and relying on simulation data often leads to visual generalization gaps upon deployment. To overcome these challenges, we then focus on state-based policy generalization and present \textbf{ManiBox}, a novel bounding-box-guided manipulation method built on a simulation-based teacher-student framework. The teacher policy efficiently generates scalable simulation data using bounding boxes, which are proven to uniquely determine the objects' spatial positions. The student policy then utilizes these low-dimensional spatial states to enable zero-shot transfer to real robots. Through comprehensive evaluations in simulated and real-world environments, ManiBox demonstrates a marked improvement in spatial grasping generalization and adaptability to diverse objects and backgrounds. Further, our empirical study into scaling laws for policy performance indicates that spatial volume generalization scales positively with data volume. For a certain level of spatial volume, the success rate of grasping empirically follows Michaelis-Menten kinetics relative to data volume, showing a saturation effect as data increases. Our videos and code are available in https://thkkk.github.io/manibox.</details> | http://arxiv.org/abs/2411.01850v1 |
| Toward Integrating Semantic-aware Path Planning and Reliable   Localization for UAV Operations | Thanh Nguyen Canh; Huy-Hoang Ngo; Xiem HoangVan; Nak Young Chong | 2024-11-04 | <details><summary>Click to expand</summary>Localization is one of the most crucial tasks for Unmanned Aerial Vehicle systems (UAVs) directly impacting overall performance, which can be achieved with various sensors and applied to numerous tasks related to search and rescue operations, object tracking, construction, etc. However, due to the negative effects of challenging environments, UAVs may lose signals for localization. In this paper, we present an effective path-planning system leveraging semantic segmentation information to navigate around texture-less and problematic areas like lakes, oceans, and high-rise buildings using a monocular camera. We introduce a real-time semantic segmentation architecture and a novel keyframe decision pipeline to optimize image inputs based on pixel distribution, reducing processing time. A hierarchical planner based on the Dynamic Window Approach (DWA) algorithm, integrated with a cost map, is designed to facilitate efficient path planning. The system is implemented in a photo-realistic simulation environment using Unity, aligning with segmentation model parameters. Comprehensive qualitative and quantitative evaluations validate the effectiveness of our approach, showing significant improvements in the reliability and efficiency of UAV localization in challenging environments.</details> | http://arxiv.org/abs/2411.01816v1 |
| Enhancing Social Robot Navigation with Integrated Motion Prediction and   Trajectory Planning in Dynamic Human Environments | Thanh Nguyen Canh; Xiem HoangVan; Nak Young Chong | 2024-11-04 | <details><summary>Click to expand</summary>Navigating safely in dynamic human environments is crucial for mobile service robots, and social navigation is a key aspect of this process. In this paper, we proposed an integrative approach that combines motion prediction and trajectory planning to enable safe and socially-aware robot navigation. The main idea of the proposed method is to leverage the advantages of Socially Acceptable trajectory prediction and Timed Elastic Band (TEB) by incorporating human interactive information including position, orientation, and motion into the objective function of the TEB algorithms. In addition, we designed social constraints to ensure the safety of robot navigation. The proposed system is evaluated through physical simulation using both quantitative and qualitative metrics, demonstrating its superior performance in avoiding human and dynamic obstacles, thereby ensuring safe navigation. The implementations are open source at: \url{https://github.com/thanhnguyencanh/SGan-TEB.git}</details> | http://arxiv.org/abs/2411.01814v1 |
| So You Think You Can Scale Up Autonomous Robot Data Collection? | Suvir Mirchandani; Suneel Belkhale; Joey Hejna; Evelyn Choi; Md Sazzad Islam; Dorsa Sadigh | 2024-11-04 | <details><summary>Click to expand</summary>A long-standing goal in robot learning is to develop methods for robots to acquire new skills autonomously. While reinforcement learning (RL) comes with the promise of enabling autonomous data collection, it remains challenging to scale in the real-world partly due to the significant effort required for environment design and instrumentation, including the need for designing reset functions or accurate success detectors. On the other hand, imitation learning (IL) methods require little to no environment design effort, but instead require significant human supervision in the form of collected demonstrations. To address these shortcomings, recent works in autonomous IL start with an initial seed dataset of human demonstrations that an autonomous policy can bootstrap from. While autonomous IL approaches come with the promise of addressing the challenges of autonomous RL as well as pure IL strategies, in this work, we posit that such techniques do not deliver on this promise and are still unable to scale up autonomous data collection in the real world. Through a series of real-world experiments, we demonstrate that these approaches, when scaled up to realistic settings, face much of the same scaling challenges as prior attempts in RL in terms of environment design. Further, we perform a rigorous study of autonomous IL methods across different data scales and 7 simulation and real-world tasks, and demonstrate that while autonomous data collection can modestly improve performance, simply collecting more human data often provides significantly more improvement. Our work suggests a negative result: that scaling up autonomous data collection for learning robot policies for real-world tasks is more challenging and impractical than what is suggested in prior work. We hope these insights about the core challenges of scaling up data collection help inform future efforts in autonomous learning.</details> | http://arxiv.org/abs/2411.01813v1 |
| Semantic Masking and Visual Feature Matching for Robust Localization | Luisa Mao; Ryan Soussan; Brian Coltin; Trey Smith; Joydeep Biswas | 2024-11-04 | <details><summary>Click to expand</summary>We are interested in long-term deployments of autonomous robots to aid astronauts with maintenance and monitoring operations in settings such as the International Space Station. Unfortunately, such environments tend to be highly dynamic and unstructured, and their frequent reconfiguration poses a challenge for robust long-term localization of robots. Many state-of-the-art visual feature-based localization algorithms are not robust towards spatial scene changes, and SLAM algorithms, while promising, cannot run within the low-compute budget available to space robots. To address this gap, we present a computationally efficient semantic masking approach for visual feature matching that improves the accuracy and robustness of visual localization systems during long-term deployment in changing environments. Our method introduces a lightweight check that enforces matches to be within long-term static objects and have consistent semantic classes. We evaluate this approach using both map-based relocalization and relative pose estimation and show that it improves Absolute Trajectory Error (ATE) and correct match ratios on the publicly available Astrobee dataset. While this approach was originally developed for microgravity robotic freeflyers, it can be applied to any visual feature matching pipeline to improve robustness.</details> | http://arxiv.org/abs/2411.01804v1 |
| Constrained Human-AI Cooperation: An Inclusive Embodied Social   Intelligence Challenge | Weihua Du; Qiushi Lyu; Jiaming Shan; Zhenting Qi; Hongxin Zhang; Sunli Chen; Andi Peng; Tianmin Shu; Kwonjoon Lee; Behzad Dariush; Chuang Gan | 2024-11-04 | <details><summary>Click to expand</summary>We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints -- e.g., unable to reach high places or confined to a wheelchair -- in performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human's intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). To benchmark this challenge, we create four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learning-based baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at this URL: https://github.com/UMass-Foundation-Model/CHAIC.</details> | http://arxiv.org/abs/2411.01796v1 |
| Eurekaverse: Environment Curriculum Generation via Large Language Models | William Liang; Sam Wang; Hung-Ju Wang; Osbert Bastani; Dinesh Jayaraman; Yecheng Jason Ma | 2024-11-04 | <details><summary>Click to expand</summary>Recent work has demonstrated that a promising strategy for teaching robots a wide range of complex skills is by training them on a curriculum of progressively more challenging environments. However, developing an effective curriculum of environment distributions currently requires significant expertise, which must be repeated for every new domain. Our key insight is that environments are often naturally represented as code. Thus, we probe whether effective environment curriculum design can be achieved and automated via code generation by large language models (LLM). In this paper, we introduce Eurekaverse, an unsupervised environment design algorithm that uses LLMs to sample progressively more challenging, diverse, and learnable environments for skill training. We validate Eurekaverse's effectiveness in the domain of quadrupedal parkour learning, in which a quadruped robot must traverse through a variety of obstacle courses. The automatic curriculum designed by Eurekaverse enables gradual learning of complex parkour skills in simulation and can successfully transfer to the real-world, outperforming manual training courses designed by humans.</details> | http://arxiv.org/abs/2411.01775v1 |
| Next Best View For Point-Cloud Model Acquisition: Bayesian Approximation   and Uncertainty Analysis | Madalena Caldeira; Plinio Moreno | 2024-11-04 | <details><summary>Click to expand</summary>The Next Best View problem is a computer vision problem widely studied in robotics. To solve it, several methodologies have been proposed over the years. Some, more recently, propose the use of deep learning models. Predictions obtained with the help of deep learning models naturally have some uncertainty associated with them. Despite this, the standard models do not allow for their quantification. However, Bayesian estimation theory contributed to the demonstration that dropout layers allow to estimate prediction uncertainty in neural networks.   This work adapts the point-net-based neural network for Next-Best-View (PC-NBV). It incorporates dropout layers into the model's architecture, thus allowing the computation of the uncertainty estimate associated with its predictions. The aim of the work is to improve the network's accuracy in correctly predicting the next best viewpoint, proposing a way to make the 3D reconstruction process more efficient.   Two uncertainty measurements capable of reflecting the prediction's error and accuracy, respectively, were obtained. These enabled the reduction of the model's error and the increase in its accuracy from 30\% to 80\% by identifying and disregarding predictions with high values of uncertainty. Another method that directly uses these uncertainty metrics to improve the final prediction was also proposed. However, it showed very residual improvements.</details> | http://arxiv.org/abs/2411.01734v1 |
| A Probabilistic Formulation of LiDAR Mapping with Neural Radiance Fields | Matthew McDermott; Jason Rife | 2024-11-04 | <details><summary>Click to expand</summary>In this paper we reexamine the process through which a Neural Radiance Field (NeRF) can be trained to produce novel LiDAR views of a scene. Unlike image applications where camera pixels integrate light over time, LiDAR pulses arrive at specific times. As such, multiple LiDAR returns are possible for any given detector and the classification of these returns is inherently probabilistic. Applying a traditional NeRF training routine can result in the network learning phantom surfaces in free space between conflicting range measurements, similar to how floater aberrations may be produced by an image model. We show that by formulating loss as an integral of probability (rather than as an integral of optical density) the network can learn multiple peaks for a given ray, allowing the sampling of first, nth, or strongest returns from a single output channel. Code is available at https://github.com/mcdermatt/PLINK</details> | http://arxiv.org/abs/2411.01725v1 |
| Large-Scale Multi-Robot Coverage Path Planning on Grids with Path   Deconfliction | Jingtao Tang; Zining Mao; Hang Ma | 2024-11-03 | <details><summary>Click to expand</summary>We study Multi-Robot Coverage Path Planning (MCPP) on a 4-neighbor 2D grid G, which aims to compute paths for multiple robots to cover all cells of G. Traditional approaches are limited as they first compute coverage trees on a quadrant coarsened grid H and then employ the Spanning Tree Coverage (STC) paradigm to generate paths on G, making them inapplicable to grids with partially obstructed 2x2 blocks. To address this limitation, we reformulate the problem directly on G, revolutionizing grid-based MCPP solving and establishing new NP-hardness results. We introduce Extended-STC (ESTC), a novel paradigm that extends STC to ensure complete coverage with bounded suboptimality, even when H includes partially obstructed blocks. Furthermore, we present LS-MCPP, a new algorithmic framework that integrates ESTC with three novel types of neighborhood operators within a local search strategy to optimize coverage paths directly on G. Unlike prior grid-based MCPP work, our approach also incorporates a versatile post-processing procedure that applies Multi-Agent Path Finding (MAPF) techniques to MCPP for the first time, enabling a fusion of these two important fields in multi-robot coordination. This procedure effectively resolves inter-robot conflicts and accommodates turning costs by solving a MAPF variant, making our MCPP solutions more practical for real-world applications. Extensive experiments demonstrate that our approach significantly improves solution quality and efficiency, managing up to 100 robots on grids as large as 256x256 within minutes of runtime. Validation with physical robots confirms the feasibility of our solutions under real-world conditions.</details> | http://arxiv.org/abs/2411.01707v1 |
| Neural Inverse Source Problems | Youngsun Wi; Jayjun Lee; Miquel Oller; Nima Fazeli | 2024-11-03 | <details><summary>Click to expand</summary>Reconstructing unknown external source functions is an important perception capability for a large range of robotics domains including manipulation, aerial, and underwater robotics. In this work, we propose a Physics-Informed Neural Network (PINN [1]) based approach for solving the inverse source problems in robotics, jointly identifying unknown source functions and the complete state of a system given partial and noisy observations. Our approach demonstrates several advantages over prior works (Finite Element Methods (FEM) and data-driven approaches): it offers flexibility in integrating diverse constraints and boundary conditions; eliminates the need for complex discretizations (e.g., meshing); easily accommodates gradients from real measurements; and does not limit performance based on the diversity and quality of training data. We validate our method across three simulation and real-world scenarios involving up to 4th order partial differential equations (PDEs), constraints such as Signorini and Dirichlet, and various regression losses including Chamfer distance and L2 norm.</details> | http://arxiv.org/abs/2411.01665v1 |
| Know Where You're Uncertain When Planning with Multimodal Foundation   Models: A Formal Framework | Neel P. Bhatt; Yunhao Yang; Rohan Siva; Daniel Milan; Ufuk Topcu; Zhangyang Wang | 2024-11-03 | <details><summary>Click to expand</summary>Multimodal foundation models offer a promising framework for robotic perception and planning by processing sensory inputs to generate actionable plans. However, addressing uncertainty in both perception (sensory interpretation) and decision-making (plan generation) remains a critical challenge for ensuring task reliability. We present a comprehensive framework to disentangle, quantify, and mitigate these two forms of uncertainty. We first introduce a framework for uncertainty disentanglement, isolating perception uncertainty arising from limitations in visual understanding and decision uncertainty relating to the robustness of generated plans.   To quantify each type of uncertainty, we propose methods tailored to the unique properties of perception and decision-making: we use conformal prediction to calibrate perception uncertainty and introduce Formal-Methods-Driven Prediction (FMDP) to quantify decision uncertainty, leveraging formal verification techniques for theoretical guarantees. Building on this quantification, we implement two targeted intervention mechanisms: an active sensing process that dynamically re-observes high-uncertainty scenes to enhance visual input quality and an automated refinement procedure that fine-tunes the model on high-certainty data, improving its capability to meet task specifications. Empirical validation in real-world and simulated robotic tasks demonstrates that our uncertainty disentanglement framework reduces variability by up to 40% and enhances task success rates by 5% compared to baselines. These improvements are attributed to the combined effect of both interventions and highlight the importance of uncertainty disentanglement which facilitates targeted interventions that enhance the robustness and reliability of autonomous systems.</details> | http://arxiv.org/abs/2411.01639v1 |
| GITSR: Graph Interaction Transformer-based Scene Representation for   Multi Vehicle Collaborative Decision-making | Xingyu Hu; Lijun Zhang; Dejian Meng; Ye Han; Lisha Yuan | 2024-11-03 | <details><summary>Click to expand</summary>In this study, we propose GITSR, an effective framework for Graph Interaction Transformer-based Scene Representation for multi-vehicle collaborative decision-making in intelligent transportation system. In the context of mixed traffic where Connected Automated Vehicles (CAVs) and Human Driving Vehicles (HDVs) coexist, in order to enhance the understanding of the environment by CAVs to improve decision-making capabilities, this framework focuses on efficient scene representation and the modeling of spatial interaction behaviors of traffic states. We first extract features of the driving environment based on the background of intelligent networking. Subsequently, the local scene representation, which is based on the agent-centric and dynamic occupation grid, is calculated by the Transformer module. Besides, feasible region of the map is captured through the multi-head attention mechanism to reduce the collision of vehicles. Notably, spatial interaction behaviors, based on motion information, are modeled as graph structures and extracted via Graph Neural Network (GNN). Ultimately, the collaborative decision-making among multiple vehicles is formulated as a Markov Decision Process (MDP), with driving actions output by Reinforcement Learning (RL) algorithms. Our algorithmic validation is executed within the extremely challenging scenario of highway off-ramp task, thereby substantiating the superiority of agent-centric approach to scene representation. Simulation results demonstrate that the GITSR method can not only effectively capture scene representation but also extract spatial interaction data, outperforming the baseline method across various comparative metrics.</details> | http://arxiv.org/abs/2411.01608v1 |
| An Aerial Transport System in Marine GNSS-Denied Environment | Jianjun Sun; Zhenwei Niu; Yihao Dong; Fenglin Zhang; Muhayy Ud Din; Lakmal Seneviratne; Defu Lin; Irfan Hussain; Shaoming He | 2024-11-03 | <details><summary>Click to expand</summary>This paper presents an autonomous aerial system specifically engineered for operation in challenging marine GNSS-denied environments, aimed at transporting small cargo from a target vessel. In these environments, characterized by weakly textured sea surfaces with few feature points, chaotic deck oscillations due to waves, and significant wind gusts, conventional navigation methods often prove inadequate. Leveraging the DJI M300 platform, our system is designed to autonomously navigate and transport cargo while overcoming these environmental challenges. In particular, this paper proposes an anchor-based localization method using ultrawideband (UWB) and QR codes facilities, which decouples the UAV's attitude from that of the moving landing platform, thus reducing control oscillations caused by platform movement. Additionally, a motor-driven attachment mechanism for cargo is designed, which enhances the UAV's field of view during descent and ensures a reliable attachment to the cargo upon landing. The system's reliability and effectiveness were progressively enhanced through multiple outdoor experimental iterations and were validated by the successful cargo transport during the 2024 Mohamed BinZayed International Robotics Challenge (MBZIRC2024) competition. Crucially, the system addresses uncertainties and interferences inherent in maritime transportation missions without prior knowledge of cargo locations on the deck and with strict limitations on intervention throughout the transportation.</details> | http://arxiv.org/abs/2411.01603v1 |
| Addressing Failures in Robotics using Vision-Based Language Models   (VLMs) and Behavior Trees (BT) | Faseeh Ahmad; Jonathan Styrud; Volker Krueger | 2024-11-03 | <details><summary>Click to expand</summary>In this paper, we propose an approach that combines Vision Language Models (VLMs) and Behavior Trees (BTs) to address failures in robotics. Current robotic systems can handle known failures with pre-existing recovery strategies, but they are often ill-equipped to manage unknown failures or anomalies. We introduce VLMs as a monitoring tool to detect and identify failures during task execution. Additionally, VLMs generate missing conditions or skill templates that are then incorporated into the BT, ensuring the system can autonomously address similar failures in future tasks. We validate our approach through simulations in several failure scenarios.</details> | http://arxiv.org/abs/2411.01568v1 |
| Interaction-Aware Trajectory Prediction for Safe Motion Planning in   Autonomous Driving: A Transformer-Transfer Learning Approach | Jinhao Liang; Chaopeng Tan; Longhao Yan; Jingyuan Zhou; Guodong Yin; Kaidi Yang | 2024-11-03 | <details><summary>Click to expand</summary>A critical aspect of safe and efficient motion planning for autonomous vehicles (AVs) is to handle the complex and uncertain behavior of surrounding human-driven vehicles (HDVs). Despite intensive research on driver behavior prediction, existing approaches typically overlook the interactions between AVs and HDVs assuming that HDV trajectories are not affected by AV actions. To address this gap, we present a transformer-transfer learning-based interaction-aware trajectory predictor for safe motion planning of autonomous driving, focusing on a vehicle-to-vehicle (V2V) interaction scenario consisting of an AV and an HDV. Specifically, we construct a transformer-based interaction-aware trajectory predictor using widely available datasets of HDV trajectory data and further transfer the learned predictor using a small set of AV-HDV interaction data. Then, to better incorporate the proposed trajectory predictor into the motion planning module of AVs, we introduce an uncertainty quantification method to characterize the errors of the predictor, which are integrated into the path-planning process. Our experimental results demonstrate the value of explicitly considering interactions and handling uncertainties.</details> | http://arxiv.org/abs/2411.01475v1 |
| HiMemFormer: Hierarchical Memory-Aware Transformer for Multi-Agent   Action Anticipation | Zirui Wang; Xinran Zhao; Simon Stepputtis; Woojun Kim; Tongshuang Wu; Katia Sycara; Yaqi Xie | 2024-11-03 | <details><summary>Click to expand</summary>Understanding and predicting human actions has been a long-standing challenge and is a crucial measure of perception in robotics AI. While significant progress has been made in anticipating the future actions of individual agents, prior work has largely overlooked a key aspect of real-world human activity -- interactions. To address this gap in human-like forecasting within multi-agent environments, we present the Hierarchical Memory-Aware Transformer (HiMemFormer), a transformer-based model for online multi-agent action anticipation. HiMemFormer integrates and distributes global memory that captures joint historical information across all agents through a transformer framework, with a hierarchical local memory decoder that interprets agent-specific features based on these global representations using a coarse-to-fine strategy. In contrast to previous approaches, HiMemFormer uniquely hierarchically applies the global context with agent-specific preferences to avoid noisy or redundant information in multi-agent action anticipation. Extensive experiments on various multi-agent scenarios demonstrate the significant performance of HiMemFormer, compared with other state-of-the-art methods.</details> | http://arxiv.org/abs/2411.01455v1 |
| Upstroke wing clapping in bats and bat-inspired robots improves both   lift generation and power economy | Xiaozhou Fan; Alberto Bortoni; Siyang Hao; Sharon Swartz; Kenneth Breuer | 2024-11-03 | <details><summary>Click to expand</summary>Wing articulation is critical for efficient flight of bird- and bat-sized animals. Inspired by the flight of $\textit{Cynopterus brachyotis}$, the lesser short-nosed fruit bat, we built a two-degree-of-freedom flapping wing platform with variable wing folding capability. In late upstroke, the wings "clap" and produce an air jet that significantly increases lift production, with a positive peak matched to that produced in downstroke. Though ventral clapping has been observed in avian flight, potential aerodynamic benefit of this behavior has yet to be rigorously assessed. We used multiple approaches -- quasi-steady modeling, direct force/power measurement, and PIV experiments in a wind tunnel -- to understand critical aspects of lift/power variation in relation to wing folding magnitude over Strouhal numbers between $St = 0.2 - 0.4$. While lift increases monotonically with folding amplitude in that range, power economy (ratio of lift/power) is more nuanced. At $St = 0.2 - 0.3$, it increase with wing folding amplitude monotonically. At $St = 0.3 - 0.4$, it features two maxima -- one at medium folding amplitude ($\sim 30^\circ$), and the other at maximum folding. These findings illuminate two strategies available to flapping wing animals and robots -- symmetry-breaking lift augmentation and appendage-based jet propulsion.</details> | http://arxiv.org/abs/2411.01434v1 |
| Exploring the Edges of Latent State Clusters for Goal-Conditioned   Reinforcement Learning | Yuanlin Duan; Guofeng Cui; He Zhu | 2024-11-03 | <details><summary>Click to expand</summary>Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning. While selecting exploratory goals at the frontier of previously explored states is an effective strategy, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior. We propose "Cluster Edge Exploration" ($CE^2$), a new goal-directed exploration algorithm that when choosing goals in sparsely explored areas of the state space gives priority to goal states that remain accessible to the agent. The key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, $CE^2$ demonstrates superior efficiency in exploration compared to baseline methods and ablations.</details> | http://arxiv.org/abs/2411.01396v1 |
| Wallbounce : Push wall to navigate with Contact-Implicit MPC | Xiaohan Liu; Cunxi Dai; John Z. Zhang; Arun Bishop; Zachary Manchester; Ralph Hollis | 2024-11-03 | <details><summary>Click to expand</summary>In this work, we introduce a framework that enables highly maneuverable locomotion using non-periodic contacts. This task is challenging for traditional optimization and planning methods to handle due to difficulties in specifying contact mode sequences in real-time. To address this, we use a bi-level contact-implicit planner and hybrid model predictive controller to draft and execute a motion plan. We investigate how this method allows us to plan arm contact events on the shmoobot, a smaller ballbot, which uses an inverse mouse-ball drive to achieve dynamic balancing with a low number of actuators. Through multiple experiments we show how the arms allow for acceleration, deceleration and dynamic obstacle avoidance that are not achievable with the mouse-ball drive alone. This demonstrates how a holistic approach to locomotion can increase the control authority of unique robot morpohologies without additional hardware by leveraging robot arms that are typically used only for manipulation. Project website: https://cmushmoobot.github.io/Wallbounce</details> | http://arxiv.org/abs/2411.01387v1 |
| Use Digital Twins to Support Fault Diagnosis From System-level   Condition-monitoring Data | Killian Mc Court; Xavier Mc Court; Shijia Du; Zhiguo Zeng | 2024-11-02 | <details><summary>Click to expand</summary>Deep learning models have created great opportunities for data-driven fault diagnosis but they require large amount of labeled failure data for training. In this paper, we propose to use a digital twin to support developing data-driven fault diagnosis model to reduce the amount of failure data used in the training process. The developed fault diagnosis models are also able to diagnose component-level failures based on system-level condition-monitoring data. The proposed framework is evaluated on a real-world robot system. The results showed that the deep learning model trained by digital twins is able to diagnose the locations and modes of 9 faults/failure from $4$ different motors. However, the performance of the model trained by a digital twin can still be improved, especially when the digital twin model has some discrepancy with the real system.</details> | http://arxiv.org/abs/2411.01360v1 |
| The Role of Domain Randomization in Training Diffusion Policies for   Whole-Body Humanoid Control | Oleg Kaidanov; Firas Al-Hafez; Yusuf Suvari; Boris Belousov; Jan Peters | 2024-11-02 | <details><summary>Click to expand</summary>Humanoids have the potential to be the ideal embodiment in environments designed for humans. Thanks to the structural similarity to the human body, they benefit from rich sources of demonstration data, e.g., collected via teleoperation, motion capture, or even using videos of humans performing tasks. However, distilling a policy from demonstrations is still a challenging problem. While Diffusion Policies (DPs) have shown impressive results in robotic manipulation, their applicability to locomotion and humanoid control remains underexplored. In this paper, we investigate how dataset diversity and size affect the performance of DPs for humanoid whole-body control. In a simulated IsaacGym environment, we generate synthetic demonstrations by training Adversarial Motion Prior (AMP) agents under various Domain Randomization (DR) conditions, and we compare DPs fitted to datasets of different size and diversity. Our findings show that, although DPs can achieve stable walking behavior, successful training of locomotion policies requires significantly larger and more diverse datasets compared to manipulation tasks, even in simple scenarios.</details> | http://arxiv.org/abs/2411.01349v1 |
| Control Strategies for Pursuit-Evasion Under Occlusion Using Visibility   and Safety Barrier Functions | Minnan Zhou; Mustafa Shaikh; Vatsalya Chaubey; Patrick Haggerty; Shumon Koga; Dimitra Panagou; Nikolay Atanasov | 2024-11-02 | <details><summary>Click to expand</summary>This paper develops a control strategy for pursuit-evasion problems in environments with occlusions. We address the challenge of a mobile pursuer keeping a mobile evader within its field of view (FoV) despite line-of-sight obstructions. The signed distance function (SDF) of the FoV is used to formulate visibility as a control barrier function (CBF) constraint on the pursuer's control inputs. Similarly, obstacle avoidance is formulated as a CBF constraint based on the SDF of the obstacle set. While the visibility and safety CBFs are Lipschitz continuous, they are not differentiable everywhere, necessitating the use of generalized gradients. To achieve non-myopic pursuit, we generate reference control trajectories leading to evader visibility using a sampling-based kinodynamic planner. The pursuer then tracks this reference via convex optimization under the CBF constraints. We validate our approach in CARLA simulations and real-world robot experiments, demonstrating successful visibility maintenance using only onboard sensing, even under severe occlusions and dynamic evader movements.</details> | http://arxiv.org/abs/2411.01321v1 |
| Receding Hamiltonian-Informed Optimal Neural Control and State   Estimation for Closed-Loop Dynamical Systems | Josue N. Rivera; Dengfeng Sun | 2024-11-02 | <details><summary>Click to expand</summary>This paper formalizes Hamiltonian-Informed Optimal Neural (Hion) controllers, a novel class of neural network-based controllers for dynamical systems and explicit non-linear model predictive control. Hion controllers estimate future states and compute optimal control inputs using Pontryagin's Maximum Principle. The proposed framework allows for customization of transient behavior, addressing limitations of existing methods. The Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano) architecture facilitates training and ensures accurate state estimation. Optimal control strategies are demonstrated for both linear and non-linear dynamical systems.</details> | http://arxiv.org/abs/2411.01297v1 |
| Mixed-Integer MPC-Based Motion Planning Using Hybrid Zonotopes with   Tight Relaxations | Joshua A. Robbins; Jacob A. Siefert; Sean Brennan; Herschel C. Pangborn | 2024-11-02 | <details><summary>Click to expand</summary>Autonomous vehicle (AV) motion planning problems often involve non-convex constraints, which present a major barrier to applying model predictive control (MPC) in real time on embedded hardware. This paper presents an approach for efficiently solving mixed-integer MPC motion planning problems using a hybrid zonotope representation of the obstacle-free space. The MPC optimization problem is formulated as a multi-stage mixed-integer quadratic program (MIQP) using a hybrid zonotope representation of the non-convex constraints. Risk-aware planning is supported by assigning costs to different regions of the obstacle-free space within the MPC cost function. A multi-stage MIQP solver is presented that exploits the structure of the hybrid zonotope constraints. For some hybrid zonotope representations, it is shown that the convex relaxation is tight, i.e., equal to the convex hull. In conjunction with logical constraints derived from the AV motion planning context, this property is leveraged to generate tight quadratic program (QP) sub-problems within a branch-and-bound mixed-integer solver. The hybrid zonotope structure is further leveraged to reduce the number of matrix factorizations that need to be computed within the QP sub-problems. Simulation studies are presented for obstacle-avoidance and risk-aware motion planning problems using polytopic maps and occupancy grids. In most cases, the proposed solver finds the optimal solution an order of magnitude faster than a state-of-the-art commercial solver. Processor-in-the-loop studies demonstrate the utility of the solver for real-time implementations on embedded hardware.</details> | http://arxiv.org/abs/2411.01286v1 |
| Task-Oriented Hierarchical Object Decomposition for Visuomotor Control | Jianing Qian; Yunshuang Li; Bernadette Bucher; Dinesh Jayaraman | 2024-11-02 | <details><summary>Click to expand</summary>Good pre-trained visual representations could enable robots to learn visuomotor policy efficiently. Still, existing representations take a one-size-fits-all-tasks approach that comes with two important drawbacks: (1) Being completely task-agnostic, these representations cannot effectively ignore any task-irrelevant information in the scene, and (2) They often lack the representational capacity to handle unconstrained/complex real-world scenes. Instead, we propose to train a large combinatorial family of representations organized by scene entities: objects and object parts. This hierarchical object decomposition for task-oriented representations (HODOR) permits selectively assembling different representations specific to each task while scaling in representational capacity with the complexity of the scene and the task. In our experiments, we find that HODOR outperforms prior pre-trained representations, both scene vector representations and object-centric representations, for sample-efficient imitation learning across 5 simulated and 5 real-world manipulation tasks. We further find that the invariances captured in HODOR are inherited into downstream policies, which can robustly generalize to out-of-distribution test conditions, permitting zero-shot skill chaining. Appendix, code, and videos: https://sites.google.com/view/hodor-corl24.</details> | http://arxiv.org/abs/2411.01284v1 |
| Efficient Collaborative Navigation through Perception Fusion for   Multi-Robots in Unknown Environments | Qingquan Lin; Weining Lu; Litong Meng; Chenxi Li; Bin Liang | 2024-11-02 | <details><summary>Click to expand</summary>For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings.In this paper, we propose a novel multi-robot collaborative planning method that leverages the perception of different robots to intelligently select search directions and improve planning efficiency. Specifically, a foundational planner is employed to ensure reliable exploration towards targets in unknown environments and we introduce Graph Attention Architecture with Information Gain Weight(GIWT) to synthesizes the information from the target robot and its teammates to facilitate effective navigation around obstacles.In GIWT, after regionally encoding the relative positions of the robots along with their perceptual features, we compute the shared attention scores and incorporate the information gain obtained from neighboring robots as a supplementary weight. We design a corresponding expert data generation scheme to simulate real-world decision-making conditions for network training. Simulation experiments and real robot tests demonstrates that the proposed method significantly improves efficiency and enables collaborative planning for multiple robots. Our method achieves approximately 82% accuracy on the expert dataset and reduces the average path length by about 8% and 6% across two types of tasks compared to the fundamental planner in ROS tests, and a path length reduction of over 6% in real-world experiments.</details> | http://arxiv.org/abs/2411.01274v1 |
| Rotational Odometry using Ultra Low Resolution Thermal Cameras | Ali Safa | 2024-11-02 | <details><summary>Click to expand</summary>This letter provides what is, to the best of our knowledge, a first study on the applicability of ultra-low-resolution thermal cameras for providing rotational odometry measurements to navigational devices such as rovers and drones. Our use of an ultra-low-resolution thermal camera instead of other modalities such as an RGB camera is motivated by its robustness to lighting conditions, while being one order of magnitude less cost-expensive compared to higher-resolution thermal cameras. After setting up a custom data acquisition system and acquiring thermal camera data together with its associated rotational speed label, we train a small 4-layer Convolutional Neural Network (CNN) for regressing the rotational speed from the thermal data. Experiments and ablation studies are conducted for determining the impact of thermal camera resolution and the number of successive frames on the CNN estimation precision. Finally, our novel dataset for the study of low-resolution thermal odometry is openly released with the hope of benefiting future research.</details> | http://arxiv.org/abs/2411.01227v1 |
| MonoPlane: Exploiting Monocular Geometric Cues for Generalizable 3D   Plane Reconstruction | Wang Zhao; Jiachen Liu; Sheng Zhang; Yishu Li; Sili Chen; Sharon X Huang; Yong-Jin Liu; Hengkai Guo | 2024-11-02 | <details><summary>Click to expand</summary>This paper presents a generalizable 3D plane detection and reconstruction framework named MonoPlane. Unlike previous robust estimator-based works (which require multiple images or RGB-D input) and learning-based works (which suffer from domain shift), MonoPlane combines the best of two worlds and establishes a plane reconstruction pipeline based on monocular geometric cues, resulting in accurate, robust and scalable 3D plane detection and reconstruction in the wild. Specifically, we first leverage large-scale pre-trained neural networks to obtain the depth and surface normals from a single image. These monocular geometric cues are then incorporated into a proximity-guided RANSAC framework to sequentially fit each plane instance. We exploit effective 3D point proximity and model such proximity via a graph within RANSAC to guide the plane fitting from noisy monocular depths, followed by image-level multi-plane joint optimization to improve the consistency among all plane instances. We further design a simple but effective pipeline to extend this single-view solution to sparse-view 3D plane reconstruction. Extensive experiments on a list of datasets demonstrate our superior zero-shot generalizability over baselines, achieving state-of-the-art plane reconstruction performance in a transferring setting. Our code is available at https://github.com/thuzhaowang/MonoPlane .</details> | http://arxiv.org/abs/2411.01226v1 |
| Real-Time Spatio-Temporal Reconstruction of Dynamic Endoscopic Scenes   with 4D Gaussian Splatting | Fengze Li; Jishuai He; Jieming Ma; Zhijing Wu | 2024-11-02 | <details><summary>Click to expand</summary>Dynamic scene reconstruction is essential in robotic minimally invasive surgery, providing crucial spatial information that enhances surgical precision and outcomes. However, existing methods struggle to address the complex, temporally dynamic nature of endoscopic scenes. This paper presents ST-Endo4DGS, a novel framework that models the spatio-temporal volume of dynamic endoscopic scenes using unbiased 4D Gaussian Splatting (4DGS) primitives, parameterized by anisotropic ellipses with flexible 4D rotations. This approach enables precise representation of deformable tissue dynamics, capturing intricate spatial and temporal correlations in real time. Additionally, we extend spherindrical harmonics to represent time-evolving appearance, achieving realistic adaptations to lighting and view changes. A new endoscopic normal alignment constraint (ENAC) further enhances geometric fidelity by aligning rendered normals with depth-derived geometry. Extensive evaluations show that ST-Endo4DGS outperforms existing methods in both visual quality and real-time performance, establishing a new state-of-the-art in dynamic scene reconstruction for endoscopic surgery.</details> | http://arxiv.org/abs/2411.01218v1 |
| GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation | Haoran Lu; Ruihai Wu; Yitong Li; Sijie Li; Ziyu Zhu; Chuanruo Ning; Yan Shen; Longzan Luo; Yuanpei Chen; Hao Dong | 2024-11-02 | <details><summary>Click to expand</summary>Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work. Our project page is available at: https://garmentlab.github.io/</details> | http://arxiv.org/abs/2411.01200v1 |
| Generation of Conservative Dynamical Systems Based on Stiffness Encoding | Tengyu Hou; Hanming Bai; Ye Ding; Han Ding | 2024-11-02 | <details><summary>Click to expand</summary>Dynamical systems (DSs) provide a framework for high flexibility, robustness, and control reliability and are widely used in motion planning and physical human-robot interaction. The properties of the DS directly determine the robot's specific motion patterns and the performance of the closed-loop control system. In this paper, we establish a quantitative relationship between stiffness properties and DS. We propose a stiffness encoding framework to modulate DS properties by embedding specific stiffnesses. In particular, from the perspective of the closed-loop control system's passivity, a conservative DS is learned by encoding a conservative stiffness. The generated DS has a symmetric attraction behavior and a variable stiffness profile. The proposed method is applicable to demonstration trajectories belonging to different manifolds and types (e.g., closed and self-intersecting trajectories), and the closed-loop control system is always guaranteed to be passive in different cases. For controllers tracking the general DS, the passivity of the system needs to be guaranteed by the energy tank. We further propose a generic vector field decomposition strategy based on conservative stiffness, which effectively slows down the decay rate of energy in the energy tank and improves the stability margin of the control system. Finally, a series of simulations in various scenarios and experiments on planar and curved motion tasks demonstrate the validity of our theory and methodology.</details> | http://arxiv.org/abs/2411.01120v1 |
| AquaFuse: Waterbody Fusion for Physics Guided View Synthesis of   Underwater Scenes | Md Abu Bakr Siddique; Jiayi Wu; Ioannis Rekleitis; Md Jahidul Islam | 2024-11-02 | <details><summary>Click to expand</summary>We introduce the idea of AquaFuse, a physics-based method for synthesizing waterbody properties in underwater imagery. We formulate a closed-form solution for waterbody fusion that facilitates realistic data augmentation and geometrically consistent underwater scene rendering. AquaFuse leverages the physical characteristics of light propagation underwater to synthesize the waterbody from one scene to the object contents of another. Unlike data-driven style transfer, AquaFuse preserves the depth consistency and object geometry in an input scene. We validate this unique feature by comprehensive experiments over diverse underwater scenes. We find that the AquaFused images preserve over 94% depth consistency and 90-95% structural similarity of the input scenes. We also demonstrate that it generates accurate 3D view synthesis by preserving object geometry while adapting to the inherent waterbody fusion process. AquaFuse opens up a new research direction in data augmentation by geometry-preserving style transfer for underwater imaging and robot vision applications.</details> | http://arxiv.org/abs/2411.01119v1 |
| Contrasting with Symile: Simple Model-Agnostic Representation Learning   for Unlimited Modalities | Adriel Saporta; Aahlad Puli; Mark Goldstein; Rajesh Ranganath | 2024-11-01 | <details><summary>Click to expand</summary>Contrastive learning methods, such as CLIP, leverage naturally paired data-for example, images and their corresponding text captions-to learn general representations that transfer efficiently to downstream tasks. While such approaches are generally applied to two modalities, domains such as robotics, healthcare, and video need to support many types of data at once. We show that the pairwise application of CLIP fails to capture joint information between modalities, thereby limiting the quality of the learned representations. To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile's objective, we derive a lower bound on total correlation, and show that Symile representations for any set of modalities form a sufficient statistic for predicting the remaining modalities. Symile outperforms pairwise CLIP, even with modalities missing in the data, on cross-modal classification and retrieval across several experiments including on an original multilingual dataset of 33M image, text and audio samples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. All datasets and code used in this work are publicly available at https://github.com/rajesh-lab/symile.</details> | http://arxiv.org/abs/2411.01053v1 |
| AGISim, An Open Source Airborne Gimbal Mounted IMU Signal Simulator   Considering Flight Dynamics Model | Alireza Kazemi; Reza Rohani Sarvestani | 2024-11-01 | <details><summary>Click to expand</summary>In this work we present more comprehensive evaluations on our airborne Gimbal mounted inertial measurement unit (IMU) signal simulator which also considers flight dynamic model (FDM). A flexible IMU signal simulator is an enabling tool in design, development, improvement, test and verification of aided inertial navigation systems (INS). Efforts by other researchers had been concentrated on simulation of the strapdown INS (SINS) with the IMU rigidly attached to the moving body frame. However custom airborne surveying/mapping applications that need pointing and stabilizing camera or any other surveying sensor, require mounting the IMU beside the sensor on a Gimbal onboard the airframe. Hence the proposed Gimbal mounted IMU signal simulator is of interest whilst itself requires further analysis and verifications. Extended evaluation results in terms of both unit tests and functional/integration tests (using aided inertial navigation algorithms with variable/dynamic lever arms), verifies the simulator and its applicability for the mentioned tasks. We have further packaged and published our MATLAB code for the proposed simulator as an open source GitHub repository.</details> | http://arxiv.org/abs/2411.01038v1 |
| Mixed Reality Teleoperation Assistance for Direct Control of Humanoids | Luigi Penco; Kazuhiko Momose; Stephen McCrory; Dexton Anderson; Nicholas Kitchel; Duncan Calvert; Robert J. Griffin | 2024-11-01 | <details><summary>Click to expand</summary>Teleoperation plays a crucial role in enabling robot operations in challenging environments, yet existing limitations in effectiveness and accuracy necessitate the development of innovative strategies for improving teleoperated tasks. This article introduces a novel approach that utilizes mixed reality and assistive autonomy to enhance the efficiency and precision of humanoid robot teleoperation. By leveraging Probabilistic Movement Primitives, object detection, and Affordance Templates, the assistance combines user motion with autonomous capabilities, achieving task efficiency while maintaining human-like robot motion. Experiments and feasibility studies on the Nadia robot confirm the effectiveness of the proposed framework.</details> | http://arxiv.org/abs/2411.01014v1 |
| Active Learning-augmented Intention-aware Obstacle Avoidance of   Autonomous Surface Vehicles in High-traffic Waters | Mingi Jeong; Arihant Chadda; Alberto Quattrini Li | 2024-11-01 | <details><summary>Click to expand</summary>This paper enhances the obstacle avoidance of Autonomous Surface Vehicles (ASVs) for safe navigation in high-traffic waters with an active state estimation of obstacle's passing intention and reducing its uncertainty. We introduce a topological modeling of passing intention of obstacles, which can be applied to varying encounter situations based on the inherent embedding of topological concepts in COLREGs. With a Long Short-Term Memory (LSTM) neural network, we classify the passing intention of obstacles. Then, for determining the ASV maneuver, we propose a multi-objective optimization framework including information gain about the passing obstacle intention and safety. We validate the proposed approach under extensive Monte Carlo simulations (2,400 runs) with a varying number of obstacles, dynamic properties, encounter situations, and different behavioral patterns of obstacles (cooperative, non-cooperative). We also present the results from a real marine accident case study as well as real-world experiments of a real ASV with environmental disturbances, showing successful collision avoidance with our strategy in real-time.</details> | http://arxiv.org/abs/2411.01011v1 |
| Enhancing Model-Based Step Adaptation for Push Recovery through   Reinforcement Learning of Step Timing and Region | Tobias Egle; Yashuai Yan; Dongheui Lee; Christian Ott | 2024-11-01 | <details><summary>Click to expand</summary>This paper introduces a new approach to enhance the robustness of humanoid walking under strong perturbations, such as substantial pushes. Effective recovery from external disturbances requires bipedal robots to dynamically adjust their stepping strategies, including footstep positions and timing. Unlike most advanced walking controllers that restrict footstep locations to a predefined convex region, substantially limiting recoverable disturbances, our method leverages reinforcement learning to dynamically adjust the permissible footstep region, expanding it to a larger, effectively non-convex area and allowing cross-over stepping, which is crucial for counteracting large lateral pushes. Additionally, our method adapts footstep timing in real time to further extend the range of recoverable disturbances. Based on these adjustments, feasible footstep positions and DCM trajectory are planned by solving a QP. Finally, we employ a DCM controller and an inverse dynamics whole-body control framework to ensure the robot effectively follows the trajectory.</details> | http://arxiv.org/abs/2411.01000v1 |
| Raspberry PhenoSet: A Phenology-based Dataset for Automated Growth   Detection and Yield Estimation | Parham Jafary; Anna Bazangeya; Michelle Pham; Lesley G. Campbell; Sajad Saeedi; Kourosh Zareinia; Habiba Bougherara | 2024-11-01 | <details><summary>Click to expand</summary>The future of the agriculture industry is intertwined with automation. Accurate fruit detection, yield estimation, and harvest time estimation are crucial for optimizing agricultural practices. These tasks can be carried out by robots to reduce labour costs and improve the efficiency of the process. To do so, deep learning models should be trained to perform knowledge-based tasks, which outlines the importance of contributing valuable data to the literature. In this paper, we introduce Raspberry PhenoSet, a phenology-based dataset designed for detecting and segmenting raspberry fruit across seven developmental stages. To the best of our knowledge, Raspberry PhenoSet is the first fruit dataset to integrate biology-based classification with fruit detection tasks, offering valuable insights for yield estimation and precise harvest timing. This dataset contains 1,853 high-resolution images, the highest quality in the literature, captured under controlled artificial lighting in a vertical farm. The dataset has a total of 6,907 instances of mask annotations, manually labelled to reflect the seven phenology stages. We have also benchmarked Raspberry PhenoSet using several state-of-the-art deep learning models, including YOLOv8, YOLOv10, RT-DETR, and Mask R-CNN, to provide a comprehensive evaluation of their performance on the dataset. Our results highlight the challenges of distinguishing subtle phenology stages and underscore the potential of Raspberry PhenoSet for both deep learning model development and practical robotic applications in agriculture, particularly in yield prediction and supply chain management. The dataset and the trained models are publicly available for future studies.</details> | http://arxiv.org/abs/2411.00967v1 |
| SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation | Cheng-Chun Hsu; Bowen Wen; Jie Xu; Yashraj Narang; Xiaolong Wang; Yuke Zhu; Joydeep Biswas; Stan Birchfield | 2024-11-01 | <details><summary>Click to expand</summary>We introduce SPOT, an object-centric imitation learning framework. The key idea is to capture each task by an object-centric representation, specifically the SE(3) object pose trajectory relative to the target. This approach decouples embodiment actions from sensory inputs, facilitating learning from various demonstration types, including both action-based and action-less human hand demonstrations, as well as cross-embodiment generalization. Additionally, object pose trajectories inherently capture planning constraints from demonstrations without the need for manually crafted rules. To guide the robot in executing the task, the object trajectory is used to condition a diffusion policy. We show improvement compared to prior work on RLBench simulated tasks. In real-world evaluation, using only eight demonstrations shot on an iPhone, our approach completed all tasks while fully complying with task constraints. Project page: https://nvlabs.github.io/object_centric_diffusion</details> | http://arxiv.org/abs/2411.00965v1 |
| FG-PE: Factor-graph Approach for Multi-robot Pursuit-Evasion | Messiah Abolfazli Esfahani; Ayşe Başar; Sajad Saeedi | 2024-11-01 | <details><summary>Click to expand</summary>With the increasing use of robots in daily life, there is a growing need to provide robust collaboration protocols for robots to tackle more complicated and dynamic problems effectively. This paper presents a novel, factor graph-based approach to address the pursuit-evasion problem, enabling accurate estimation, planning, and tracking of an evader by multiple pursuers working together. It is assumed that there are multiple pursuers and only one evader in this scenario. The proposed method significantly improves the accuracy of evader estimation and tracking, allowing pursuers to capture the evader in the shortest possible time and distance compared to existing techniques. In addition to these primary objectives, the proposed approach effectively minimizes uncertainty while remaining robust, even when communication issues lead to some messages being dropped or lost. Through a series of comprehensive experiments, this paper demonstrates that the proposed algorithm consistently outperforms traditional pursuit-evasion methods across several key performance metrics, such as the time required to capture the evader and the average distance traveled by the pursuers. Additionally, the proposed method is tested in real-world hardware experiments, further validating its effectiveness and applicability.</details> | http://arxiv.org/abs/2411.00741v1 |
| Multi-Agent Deep Q-Network with Layer-based Communication Channel for   Autonomous Internal Logistics Vehicle Scheduling in Smart Manufacturing | Mohammad Feizabadi; Arman Hosseini; Zakaria Yahouni | 2024-11-01 | <details><summary>Click to expand</summary>In smart manufacturing, scheduling autonomous internal logistic vehicles is crucial for optimizing operational efficiency. This paper proposes a multi-agent deep Q-network (MADQN) with a layer-based communication channel (LBCC) to address this challenge. The main goals are to minimize total job tardiness, reduce the number of tardy jobs, and lower vehicle energy consumption. The method is evaluated against nine well-known scheduling heuristics, demonstrating its effectiveness in handling dynamic job shop behaviors like job arrivals and workstation unavailabilities. The approach also proves scalable, maintaining performance across different layouts and larger problem instances, highlighting the robustness and adaptability of MADQN with LBCC in smart manufacturing.</details> | http://arxiv.org/abs/2411.00728v1 |
| Learning to Look Around: Enhancing Teleoperation and Learning with a   Human-like Actuated Neck | Bipasha Sen; Michelle Wang; Nandini Thakur; Aditya Agarwal; Pulkit Agrawal | 2024-11-01 | <details><summary>Click to expand</summary>We introduce a teleoperation system that integrates a 5 DOF actuated neck, designed to replicate natural human head movements and perception. By enabling behaviors like peeking or tilting, the system provides operators with a more intuitive and comprehensive view of the environment, improving task performance, reducing cognitive load, and facilitating complex whole-body manipulation. We demonstrate the benefits of natural perception across seven challenging teleoperation tasks, showing how the actuated neck enhances the scope and efficiency of remote operation. Furthermore, we investigate its role in training autonomous policies through imitation learning. In three distinct tasks, the actuated neck supports better spatial awareness, reduces distribution shift, and enables adaptive task-specific adjustments compared to a static wide-angle camera.</details> | http://arxiv.org/abs/2411.00704v1 |
| Path Integral Control for Hybrid Dynamical Systems | Hongzhe Yu; Diana Frias Franco; Aaron M. Johnson; Yongxin Chen | 2024-11-01 | <details><summary>Click to expand</summary>This work introduces a novel paradigm for solving optimal control problems for hybrid dynamical systems under uncertainties. Robotic systems having contact with the environment can be modeled as hybrid systems. Controller design for hybrid systems under disturbances is complicated by the discontinuous jump dynamics, mode changes with inconsistent state dimensions, and variations in jumping timing and states caused by noise. We formulate this problem into a stochastic control problem with hybrid transition constraints and propose the Hybrid Path Integral (H-PI) framework to obtain the optimal controller. Despite random mode changes across stochastic path samples, we show that the ratio between hybrid path distributions with varying drift terms remains analogous to the smooth path distributions. We then show that the optimal controller can be obtained by evaluating a path integral with hybrid constraints. Importance sampling for path distributions with hybrid dynamics constraints is introduced to reduce the variance of the path integral evaluation, where we leverage the recently developed Hybrid iterative-Linear-Quadratic-Regulator (H-iLQR) controller to induce a hybrid path distribution proposal with low variance. The proposed method is validated through numerical experiments on various hybrid systems and extensive ablation studies. All the sampling processes are conducted in parallel on a Graphics Processing Unit (GPU).</details> | http://arxiv.org/abs/2411.00659v1 |
| Identification of Analytic Nonlinear Dynamical Systems with   Non-asymptotic Guarantees | Negin Musavi; Ziyao Guo; Geir Dullerud; Yingying Li | 2024-11-01 | <details><summary>Click to expand</summary>This paper focuses on the system identification of an important class of nonlinear systems: linearly parameterized nonlinear systems, which enjoys wide applications in robotics and other mechanical systems. We consider two system identification methods: least-squares estimation (LSE), which is a point estimation method; and set-membership estimation (SME), which estimates an uncertainty set that contains the true parameters. We provide non-asymptotic convergence rates for LSE and SME under i.i.d. control inputs and control policies with i.i.d. random perturbations, both of which are considered as non-active-exploration inputs. Compared with the counter-example based on piecewise-affine systems in the literature, the success of non-active exploration in our setting relies on a key assumption on the system dynamics: we require the system functions to be real-analytic. Our results, together with the piecewise-affine counter-example, reveal the importance of differentiability in nonlinear system identification through non-active exploration. Lastly, we numerically compare our theoretical bounds with the empirical performance of LSE and SME on a pendulum example and a quadrotor example.</details> | http://arxiv.org/abs/2411.00656v1 |
| HopTrack: A Real-time Multi-Object Tracking System for Embedded Devices | Xiang Li; Cheng Chen; Yuan-yao Lou; Mustafa Abdallah; Kwang Taik Kim; Saurabh Bagchi | 2024-11-01 | <details><summary>Click to expand</summary>Multi-Object Tracking (MOT) poses significant challenges in computer vision. Despite its wide application in robotics, autonomous driving, and smart manufacturing, there is limited literature addressing the specific challenges of running MOT on embedded devices. State-of-the-art MOT trackers designed for high-end GPUs often experience low processing rates (<11fps) when deployed on embedded devices. Existing MOT frameworks for embedded devices proposed strategies such as fusing the detector model with the feature embedding model to reduce inference latency or combining different trackers to improve tracking accuracy, but tend to compromise one for the other. This paper introduces HopTrack, a real-time multi-object tracking system tailored for embedded devices. Our system employs a novel discretized static and dynamic matching approach along with an innovative content-aware dynamic sampling technique to enhance tracking accuracy while meeting the real-time requirement. Compared with the best high-end GPU modified baseline Byte (Embed) and the best existing baseline on embedded devices MobileNet-JDE, HopTrack achieves a processing speed of up to 39.29 fps on NVIDIA AGX Xavier with a multi-object tracking accuracy (MOTA) of up to 63.12% on the MOT16 benchmark, outperforming both counterparts by 2.15% and 4.82%, respectively. Additionally, the accuracy improvement is coupled with the reduction in energy consumption (20.8%), power (5%), and memory usage (8%), which are crucial resources on embedded devices. HopTrack is also detector agnostic allowing the flexibility of plug-and-play.</details> | http://arxiv.org/abs/2411.00608v1 |
| On Deep Learning for Geometric and Semantic Scene Understanding Using   On-Vehicle 3D LiDAR | Li Li | 2024-11-01 | <details><summary>Click to expand</summary>3D LiDAR point cloud data is crucial for scene perception in computer vision, robotics, and autonomous driving. Geometric and semantic scene understanding, involving 3D point clouds, is essential for advancing autonomous driving technologies. However, significant challenges remain, particularly in improving the overall accuracy (e.g., segmentation accuracy, depth estimation accuracy, etc.) and efficiency of these systems. To address the challenge in terms of accuracy related to LiDAR-based tasks, we present DurLAR, the first high-fidelity 128-channel 3D LiDAR dataset featuring panoramic ambient (near infrared) and reflectivity imagery. To improve efficiency in 3D segmentation while ensuring the accuracy, we propose a novel pipeline that employs a smaller architecture, requiring fewer ground-truth annotations while achieving superior segmentation accuracy compared to contemporary approaches. To improve the segmentation accuracy, we introduce Range-Aware Pointwise Distance Distribution (RAPiD) features and the associated RAPiD-Seg architecture. All contributions have been accepted by peer-reviewed conferences, underscoring the advancements in both accuracy and efficiency in 3D LiDAR applications for autonomous driving. Full abstract: https://etheses.dur.ac.uk/15738/.</details> | http://arxiv.org/abs/2411.00600v1 |
| Differentiable Physics-based System Identification for Robotic   Manipulation of Elastoplastic Materials | Xintong Yang; Ze Ji; Yu-Kun Lai | 2024-11-01 | <details><summary>Click to expand</summary>Robotic manipulation of volumetric elastoplastic deformable materials, from foods such as dough to construction materials like clay, is in its infancy, largely due to the difficulty of modelling and perception in a high-dimensional space. Simulating the dynamics of such materials is computationally expensive. It tends to suffer from inaccurately estimated physics parameters of the materials and the environment, impeding high-precision manipulation. Estimating such parameters from raw point clouds captured by optical cameras suffers further from heavy occlusions. To address this challenge, this work introduces a novel Differentiable Physics-based System Identification (DPSI) framework that enables a robot arm to infer the physics parameters of elastoplastic materials and the environment using simple manipulation motions and incomplete 3D point clouds, aligning the simulation with the real world. Extensive experiments show that with only a single real-world interaction, the estimated parameters, Young's modulus, Poisson's ratio, yield stress and friction coefficients, can accurately simulate visually and physically realistic deformation behaviours induced by unseen and long-horizon manipulation motions. Additionally, the DPSI framework inherently provides physically intuitive interpretations for the parameters in contrast to black-box approaches such as deep neural networks.</details> | http://arxiv.org/abs/2411.00554v1 |
| 3D Equivariant Pose Regression via Direct Wigner-D Harmonics Prediction | Jongmin Lee; Minsu Cho | 2024-11-01 | <details><summary>Click to expand</summary>Determining the 3D orientations of an object in an image, known as single-image pose estimation, is a crucial task in 3D vision applications. Existing methods typically learn 3D rotations parametrized in the spatial domain using Euler angles or quaternions, but these representations often introduce discontinuities and singularities. SO(3)-equivariant networks enable the structured capture of pose patterns with data-efficient learning, but the parametrizations in spatial domain are incompatible with their architecture, particularly spherical CNNs, which operate in the frequency domain to enhance computational efficiency. To overcome these issues, we propose a frequency-domain approach that directly predicts Wigner-D coefficients for 3D rotation regression, aligning with the operations of spherical CNNs. Our SO(3)-equivariant pose harmonics predictor overcomes the limitations of spatial parameterizations, ensuring consistent pose estimation under arbitrary rotations. Trained with a frequency-domain regression loss, our method achieves state-of-the-art results on benchmarks such as ModelNet10-SO(3) and PASCAL3D+, with significant improvements in accuracy, robustness, and data efficiency.</details> | http://arxiv.org/abs/2411.00543v2 |
| Analyzing Multimodal Integration in the Variational Autoencoder from an   Information-Theoretic Perspective | Carlotta Langer; Yasmin Kim Georgie; Ilja Porohovoj; Verena Vanessa Hafner; Nihat Ay | 2024-11-01 | <details><summary>Click to expand</summary>Human perception is inherently multimodal. We integrate, for instance, visual, proprioceptive and tactile information into one experience. Hence, multimodal learning is of importance for building robotic systems that aim at robustly interacting with the real world. One potential model that has been proposed for multimodal integration is the multimodal variational autoencoder. A variational autoencoder (VAE) consists of two networks, an encoder that maps the data to a stochastic latent space and a decoder that reconstruct this data from an element of this latent space. The multimodal VAE integrates inputs from different modalities at two points in time in the latent space and can thereby be used as a controller for a robotic agent. Here we use this architecture and introduce information-theoretic measures in order to analyze how important the integration of the different modalities are for the reconstruction of the input data. Therefore we calculate two different types of measures, the first type is called single modality error and assesses how important the information from a single modality is for the reconstruction of this modality or all modalities. Secondly, the measures named loss of precision calculate the impact that missing information from only one modality has on the reconstruction of this modality or the whole vector. The VAE is trained via the evidence lower bound, which can be written as a sum of two different terms, namely the reconstruction and the latent loss. The impact of the latent loss can be weighted via an additional variable, which has been introduced to combat posterior collapse. Here we train networks with four different weighting schedules and analyze them with respect to their capabilities for multimodal integration.</details> | http://arxiv.org/abs/2411.00522v1 |
| CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural   Language Supervision | Gi-Cheon Kang; Junghyun Kim; Kyuhwan Shim; Jun Ki Lee; Byoung-Tak Zhang | 2024-11-01 | <details><summary>Click to expand</summary>This paper explores how non-experts can teach robots desired skills in their environments. We argue that natural language is an intuitive and accessible interface for robot learning. To this end, we investigate two key aspects: (1) how non-experts collect robotic data using natural language supervision and (2) how pre-trained vision-language models learn end-to-end policies directly from this supervision. We propose a data collection framework that collects robot demonstrations based on natural language supervision (e.g., "move forward") and further augments these demonstrations. Next, we introduce a model that learns language-conditioned policies from natural language supervision called CLIP-RT. Our model employs pre-trained CLIP models and learns to predict actions represented in language via contrastive imitation learning. We first train CLIP-RT on large-scale robotic data and then enable it to learn desired skills using data collected from our framework. CLIP-RT shows strong capabilities in acquiring novel manipulation skills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by 17% in average success rates, while using 7x fewer parameters (1B).</details> | http://arxiv.org/abs/2411.00508v1 |
| PlanScope: Learning to Plan Within Decision Scope Does Matter | Ren Xin; Jie Cheng; Jun Ma | 2024-11-01 | <details><summary>Click to expand</summary>In the context of autonomous driving, learning-based methods have been promising for the development of planning modules. During the training process of planning modules, directly minimizing the discrepancy between expert-driving logs and planning output is widely deployed. In general, driving logs consist of suddenly appearing obstacles or swiftly changing traffic signals, which typically necessitate swift and nuanced adjustments in driving maneuvers. Concurrently, future trajectories of the vehicles exhibit their long-term decisions, such as adhering to a reference lane or circumventing stationary obstacles. Due to the unpredictable influence of future events in driving logs, reasoning bias could be naturally introduced to learning based planning modules, which leads to a possible degradation of driving performance. To address this issue, we identify the decisions and their corresponding time horizons, and characterize a so-called decision scope by retaining decisions within derivable horizons only, to mitigate the effect of irrational behaviors caused by unpredictable events. This framework employs wavelet transformation based log preprocessing with an effective loss computation approach, rendering the planning model only sensitive to valuable decisions at the current state. Since frequency domain characteristics are extracted in conjunction with time domain features by wavelets, decision information across various frequency bands within the corresponding time horizon can be suitably captured. Furthermore, to achieve valuable decision learning, this framework leverages a transformer based decoder that incrementally generates the detailed profiles of future decisions over multiple steps. Our experiments demonstrate that our proposed method outperforms baselines in terms of driving scores with closed-loop evaluations on the nuPlan dataset.</details> | http://arxiv.org/abs/2411.00476v1 |
| ConceptFactory: Facilitate 3D Object Knowledge Annotation with Object   Conceptualization | Jianhua Sun; Yuxuan Li; Longfei Xu; Nange Wang; Jiude Wei; Yining Zhang; Cewu Lu | 2024-11-01 | <details><summary>Click to expand</summary>We present ConceptFactory, a novel scope to facilitate more efficient annotation of 3D object knowledge by recognizing 3D objects through generalized concepts (i.e. object conceptualization), aiming at promoting machine intelligence to learn comprehensive object knowledge from both vision and robotics aspects. This idea originates from the findings in human cognition research that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i) ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization, and ii) ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite. Our approach enables researchers to effortlessly acquire or customize extensive varieties of object knowledge to comprehensively study different object understanding tasks. We validate our idea on a wide range of benchmark tasks from both vision and robotics aspects with state-of-the-art algorithms, demonstrating the high quality and versatility of annotations provided by our approach. Our website is available at https://apeirony.github.io/ConceptFactory.</details> | http://arxiv.org/abs/2411.00448v1 |
| Expert-level protocol translation for self-driving labs | Yu-Zhe Shi; Fanxu Meng; Haofei Hou; Zhangqian Bi; Qiao Xu; Lecheng Ruan; Qining Wang | 2024-11-01 | <details><summary>Click to expand</summary>Recent development in Artificial Intelligence (AI) models has propelled their application in scientific discovery, but the validation and exploration of these discoveries require subsequent empirical experimentation. The concept of self-driving laboratories promises to automate and thus boost the experimental process following AI-driven discoveries. However, the transition of experimental protocols, originally crafted for human comprehension, into formats interpretable by machines presents significant challenges, which, within the context of specific expert domain, encompass the necessity for structured as opposed to natural language, the imperative for explicit rather than tacit knowledge, and the preservation of causality and consistency throughout protocol steps. Presently, the task of protocol translation predominantly requires the manual and labor-intensive involvement of domain experts and information technology specialists, rendering the process time-intensive. To address these issues, we propose a framework that automates the protocol translation process through a three-stage workflow, which incrementally constructs Protocol Dependence Graphs (PDGs) that approach structured on the syntax level, completed on the semantics level, and linked on the execution level. Quantitative and qualitative evaluations have demonstrated its performance at par with that of human experts, underscoring its potential to significantly expedite and democratize the process of scientific discovery by elevating the automation capabilities within self-driving laboratories.</details> | http://arxiv.org/abs/2411.00444v1 |
| NAMR-RRT: Neural Adaptive Motion Planning for Mobile Robots in Dynamic   Environments | Zhirui Sun; Bingyi Xia; Peijia Xie; Xiaoxiao Li; Jiankun Wang | 2024-11-01 | <details><summary>Click to expand</summary>Robots are increasingly deployed in dynamic and crowded environments, such as urban areas and shopping malls, where efficient and robust navigation is crucial. Traditional risk-based motion planning algorithms face challenges in such scenarios due to the lack of a well-defined search region, leading to inefficient exploration in irrelevant areas. While bi-directional and multi-directional search strategies can improve efficiency, they still result in significant unnecessary exploration. This article introduces the Neural Adaptive Multi-directional Risk-based Rapidly-exploring Random Tree (NAMR-RRT) to address these limitations. NAMR-RRT integrates neural network-generated heuristic regions to dynamically guide the exploration process, continuously refining the heuristic region and sampling rates during the planning process. This adaptive feature significantly enhances performance compared to neural-based methods with fixed heuristic regions and sampling rates. NAMR-RRT improves planning efficiency, reduces trajectory length, and ensures higher success by focusing the search on promising areas and continuously adjusting to environments. The experiment results from both simulations and real-world applications demonstrate the robustness and effectiveness of our proposed method in navigating dynamic environments. A website about this work is available at https://sites.google.com/view/namr-rrt.</details> | http://arxiv.org/abs/2411.00440v1 |
| PLATYPUS: Progressive Local Surface Estimator for Arbitrary-Scale Point   Cloud Upsampling | Donghyun Kim; Hyeonkyeong Kwon; Yumin Kim; Seong Jae Hwang | 2024-11-01 | <details><summary>Click to expand</summary>3D point clouds are increasingly vital for applications like autonomous driving and robotics, yet the raw data captured by sensors often suffer from noise and sparsity, creating challenges for downstream tasks. Consequently, point cloud upsampling becomes essential for improving density and uniformity, with recent approaches showing promise by projecting randomly generated query points onto the underlying surface of sparse point clouds. However, these methods often result in outliers, non-uniformity, and difficulties in handling regions with high curvature and intricate structures. In this work, we address these challenges by introducing the Progressive Local Surface Estimator (PLSE), which more effectively captures local features in complex regions through a curvature-based sampling technique that selectively targets high-curvature areas. Additionally, we incorporate a curriculum learning strategy that leverages the curvature distribution within the point cloud to naturally assess the sample difficulty, enabling curriculum learning on point cloud data for the first time. The experimental results demonstrate that our approach significantly outperforms existing methods, achieving high-quality, dense point clouds with superior accuracy and detail.</details> | http://arxiv.org/abs/2411.00432v1 |
| Closed-Loop Stability of a Lyapunov-Based Switching Attitude Controller   for Energy-Efficient Torque-Input-Selection During Flight | Francisco M. F. R. Gonçalves; Ryan M. Bena; Néstor O. Pérez-Arancibia | 2024-11-01 | <details><summary>Click to expand</summary>We present a new Lyapunov-based switching attitude controller for energy-efficient real-time selection of the torque inputted to an uncrewed aerial vehicle (UAV) during flight. The proposed method, using quaternions to describe the attitude of the controlled UAV, interchanges the stability properties of the two fixed points-one locally asymptotically stable and another unstable-of the resulting closed-loop (CL) switching dynamics of the system. In this approach, the switching events are triggered by the value of a compound energy-based function. To analyze and ensure the stability of the CL switching dynamics, we use classical nonlinear Lyapunov techniques, in combination with switching-systems theory. For this purpose, we introduce a new compound Lyapunov function (LF) that not only enables us to derive the conditions for CL asymptotic and exponential stability, but also provides us with an estimate of the CL system's region of attraction. This new estimate is considerably larger than those previously reported for systems of the type considered in this paper. To test and demonstrate the functionality, suitability, and performance of the proposed method, we present and discuss experimental data obtained using a 31-g quadrotor during the execution of high-speed yaw-tracking maneuvers. Also, we provide empirical evidence indicating that all the initial conditions chosen for these maneuvers, as estimated, lie inside the system's region of attraction. Last, experimental data obtained through these flight tests show that the proposed switching controller reduces the control effort by about 53%, on average, with respect to that corresponding to a commonly used benchmark control scheme, when executing a particular type of high-speed yaw-tracking maneuvers.</details> | http://arxiv.org/abs/2411.00417v1 |
| Multi-Uncertainty Aware Autonomous Cooperative Planning | Shiyao Zhang; He Li; Shengyu Zhang; Shuai Wang; Derrick Wing Kwan Ng; Chengzhong Xu | 2024-11-01 | <details><summary>Click to expand</summary>Autonomous cooperative planning (ACP) is a promising technique to improve the efficiency and safety of multi-vehicle interactions for future intelligent transportation systems. However, realizing robust ACP is a challenge due to the aggregation of perception, motion, and communication uncertainties. This paper proposes a novel multi-uncertainty aware ACP (MUACP) framework that simultaneously accounts for multiple types of uncertainties via regularized cooperative model predictive control (RC-MPC). The regularizers and constraints for perception, motion, and communication are constructed according to the confidence levels, weather conditions, and outage probabilities, respectively. The effectiveness of the proposed method is evaluated in the Car Learning to Act (CARLA) simulation platform. Results demonstrate that the proposed MUACP efficiently performs cooperative formation in real time and outperforms other benchmark approaches in various scenarios under imperfect knowledge of the environment.</details> | http://arxiv.org/abs/2411.00413v1 |
| Capability-aware Task Allocation and Team Formation Analysis for   Cooperative Exploration of Complex Environments | Muhammad Fadhil Ginting; Kyohei Otsu; Mykel J. Kochenderfer; Ali-akbar Agha-mohammadi | 2024-11-01 | <details><summary>Click to expand</summary>To achieve autonomy in complex real-world exploration missions, we consider deployment strategies for a team of robots with heterogeneous autonomy capabilities. In this work, we formulate a multi-robot exploration mission and compute an operation policy to maintain robot team productivity and maximize mission rewards. The environment description, robot capability, and mission outcome are modeled as a Markov decision process (MDP). We also include constraints in real-world operation, such as sensor failures, limited communication coverage, and mobility-stressing elements. Then, we study the proposed operation model on a real-world scenario in the context of the DARPA Subterranean (SubT) Challenge. The computed deployment policy is also compared against the human-based operation strategy in the final competition of the SubT Challenge. Finally, using the proposed model, we discuss the design trade-off on building a multi-robot team with heterogeneous capabilities.</details> | http://arxiv.org/abs/2411.00400v1 |
| Hierarchical Preference Optimization: Learning to achieve goals via   feasible subgoals prediction | Utsav Singh; Souradip Chakraborty; Wesley A. Suttle; Brian M. Sadler; Anit Kumar Sahu; Mubarak Shah; Vinay P. Namboodiri; Amrit Singh Bedi | 2024-11-01 | <details><summary>Click to expand</summary>This work introduces Hierarchical Preference Optimization (HPO), a novel approach to hierarchical reinforcement learning (HRL) that addresses non-stationarity and infeasible subgoal generation issues when solving complex robotic control tasks. HPO leverages maximum entropy reinforcement learning combined with token-level Direct Preference Optimization (DPO), eliminating the need for pre-trained reference policies that are typically unavailable in challenging robotic scenarios. Mathematically, we formulate HRL as a bi-level optimization problem and transform it into a primitive-regularized DPO formulation, ensuring feasible subgoal generation and avoiding degenerate solutions. Extensive experiments on challenging robotic navigation and manipulation tasks demonstrate impressive performance of HPO, where it shows an improvement of up to 35% over the baselines. Furthermore, ablation studies validate our design choices, and quantitative analyses confirm the ability of HPO to mitigate non-stationarity and infeasible subgoal generation issues in HRL.</details> | http://arxiv.org/abs/2411.00361v1 |
| An Improved Rapidly Exploring Random Tree Algorithm for Path Planning in   Configuration Spaces with Narrow Channels | Mathew Mithra Noel; Akshay Chawla | 2024-11-01 | <details><summary>Click to expand</summary>Rapidly-exploring Random Tree (RRT) algorithms have been applied successfully to challenging robot motion planning and under-actuated nonlinear control problems. However a fundamental limitation of the RRT approach is the slow convergence in configuration spaces with narrow channels because of the small probability of generating test points inside narrow channels. This paper presents an improved RRT algorithm that takes advantage of narrow channels between the initial and goal states to find shorter paths by improving the exploration of narrow regions in the configuration space. The proposed algorithm detects the presence of narrow channel by checking for collision of neighborhood points with the infeasible set and attempts to add points within narrow channels with a predetermined bias. This approach is compared with the classical RRT and its variants on a variety of benchmark planning problems. Simulation results indicate that the algorithm presented in this paper computes a significantly shorter path in spaces with narrow channels.</details> | http://arxiv.org/abs/2411.00357v1 |
| An Untethered Bioinspired Robotic Tensegrity Dolphin with   Multi-Flexibility Design for Aquatic Locomotion | Luyang Zhao; Yitao Jiang; Chun-Yi She; Mingi Jeong; Haibo Dong; Alberto Quattrini Li; Muhao Chen; Devin Balkcom | 2024-11-01 | <details><summary>Click to expand</summary>This paper presents the first steps toward a soft dolphin robot using a bio-inspired approach to mimic dolphin flexibility. The current dolphin robot uses a minimalist approach, with only two actuated cable-driven degrees of freedom actuated by a pair of motors. The actuated tail moves up and down in a swimming motion, but this first proof of concept does not permit controlled turns of the robot. While existing robotic dolphins typically use revolute joints to articulate rigid bodies, our design -- which will be made opensource -- incorporates a flexible tail with tunable silicone skin and actuation flexibility via a cable-driven system, which mimics muscle dynamics and design flexibility with a tunable skeleton structure. The design is also tunable since the backbone can be easily printed in various geometries. The paper provides insights into how a few such variations affect robot motion and efficiency, measured by speed and cost of transport (COT). This approach demonstrates the potential of achieving dolphin-like motion through enhanced flexibility in bio-inspired robotics.</details> | http://arxiv.org/abs/2411.00347v1 |
| On the Exploration of LM-Based Soft Modular Robot Design | Weicheng Ma; Luyang Zhao; Chun-Yi She; Yitao Jiang; Alan Sun; Bo Zhu; Devin Balkcom; Soroush Vosoughi | 2024-11-01 | <details><summary>Click to expand</summary>Recent large language models (LLMs) have demonstrated promising capabilities in modeling real-world knowledge and enhancing knowledge-based generation tasks. In this paper, we further explore the potential of using LLMs to aid in the design of soft modular robots, taking into account both user instructions and physical laws, to reduce the reliance on extensive trial-and-error experiments typically needed to achieve robot designs that meet specific structural or task requirements. Specifically, we formulate the robot design process as a sequence generation task and find that LLMs are able to capture key requirements expressed in natural language and reflect them in the construction sequences of robots. To simplify, rather than conducting real-world experiments to assess design quality, we utilize a simulation tool to provide feedback to the generative model, allowing for iterative improvements without requiring extensive human annotations. Furthermore, we introduce five evaluation metrics to assess the quality of robot designs from multiple angles including task completion and adherence to instructions, supporting an automatic evaluation process. Our model performs well in evaluations for designing soft modular robots with uni- and bi-directional locomotion and stair-descending capabilities, highlighting the potential of using natural language and LLMs for robot design. However, we also observe certain limitations that suggest areas for further improvement.</details> | http://arxiv.org/abs/2411.00345v1 |
| Parameter Estimation on Homogeneous Spaces | Shiraz Khan; Gregory S. Chirikjian | 2024-10-31 | <details><summary>Click to expand</summary>The Fisher Information Metric (FIM) and the associated Cram\'er-Rao Bound (CRB) are fundamental tools in statistical signal processing, which inform the efficient design of experiments and algorithms for estimating the underlying parameters. In this article, we investigate these concepts for the case where the parameters lie on a homogeneous space. Unlike the existing Fisher-Rao theory for general Riemannian manifolds, our focus is to leverage the group-theoretic structure of homogeneous spaces, which is often much easier to work with than their Riemannian structure. The FIM is characterized by identifying the homogeneous space with a coset space, the group-theoretic CRB and its corollaries are presented, and its relationship to the Riemannian CRB is clarified. The application of our theory is illustrated using two examples from engineering: (i) estimation of the pose of a robot and (ii) sensor network localization. In particular, these examples demonstrate that homogeneous spaces provide a natural framework for studying statistical models that are invariant with respect to a group of symmetries.</details> | http://arxiv.org/abs/2411.00258v1 |
| A Fast and Model Based Approach for Evaluating Task-Competence of   Antagonistic Continuum Arms | Bill Fan; Jacob Roulier; Gina Olson | 2024-10-31 | <details><summary>Click to expand</summary>Soft robot arms have made significant progress towards completing human-scale tasks, but designing arms for tasks with specific load and workspace requirements remains difficult. A key challenge is the lack of model-based design tools, forcing advancement to occur through empirical iteration and observation. Existing models are focused on control and rely on parameter fits, which means they cannot provide general conclusions about the mapping between design and performance or the influence of factors outside the fitting data. As a first step toward model-based design tools, we introduce a novel method of analyzing whether a proposed arm design can complete desired tasks. Our method is informative, interpretable, and fast; it provides novel metrics for quantifying a proposed arm design's ability to perform a task, it yields a graphical interpretation of performance through segment forces, and computing it is over 80x faster than optimization based methods. Our formulation focuses on antagonistic, pneumatically-driven soft arms. We demonstrate our approach through example analysis, and also through consideration of antagonistic vs non-antagonistic designs. Our method enables fast, direct and task-specific comparison of these two architectures, and provides a new visualization of the comparative mechanics. While only a first step, the proposed approach will support advancement of model-based design tools, leading to highly capable soft arms.</details> | http://arxiv.org/abs/2411.00241v2 |
| BOMP: Bin-Optimized Motion Planning | Zachary Tam; Karthik Dharmarajan; Tianshuang Qiu; Yahav Avigal; Jeffrey Ichnowski; Ken Goldberg | 2024-10-31 | <details><summary>Click to expand</summary>In logistics, the ability to quickly compute and execute pick-and-place motions from bins is critical to increasing productivity. We present Bin-Optimized Motion Planning (BOMP), a motion planning framework that plans arm motions for a six-axis industrial robot with a long-nosed suction tool to remove boxes from deep bins. BOMP considers robot arm kinematics, actuation limits, the dimensions of a grasped box, and a varying height map of a bin environment to rapidly generate time-optimized, jerk-limited, and collision-free trajectories. The optimization is warm-started using a deep neural network trained offline in simulation with 25,000 scenes and corresponding trajectories. Experiments with 96 simulated and 15 physical environments suggest that BOMP generates collision-free trajectories that are up to 58 % faster than baseline sampling-based planners and up to 36 % faster than an industry-standard Up-Over-Down algorithm, which has an extremely low 15 % success rate in this context. BOMP also generates jerk-limited trajectories while baselines do not. Website: https://sites.google.com/berkeley.edu/bomp.</details> | http://arxiv.org/abs/2411.00221v1 |
| Pedestrian Trajectory Prediction with Missing Data: Datasets,   Imputation, and Benchmarking | Pranav Singh Chib; Pravendra Singh | 2024-10-31 | <details><summary>Click to expand</summary>Pedestrian trajectory prediction is crucial for several applications such as robotics and self-driving vehicles. Significant progress has been made in the past decade thanks to the availability of pedestrian trajectory datasets, which enable trajectory prediction methods to learn from pedestrians' past movements and predict future trajectories. However, these datasets and methods typically assume that the observed trajectory sequence is complete, ignoring real-world issues such as sensor failure, occlusion, and limited fields of view that can result in missing values in observed trajectories. To address this challenge, we present TrajImpute, a pedestrian trajectory prediction dataset that simulates missing coordinates in the observed trajectory, enhancing real-world applicability. TrajImpute maintains a uniform distribution of missing data within the observed trajectories. In this work, we comprehensively examine several imputation methods to reconstruct the missing coordinates and benchmark them for imputing pedestrian trajectories. Furthermore, we provide a thorough analysis of recent trajectory prediction methods and evaluate the performance of these models on the imputed trajectories. Our experimental evaluation of the imputation and trajectory prediction methods offers several valuable insights. Our dataset provides a foundational resource for future research on imputation-aware pedestrian trajectory prediction, potentially accelerating the deployment of these methods in real-world applications. Publicly accessible links to the datasets and code files are available at https://github.com/Pranav-chib/TrajImpute.</details> | http://arxiv.org/abs/2411.00174v1 |
| Residual Deep Gaussian Processes on Manifolds | Kacper Wyrwal; Andreas Krause; Viacheslav Borovitskiy | 2024-10-31 | <details><summary>Click to expand</summary>We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks. With manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields. We target data inherently supported on manifolds, which is too complex for shallow Gaussian processes thereon. For example, while the latter perform well on high-altitude wind data, they struggle with the more intricate, nonstationary patterns at low altitudes. Our models significantly improve performance in these settings, enhancing prediction quality and uncertainty calibration, and remain robust to overfitting, reverting to shallow models when additional complexity is unneeded. We further showcase our models on Bayesian optimisation problems on manifolds, using stylised examples motivated by robotics, and obtain substantial improvements in later stages of the optimisation process. Finally, we show our models to have potential for speeding up inference for non-manifold data, when, and if, it can be mapped to a proxy manifold well enough.</details> | http://arxiv.org/abs/2411.00161v1 |
| Learning Low-Dimensional Strain Models of Soft Robots by Looking at the   Evolution of Their Shape with Application to Model-Based Control | Ricardo Valadas; Maximilian Stölzle; Jingyue Liu; Cosimo Della Santina | 2024-10-31 | <details><summary>Click to expand</summary>Obtaining dynamic models of continuum soft robots is central to the analysis and control of soft robots, and researchers have devoted much attention to the challenge of proposing both data-driven and first-principle solutions. Both avenues have, however, shown their limitations; the former lacks structure and performs poorly outside training data, while the latter requires significant simplifications and extensive expert knowledge to be used in practice. This paper introduces a streamlined method for learning low-dimensional, physics-based models that are both accurate and easy to interpret. We start with an algorithm that uses image data (i.e., shape evolutions) to determine the minimal necessary segments for describing a soft robot's movement. Following this, we apply a dynamic regression and strain sparsification algorithm to identify relevant strains and define the model's dynamics. We validate our approach through simulations with various planar soft manipulators, comparing its performance against other learning strategies, showing that our models are both computationally efficient and 25x more accurate on out-of-training distribution inputs. Finally, we demonstrate that thanks to the capability of the method of generating physically compatible models, the learned models can be straightforwardly combined with model-based control policies.</details> | http://arxiv.org/abs/2411.00138v1 |
| Cost-Aware Query Policies in Active Learning for Efficient Autonomous   Robotic Exploration | Sapphira Akins; Hans Mertens; Frances Zhu | 2024-10-31 | <details><summary>Click to expand</summary>In missions constrained by finite resources, efficient data collection is critical. Informative path planning, driven by automated decision-making, optimizes exploration by reducing the costs associated with accurate characterization of a target in an environment. Previous implementations of active learning did not consider the action cost for regression problems or only considered the action cost for classification problems. This paper analyzes an AL algorithm for Gaussian Process regression while incorporating action cost. The algorithm's performance is compared on various regression problems to include terrain mapping on diverse simulated surfaces along metrics of root mean square error, samples and distance until convergence, and model variance upon convergence. The cost-dependent acquisition policy doesn't organically optimize information gain over distance. Instead, the traditional uncertainty metric with a distance constraint best minimizes root-mean-square error over trajectory distance. This studys impact is to provide insight into incorporating action cost with AL methods to optimize exploration under realistic mission constraints.</details> | http://arxiv.org/abs/2411.00137v1 |
| First, Learn What You Don't Know: Active Information Gathering for   Driving at the Limits of Handling | Alexander Davydov; Franck Djeumou; Marcus Greiff; Makoto Suminaka; Michael Thompson; John Subosits; Thomas Lew | 2024-10-31 | <details><summary>Click to expand</summary>Combining data-driven models that adapt online and model predictive control (MPC) has enabled effective control of nonlinear systems. However, when deployed on unstable systems, online adaptation may not be fast enough to ensure reliable simultaneous learning and control. For example, controllers on a vehicle executing highly dynamic maneuvers may push the tires to their friction limits, destabilizing the vehicle and allowing modeling errors to quickly compound and cause a loss of control. In this work, we present a Bayesian meta-learning MPC framework. We propose an expressive vehicle dynamics model that leverages Bayesian last-layer meta-learning to enable rapid online adaptation. The model's uncertainty estimates are used to guide informative data collection and quickly improve the model prior to deployment. Experiments on a Toyota Supra show that (i) the framework enables reliable control in dynamic drifting maneuvers, (ii) online adaptation alone may not suffice for zero-shot control of a vehicle at the edge of stability, and (iii) active data collection helps achieve reliable performance.</details> | http://arxiv.org/abs/2411.00107v1 |
| Tensegrity Robot Proprioceptive State Estimation with Geometric   Constraints | Wenzhe Tong; Tzu-Yuan Lin; Jonathan Mi; Yicheng Jiang; Maani Ghaffari; Xiaonan Huang | 2024-10-31 | <details><summary>Click to expand</summary>Tensegrity robots, characterized by a synergistic assembly of rigid rods and elastic cables, form robust structures that are resistant to impacts. However, this design introduces complexities in kinematics and dynamics, complicating control and state estimation. This work presents a novel proprioceptive state estimator for tensegrity robots. The estimator initially uses the geometric constraints of 3-bar prism tensegrity structures, combined with IMU and motor encoder measurements, to reconstruct the robot's shape and orientation. It then employs a contact-aided invariant extended Kalman filter with forward kinematics to estimate the global position and orientation of the tensegrity robot. The state estimator's accuracy is assessed against ground truth data in both simulated environments and real-world tensegrity robot applications. It achieves an average drift percentage of 4.2%, comparable to the state estimation performance of traditional rigid robots. This state estimator advances the state of the art in tensegrity robot state estimation and has the potential to run in real-time using onboard sensors, paving the way for full autonomy of tensegrity robots in unstructured environments.</details> | http://arxiv.org/abs/2410.24226v1 |
| EgoMimic: Scaling Imitation Learning via Egocentric Video | Simar Kareer; Dhruv Patel; Ryan Punamiya; Pranay Mathur; Shuo Cheng; Chen Wang; Judy Hoffman; Danfei Xu | 2024-10-31 | <details><summary>Click to expand</summary>The scale and diversity of demonstration data required for imitation learning is a significant challenge. We present EgoMimic, a full-stack framework which scales manipulation via human embodiment data, specifically egocentric human videos paired with 3D hand tracking. EgoMimic achieves this through: (1) a system to capture human embodiment data using the ergonomic Project Aria glasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap to human data, (3) cross-domain data alignment techniques, and (4) an imitation learning architecture that co-trains on human and robot data. Compared to prior works that only extract high-level intent from human videos, our approach treats human and robot data equally as embodied demonstration data and learns a unified policy from both data sources. EgoMimic achieves significant improvement on a diverse set of long-horizon, single-arm and bimanual manipulation tasks over state-of-the-art imitation learning methods and enables generalization to entirely new scenes. Finally, we show a favorable scaling trend for EgoMimic, where adding 1 hour of additional hand data is significantly more valuable than 1 hour of additional robot data. Videos and additional information can be found at https://egomimic.github.io/</details> | http://arxiv.org/abs/2410.24221v1 |
| Teaching Embodied Reinforcement Learning Agents: Informativeness and   Diversity of Language Use | Jiajun Xi; Yinong He; Jianing Yang; Yinpei Dai; Joyce Chai | 2024-10-31 | <details><summary>Click to expand</summary>In real-world scenarios, it is desirable for embodied agents to have the ability to leverage human language to gain explicit or implicit knowledge for learning tasks. Despite recent progress, most previous approaches adopt simple low-level instructions as language inputs, which may not reflect natural human communication. It's not clear how to incorporate rich language use to facilitate task learning. To address this question, this paper studies different types of language inputs in facilitating reinforcement learning (RL) embodied agents. More specifically, we examine how different levels of language informativeness (i.e., feedback on past behaviors and future guidance) and diversity (i.e., variation of language expressions) impact agent learning and inference. Our empirical results based on four RL benchmarks demonstrate that agents trained with diverse and informative language feedback can achieve enhanced generalization and fast adaptation to new tasks. These findings highlight the pivotal role of language use in teaching embodied agents new tasks in an open world. Project website: https://github.com/sled-group/Teachable_RL</details> | http://arxiv.org/abs/2410.24218v1 |
| Learning Visual Parkour from Generated Images | Alan Yu; Ge Yang; Ran Choi; Yajvan Ravan; John Leonard; Phillip Isola | 2024-10-31 | <details><summary>Click to expand</summary>Fast and accurate physics simulation is an essential component of robot learning, where robots can explore failure scenarios that are difficult to produce in the real world and learn from unlimited on-policy data. Yet, it remains challenging to incorporate RGB-color perception into the sim-to-real pipeline that matches the real world in its richness and realism. In this work, we train a robot dog in simulation for visual parkour. We propose a way to use generative models to synthesize diverse and physically accurate image sequences of the scene from the robot's ego-centric perspective. We present demonstrations of zero-shot transfer to the RGB-only observations of the real world on a robot equipped with a low-cost, off-the-shelf color camera. website visit https://lucidsim.github.io</details> | http://arxiv.org/abs/2411.00083v1 |
| Zonal RL-RRT: Integrated RL-RRT Path Planning with Collision Probability   and Zone Connectivity | AmirMohammad Tahmasbi; MohammadSaleh Faghfoorian; Saeed Khodaygan; Aniket Bera | 2024-10-31 | <details><summary>Click to expand</summary>Path planning in high-dimensional spaces poses significant challenges, particularly in achieving both time efficiency and a fair success rate. To address these issues, we introduce a novel path-planning algorithm, Zonal RL-RRT, that leverages kd-tree partitioning to segment the map into zones while addressing zone connectivity, ensuring seamless transitions between zones. By breaking down the complex environment into multiple zones and using Q-learning as the high-level decision-maker, our algorithm achieves a 3x improvement in time efficiency compared to basic sampling methods such as RRT and RRT* in forest-like maps. Our approach outperforms heuristic-guided methods like BIT* and Informed RRT* by 1.5x in terms of runtime while maintaining robust and reliable success rates across 2D to 6D environments. Compared to learning-based methods like NeuralRRT* and MPNetSMP, as well as the heuristic RRT*J, our algorithm demonstrates, on average, 1.5x better performance in the same environments. We also evaluate the effectiveness of our approach through simulations of the UR10e arm manipulator in the MuJoCo environment. A key observation of our approach lies in its use of zone partitioning and Reinforcement Learning (RL) for adaptive high-level planning allowing the algorithm to accommodate flexible policies across diverse environments, making it a versatile tool for advanced path planning.</details> | http://arxiv.org/abs/2410.24205v1 |
| DiffPano: Scalable and Consistent Text to Panorama Generation with   Spherical Epipolar-Aware Diffusion | Weicai Ye; Chenhao Ji; Zheng Chen; Junyao Gao; Xiaoshui Huang; Song-Hai Zhang; Wanli Ouyang; Tong He; Cairong Zhao; Guofeng Zhang | 2024-10-31 | <details><summary>Click to expand</summary>Diffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even $360^{\circ}$ images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images. To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions. Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset. We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images. Extensive experiments demonstrate that DiffPano can generate scalable, consistent, and diverse panoramic images with given unseen text descriptions and camera poses.</details> | http://arxiv.org/abs/2410.24203v1 |
| A Sagittal Planar Ankle-Foot Prosthesis with Powered Plantarflexion and   Socket Alignment | Mark A. Price; Frank C. Sup IV | 2024-10-31 | <details><summary>Click to expand</summary>Powered ankle-foot prostheses can often reduce the energy cost of walking by assisting with push-off. However, focus on providing mechanical work may lead to ignoring or exacerbating common issues with chronic pain, irritation, pressure ulcer development, and eventual osteoarthritis in persons with amputation. This paper presents the design and validation of a novel transtibial prosthesis informed by predictive biomechanical simulations of gait which minimize a combination of user effort and interaction loading from the prosthesis socket. From these findings, the device was designed with a non-biomimetic anterior-posterior translation degree of freedom with a 10 cm range of motion which is primarily position-controlled to change the alignment of the prosthetic foot with the residual limb. The system is both mobile and tethered, with the batteries, actuators, and majority of electronics located in a small backpack. Mechanical loads are transmitted through cables to the prosthesis, minimizing the distal mass carriage required. We measured torque and force sensing accuracy, open loop actuator performance, closed loop torque and position control bandwidth, and torque and position tracking error during walking. The system is capable of producing up to 160 N-m of plantarflexion torque and 394 N of AP translation force with a closed loop control bandwidth of about 7 Hz in both degrees of freedom. Torque tracking during walking was accurate within about 10 N-m but position tracking was substantially affected by phase lag, possibly due to cable slack in the bidirectional mechanism. The prototype was capable of replicating our simulated prosthesis dynamics during gait and offers useful insights into the advantages and the practical considerations of using predictive biomechanical simulation as a design tool for wearable robots.</details> | http://arxiv.org/abs/2410.24196v1 |
| PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent   Tasks | Matthew Chang; Gunjan Chhablani; Alexander Clegg; Mikael Dallaire Cote; Ruta Desai; Michal Hlavac; Vladimir Karashchuk; Jacob Krantz; Roozbeh Mottaghi; Priyam Parashar; Siddharth Patki; Ishita Prasad; Xavier Puig; Akshara Rai; Ram Ramrakhya; Daniel Tran; Joanne Truong; John M. Turner; Eric Undersander; Tsung-Yen Yang | 2024-10-31 | <details><summary>Click to expand</summary>We present a benchmark for Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR) designed to study human-robot coordination in household activities. PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We employ a semi-automated task generation pipeline using Large Language Models (LLMs), incorporating simulation in the loop for grounding and verification. PARTNR stands as the largest benchmark of its kind, comprising 100,000 natural language tasks, spanning 60 houses and 5,819 unique objects. We analyze state-of-the-art LLMs on PARTNR tasks, across the axes of planning, perception and skill execution. The analysis reveals significant limitations in SoTA models, such as poor coordination and failures in task tracking and recovery from errors. When LLMs are paired with real humans, they require 1.5x as many steps as two humans collaborating and 1.1x more steps than a single human, underscoring the potential for improvement in these models. We further show that fine-tuning smaller LLMs with planning data can achieve performance on par with models 9 times larger, while being 8.6x faster at inference. Overall, PARTNR highlights significant challenges facing collaborative embodied agents and aims to drive research in this direction.</details> | http://arxiv.org/abs/2411.00081v1 |
| DexMimicGen: Automated Data Generation for Bimanual Dexterous   Manipulation via Imitation Learning | Zhenyu Jiang; Yuqi Xie; Kevin Lin; Zhenjia Xu; Weikang Wan; Ajay Mandlekar; Linxi Fan; Yuke Zhu | 2024-10-31 | <details><summary>Click to expand</summary>Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the amount of cost and human effort involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the challenges of simultaneously controlling multiple arms and multi-fingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-to-sim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Videos and more are at https://dexmimicgen.github.io/</details> | http://arxiv.org/abs/2410.24185v1 |
| $π_0$: A Vision-Language-Action Flow Model for General Robot Control | Kevin Black; Noah Brown; Danny Driess; Adnan Esmail; Michael Equi; Chelsea Finn; Niccolo Fusai; Lachy Groom; Karol Hausman; Brian Ichter; Szymon Jakubczak; Tim Jones; Liyiming Ke; Sergey Levine; Adrian Li-Bell; Mohith Mothukuri; Suraj Nair; Karl Pertsch; Lucy Xiaoyang Shi; James Tanner; Quan Vuong; Anna Walling; Haohuan Wang; Ury Zhilinsky | 2024-10-31 | <details><summary>Click to expand</summary>Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.</details> | http://arxiv.org/abs/2410.24164v2 |
| Language-Driven Policy Distillation for Cooperative Driving in   Multi-Agent Reinforcement Learning | Jiaqi Liu; Chengkai Xu; Peng Hang; Jian Sun; Mingyu Ding; Wei Zhan; Masayoshi Tomizuka | 2024-10-31 | <details><summary>Click to expand</summary>The cooperative driving technology of Connected and Autonomous Vehicles (CAVs) is crucial for improving the efficiency and safety of transportation systems. Learning-based methods, such as Multi-Agent Reinforcement Learning (MARL), have demonstrated strong capabilities in cooperative decision-making tasks. However, existing MARL approaches still face challenges in terms of learning efficiency and performance. In recent years, Large Language Models (LLMs) have rapidly advanced and shown remarkable abilities in various sequential decision-making tasks. To enhance the learning capabilities of cooperative agents while ensuring decision-making efficiency and cost-effectiveness, we propose LDPD, a language-driven policy distillation method for guiding MARL exploration. In this framework, a teacher agent based on LLM trains smaller student agents to achieve cooperative decision-making through its own decision-making demonstrations. The teacher agent enhances the observation information of CAVs and utilizes LLMs to perform complex cooperative decision-making reasoning, which also leverages carefully designed decision-making tools to achieve expert-level decisions, providing high-quality teaching experiences. The student agent then refines the teacher's prior knowledge into its own model through gradient policy updates. The experiments demonstrate that the students can rapidly improve their capabilities with minimal guidance from the teacher and eventually surpass the teacher's performance. Extensive experiments show that our approach demonstrates better performance and learning efficiency compared to baseline methods.</details> | http://arxiv.org/abs/2410.24152v1 |
| Two-Phase Switched Reluctance Motors: Optimal Magnet Placement and Drive   System for Torque Density and Efficiency Enhancement | Gholamreza Davarpanah; Sajjad Mohammadi; James L. Kirtley | 2024-10-31 | <details><summary>Click to expand</summary>This paper focuses on designing new motors with high torque density, which is crucial for applications ranging from electric vehicles to robotics. We propose a double-teeth C-core switched reluctance motor with hybrid excitation, integrating permanent magnets and a novel drive technique to enhance motor torque density. We explore three magnet placement configurations to maximize torque. A common challenge with most self-starting methods used in two-phase SRMs is the generation of negative torque, which reduces the motor's torque density. Our adopted self-starting method minimizes negative torque, and we introduce a new drive strategy to control the switching on and off, effectively eliminating negative torque. Additionally, magnetic equivalent circuits are developed for the analytical design and theoretical analysis of all configurations. The SRMs under study are prototyped and tested, and their performances are evaluated in terms of torque-angle characteristics, current, and voltage. Both experimental and simulation results validate the effectiveness of the PM-assisted SRMs in enhancing torque density and efficiency.</details> | http://arxiv.org/abs/2410.24121v1 |
| 3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing | Binghao Huang; Yixuan Wang; Xinyi Yang; Yiyue Luo; Yunzhu Li | 2024-10-31 | <details><summary>Click to expand</summary>Tactile and visual perception are both crucial for humans to perform fine-grained interactions with their environment. Developing similar multi-modal sensing capabilities for robots can significantly enhance and expand their manipulation skills. This paper introduces \textbf{3D-ViTac}, a multi-modal sensing and learning system designed for dexterous bimanual manipulation. Our system features tactile sensors equipped with dense sensing units, each covering an area of 3$mm^2$. These sensors are low-cost and flexible, providing detailed and extensive coverage of physical contacts, effectively complementing visual information. To integrate tactile and visual data, we fuse them into a unified 3D representation space that preserves their 3D structures and spatial relationships. The multi-modal representation can then be coupled with diffusion policies for imitation learning. Through concrete hardware experiments, we demonstrate that even low-cost robots can perform precise manipulations and significantly outperform vision-only policies, particularly in safe interactions with fragile items and executing long-horizon tasks involving in-hand manipulation. Our project page is available at \url{https://binghao-huang.github.io/3D-ViTac/}.</details> | http://arxiv.org/abs/2410.24091v1 |
| Sparsh: Self-supervised touch representations for vision-based tactile   sensing | Carolina Higuera; Akash Sharma; Chaithanya Krishna Bodduluri; Taosha Fan; Patrick Lancaster; Mrinal Kalakrishnan; Michael Kaess; Byron Boots; Mike Lambeta; Tingfan Wu; Mustafa Mukadam | 2024-10-31 | <details><summary>Click to expand</summary>In this work, we introduce general purpose touch representations for the increasingly accessible class of vision-based tactile sensors. Such sensors have led to many recent advances in robot manipulation as they markedly complement vision, yet solutions today often rely on task and sensor specific handcrafted perception models. Collecting real data at scale with task centric ground truth labels, like contact forces and slip, is a challenge further compounded by sensors of various form factor differing in aspects like lighting and gel markings. To tackle this we turn to self-supervised learning (SSL) that has demonstrated remarkable performance in computer vision. We present Sparsh, a family of SSL models that can support various vision-based tactile sensors, alleviating the need for custom labels through pre-training on 460k+ tactile images with masking and self-distillation in pixel and latent spaces. We also build TacBench, to facilitate standardized benchmarking across sensors and models, comprising of six tasks ranging from comprehending tactile properties to enabling physical perception and manipulation planning. In evaluations, we find that SSL pre-training for touch representation outperforms task and sensor-specific end-to-end training by 95.1% on average over TacBench, and Sparsh (DINO) and Sparsh (IJEPA) are the most competitive, indicating the merits of learning in latent space for tactile images. Project page: https://sparsh-ssl.github.io/</details> | http://arxiv.org/abs/2410.24090v1 |
| State- and context-dependent robotic manipulation and grasping via   uncertainty-aware imitation learning | Tim R. Winter; Ashok M. Sundaram; Werner Friedl; Maximo A. Roa; Freek Stulp; João Silvério | 2024-10-31 | <details><summary>Click to expand</summary>Generating context-adaptive manipulation and grasping actions is a challenging problem in robotics. Classical planning and control algorithms tend to be inflexible with regard to parameterization by external variables such as object shapes. In contrast, Learning from Demonstration (LfD) approaches, due to their nature as function approximators, allow for introducing external variables to modulate policies in response to the environment. In this paper, we utilize this property by introducing an LfD approach to acquire context-dependent grasping and manipulation strategies. We treat the problem as a kernel-based function approximation, where the kernel inputs include generic context variables describing task-dependent parameters such as the object shape. We build on existing work on policy fusion with uncertainty quantification to propose a state-dependent approach that automatically returns to demonstrations, avoiding unpredictable behavior while smoothly adapting to context changes. The approach is evaluated against the LASA handwriting dataset and on a real 7-DoF robot in two scenarios: adaptation to slippage while grasping and manipulating a deformable food item.</details> | http://arxiv.org/abs/2410.24035v1 |
| UAV-based detection of landmines using infrared thermography | Muhammad Umair Akram Butt; Zaighum Naveed; Usama Javed | 2024-10-31 | <details><summary>Click to expand</summary>Landmines remain a pervasive threat in conflict-affected regions worldwide, exacting a toll on innocent lives. Shockingly, every 95 minutes, another individual becomes a victim of these lethal explosive devices (Landmines Monitor 2022 2022), with a significant proportion being innocent civilians. Current methods for landmine detection suffer from inefficiency, high costs, and risks to the operator and system integrity. In this paper, we present a novel, efficient, safe, and cost-effective approach to unearth these hidden dangers. Our proposed method integrates an unmanned aerial vehicle (UAV) with a thermal camera to capture high-resolution images of minefields. These images are subsequently transmitted to a base computer, where a state-of-the-art image processing algorithm is applied to identify the presence of landmines. Notably, our solution performs exceptionally well, particularly during evening hours, achieving an impressive detection accuracy of nearly 88%. These results exhibit great promise when compared to existing methods constrained by their design limitations.</details> | http://arxiv.org/abs/2410.23998v1 |
| GAMap: Zero-Shot Object Goal Navigation with Multi-Scale   Geometric-Affordance Guidance | Shuaihang Yuan; Hao Huang; Yu Hao; Congcong Wen; Anthony Tzes; Yi Fang | 2024-10-31 | <details><summary>Click to expand</summary>Zero-Shot Object Goal Navigation (ZS-OGN) enables robots or agents to navigate toward objects of unseen categories without object-specific training. Traditional approaches often leverage categorical semantic information for navigation guidance, which struggles when only objects are partially observed or detailed and functional representations of the environment are lacking. To resolve the above two issues, we propose \textit{Geometric-part and Affordance Maps} (GAMap), a novel method that integrates object parts and affordance attributes as navigation guidance. Our method includes a multi-scale scoring approach to capture geometric-part and affordance attributes of objects at different scales. Comprehensive experiments conducted on HM3D and Gibson benchmark datasets demonstrate improvements in Success Rate and Success weighted by Path Length, underscoring the efficacy of our geometric-part and affordance-guided navigation approach in enhancing robot autonomy and versatility, without any additional object-specific training or fine-tuning with the semantics of unseen objects and/or the locomotions of the robot.</details> | http://arxiv.org/abs/2410.23978v1 |
| EmbodiedRAG: Dynamic 3D Scene Graph Retrieval for Efficient and Scalable   Robot Task Planning | Meghan Booker; Grayson Byrd; Bethany Kemp; Aurora Schmidt; Corban Rivera | 2024-10-31 | <details><summary>Click to expand</summary>Recent advances in Large Language Models (LLMs) have helped facilitate exciting progress for robotic planning in real, open-world environments. 3D scene graphs (3DSGs) offer a promising environment representation for grounding such LLM-based planners as they are compact and semantically rich. However, as the robot's environment scales (e.g., number of entities tracked) and the complexity of scene graph information increases (e.g., maintaining more attributes), providing the 3DSG as-is to an LLM-based planner quickly becomes infeasible due to input token count limits and attentional biases present in LLMs. Inspired by the successes of Retrieval-Augmented Generation (RAG) methods that retrieve query-relevant document chunks for LLM question and answering, we adapt the paradigm for our embodied domain. Specifically, we propose a 3D scene subgraph retrieval framework, called EmbodiedRAG, that we augment an LLM-based planner with for executing natural language robotic tasks. Notably, our retrieved subgraphs adapt to changes in the environment as well as changes in task-relevancy as the robot executes its plan. We demonstrate EmbodiedRAG's ability to significantly reduce input token counts (by an order of magnitude) and planning time (up to 70% reduction in average time per planning step) while improving success rates on AI2Thor simulated household tasks with a single-arm, mobile manipulator. Additionally, we implement EmbodiedRAG on a quadruped with a manipulator to highlight the performance benefits for robot deployment at the edge in real environments.</details> | http://arxiv.org/abs/2410.23968v1 |
| Exploiting Information Theory for Intuitive Robot Programming of Manual   Activities | Elena Merlo; Marta Lagomarsino; Edoardo Lamon; Arash Ajoudani | 2024-10-31 | <details><summary>Click to expand</summary>Observational learning is a promising approach to enable people without expertise in programming to transfer skills to robots in a user-friendly manner, since it mirrors how humans learn new behaviors by observing others. Many existing methods focus on instructing robots to mimic human trajectories, but motion-level strategies often pose challenges in skills generalization across diverse environments. This paper proposes a novel framework that allows robots to achieve a \textit{higher-level} understanding of human-demonstrated manual tasks recorded in RGB videos. By recognizing the task structure and goals, robots generalize what observed to unseen scenarios. We found our task representation on Shannon's Information Theory (IT), which is applied for the first time to manual tasks. IT helps extract the active scene elements and quantify the information shared between hands and objects. We exploit scene graph properties to encode the extracted interaction features in a compact structure and segment the demonstration into blocks, streamlining the generation of Behavior Trees for robot replicas. Experiments validated the effectiveness of IT to automatically generate robot execution plans from a single human demonstration. Additionally, we provide HANDSOME, an open-source dataset of HAND Skills demOnstrated by Multi-subjEcts, to promote further research and evaluation in this field.</details> | http://arxiv.org/abs/2410.23963v1 |
| Redundant Observer-Based Tracking Control for Object Extraction Using a   Cable Connected UAV | Benjamin J. Marshall; Yunda Yan; James Knowles; Chenguang Yang; Cunjia Liu | 2024-10-31 | <details><summary>Click to expand</summary>A new disturbance observer based control scheme is developed for a quadrotor under the concurrent disturbances from a lightweight elastic tether cable and a lumped vertical disturbance. This elastic tether is unusual as it creates a disturbance proportional to the multicopter's translational movement. This paper takes an observer-based approach to estimate the stiffness coefficient of the cable and uses the system model to update the estimates of the external forces, which are then compensated in the control action. Given that the tethered cable force affects both horizontal channels of the quadrotor and is also coupled with the vertical channel, the proposed disturbance observer is constructed to exploit the redundant measurements across all three channels to jointly estimate the cable stiffness and the vertical disturbance. A pseudo-inverse method is used to determine the observer gain functions, such that the estimation of the two quantities is decoupled and stable. Compared to standard disturbance observers which assume nearly constant disturbances, the proposed approach can quickly adjust its total force estimate as the tethered quadrotor changes its position or tautness of the tether. This is applied to two experiments - a tracking performance test where the multicopter moves under a constant tether strain, and an object extraction test. In the second test, the multicopter manipulates a nonlinear mechanism mimicking the extraction of a wedged object. In both cases, the proposed approach shows significant improvement over standard Disturbance Observer and Extended State Observer approaches. A video summary of the experiments can be found at https://youtu.be/9gKr13WTj-k.</details> | http://arxiv.org/abs/2410.23929v1 |
| Transformer-based Model Predictive Control: Trajectory Optimization via   Sequence Modeling | Davide Celestini; Daniele Gammelli; Tommaso Guffanti; Simone D'Amico; Elisa Capello; Marco Pavone | 2024-10-31 | <details><summary>Click to expand</summary>Model predictive control (MPC) has established itself as the primary methodology for constrained control, enabling general-purpose robot autonomy in diverse real-world scenarios. However, for most problems of interest, MPC relies on the recursive solution of highly non-convex trajectory optimization problems, leading to high computational complexity and strong dependency on initialization. In this work, we present a unified framework to combine the main strengths of optimization-based and learning-based methods for MPC. Our approach entails embedding high-capacity, transformer-based neural network models within the optimization process for trajectory generation, whereby the transformer provides a near-optimal initial guess, or target plan, to a non-convex optimization problem. Our experiments, performed in simulation and the real world onboard a free flyer platform, demonstrate the capabilities of our framework to improve MPC convergence and runtime. Compared to purely optimization-based approaches, results show that our approach can improve trajectory generation performance by up to 75%, reduce the number of solver iterations by up to 45%, and improve overall MPC runtime by 7x without loss in performance.</details> | http://arxiv.org/abs/2410.23916v1 |
| Uncertainty Estimation for 3D Object Detection via Evidential Learning | Nikita Durasov; Rafid Mahmood; Jiwoong Choi; Marc T. Law; James Lucas; Pascal Fua; Jose M. Alvarez | 2024-10-31 | <details><summary>Click to expand</summary>3D object detection is an essential task for computer vision applications in autonomous vehicles and robotics. However, models often struggle to quantify detection reliability, leading to poor performance on unfamiliar scenes. We introduce a framework for quantifying uncertainty in 3D object detection by leveraging an evidential learning loss on Bird's Eye View representations in the 3D detector. These uncertainty estimates require minimal computational overhead and are generalizable across different architectures. We demonstrate both the efficacy and importance of these uncertainty estimates on identifying out-of-distribution scenes, poorly localized objects, and missing (false negative) detections; our framework consistently improves over baselines by 10-20% on average. Finally, we integrate this suite of tasks into a system where a 3D object detector auto-labels driving scenes and our uncertainty estimates verify label correctness before the labels are used to train a second model. Here, our uncertainty-driven verification results in a 1% improvement in mAP and a 1-2% improvement in NDS.</details> | http://arxiv.org/abs/2410.23910v1 |
| From Web Data to Real Fields: Low-Cost Unsupervised Domain Adaptation   for Agricultural Robots | Vasileios Tzouras; Lazaros Nalpantidis; Ronja Güldenring | 2024-10-31 | <details><summary>Click to expand</summary>In precision agriculture, vision models often struggle with new, unseen fields where crops and weeds have been influenced by external factors, resulting in compositions and appearances that differ from the learned distribution. This paper aims to adapt to specific fields at low cost using Unsupervised Domain Adaptation (UDA). We explore a novel domain shift from a diverse, large pool of internet-sourced data to a small set of data collected by a robot at specific locations, minimizing the need for extensive on-field data collection. Additionally, we introduce a novel module -- the Multi-level Attention-based Adversarial Discriminator (MAAD) -- which can be integrated at the feature extractor level of any detection model. In this study, we incorporate MAAD with CenterNet to simultaneously detect leaf, stem, and vein instances. Our results show significant performance improvements in the unlabeled target domain compared to baseline models, with a 7.5% increase in object detection accuracy and a 5.1% improvement in keypoint detection.</details> | http://arxiv.org/abs/2410.23906v1 |
| Analysing the Interplay of Vision and Touch for Dexterous Insertion   Tasks | Janis Lenz; Theo Gruner; Daniel Palenicek; Tim Schneider; Jan Peters | 2024-10-31 | <details><summary>Click to expand</summary>Robotic insertion tasks remain challenging due to uncertainties in perception and the need for precise control, particularly in unstructured environments. While humans seamlessly combine vision and touch for such tasks, effectively integrating these modalities in robotic systems is still an open problem. Our work presents an extensive analysis of the interplay between visual and tactile feedback during dexterous insertion tasks, showing that tactile sensing can greatly enhance success rates on challenging insertions with tight tolerances and varied hole orientations that vision alone cannot solve. These findings provide valuable insights for designing more effective multi-modal robotic control systems and highlight the critical role of tactile feedback in contact-rich manipulation tasks.</details> | http://arxiv.org/abs/2410.23860v1 |
| A Comprehensive Review of Current Robot- Based Pollinators in Greenhouse   Farming | Rajmeet Singh; lakmal Seneviratne; Irfan Hussain | 2024-10-31 | <details><summary>Click to expand</summary>The decline of bee and wind-based pollination systems in greenhouses due to controlled environments and limited access has boost the importance of finding alternative pollination methods. Robotic based pollination systems have emerged as a promising solution, ensuring adequate crop yield even in challenging pollination scenarios. This paper presents a comprehensive review of the current robotic-based pollinators employed in greenhouses. The review categorizes pollinator technologies into major categories such as air-jet, water-jet, linear actuator, ultrasonic wave, and air-liquid spray, each suitable for specific crop pollination requirements. However, these technologies are often tailored to particular crops, limiting their versatility. The advancement of science and technology has led to the integration of automated pollination technology, encompassing information technology, automatic perception, detection, control, and operation. This integration not only reduces labor costs but also fosters the ongoing progress of modern agriculture by refining technology, enhancing automation, and promoting intelligence in agricultural practices. Finally, the challenges encountered in design of pollinator are addressed, and a forward-looking perspective is taken towards future developments, aiming to contribute to the sustainable advancement of this technology.</details> | http://arxiv.org/abs/2410.23747v1 |
| Features characterizing safe aerial-aquatic robots | Andrea Giordano; Luca Romanello; Diego Perez Gonzalez; Mirko Kovac; Sophie F. Armanini | 2024-10-31 | <details><summary>Click to expand</summary>This paper underscores the importance of environmental monitoring, and specifically of freshwater ecosystems, which play a critical role in sustaining life and global economy. Despite their importance, insufficient data availability prevents a comprehensive understanding of these ecosystems, thereby impeding informed decision-making concerning their preservation. Aerial-aquatic robots are identified as effective tools for freshwater sensing, offering rapid deployment and avoiding the need of using ships and manned teams.   To advance the field of aerial aquatic robots, this paper conducts a comprehensive review of air-water transitions focusing on the water entry strategy of existing prototypes. This analysis also highlights the safety risks associated with each transition and proposes a set of design requirements relating to robots' tasks, mission objectives, and safety measures. To further explore the proposed design requirements, we present a novel robot with VTOL capability, enabling seamless air water transitions.</details> | http://arxiv.org/abs/2410.23722v1 |
| Get a Grip: Multi-Finger Grasp Evaluation at Scale Enables Robust   Sim-to-Real Transfer | Tyler Ga Wei Lum; Albert H. Li; Preston Culbertson; Krishnan Srinivasan; Aaron D. Ames; Mac Schwager; Jeannette Bohg | 2024-10-31 | <details><summary>Click to expand</summary>This work explores conditions under which multi-finger grasping algorithms can attain robust sim-to-real transfer. While numerous large datasets facilitate learning generative models for multi-finger grasping at scale, reliable real-world dexterous grasping remains challenging, with most methods degrading when deployed on hardware. An alternate strategy is to use discriminative grasp evaluation models for grasp selection and refinement, conditioned on real-world sensor measurements. This paradigm has produced state-of-the-art results for vision-based parallel-jaw grasping, but remains unproven in the multi-finger setting. In this work, we find that existing datasets and methods have been insufficient for training discriminitive models for multi-finger grasping. To train grasp evaluators at scale, datasets must provide on the order of millions of grasps, including both positive and negative examples, with corresponding visual data resembling measurements at inference time. To that end, we release a new, open-source dataset of 3.5M grasps on 4.3K objects annotated with RGB images, point clouds, and trained NeRFs. Leveraging this dataset, we train vision-based grasp evaluators that outperform both analytic and generative modeling-based baselines on extensive simulated and real-world trials across a diverse range of objects. We show via numerous ablations that the key factor for performance is indeed the evaluator, and that its quality degrades as the dataset shrinks, demonstrating the importance of our new dataset. Project website at: https://sites.google.com/view/get-a-grip-dataset.</details> | http://arxiv.org/abs/2410.23701v1 |
| XRDSLAM: A Flexible and Modular Framework for Deep Learning based SLAM | Xiaomeng Wang; Nan Wang; Guofeng Zhang | 2024-10-31 | <details><summary>Click to expand</summary>In this paper, we propose a flexible SLAM framework, XRDSLAM. It adopts a modular code design and a multi-process running mechanism, providing highly reusable foundational modules such as unified dataset management, 3d visualization, algorithm configuration, and metrics evaluation. It can help developers quickly build a complete SLAM system, flexibly combine different algorithm modules, and conduct standardized benchmarking for accuracy and efficiency comparison. Within this framework, we integrate several state-of-the-art SLAM algorithms with different types, including NeRF and 3DGS based SLAM, and even odometry or reconstruction algorithms, which demonstrates the flexibility and extensibility. We also conduct a comprehensive comparison and evaluation of these integrated algorithms, analyzing the characteristics of each. Finally, we contribute all the code, configuration and data to the open-source community, which aims to promote the widespread research and development of SLAM technology within the open-source ecosystem.</details> | http://arxiv.org/abs/2410.23690v1 |
| CubiXMusashi: Fusion of Wire-Driven CubiX and Musculoskeletal Humanoid   Musashi toward Unlimited Performance | Shintaro Inoue; Kento Kawaharazuka; Temma Suzuki; Sota Yuzaki; Yoshimoto Ribayashi; Yuta Sahara; Kei Okada | 2024-10-31 | <details><summary>Click to expand</summary>Humanoids exhibit a wide variety in terms of joint configuration, actuators, and degrees of freedom, resulting in different achievable movements and tasks for each type. Particularly, musculoskeletal humanoids are developed to closely emulate human body structure and movement functions, consisting of a skeletal framework driven by numerous muscle actuators. The redundant arrangement of muscles relative to the skeletal degrees of freedom has been used to represent the flexible and complex body movements observed in humans. However, due to this flexible body and high degrees of freedom, modeling, simulation, and control become extremely challenging, limiting the feasible movements and tasks. In this study, we integrate the musculoskeletal humanoid Musashi with the wire-driven robot CubiX, capable of connecting to the environment, to form CubiXMusashi. This combination addresses the shortcomings of traditional musculoskeletal humanoids and enables movements beyond the capabilities of other humanoids. CubiXMusashi connects to the environment with wires and drives by winding them, successfully achieving movements such as pull-up, rising from a lying pose, and mid-air kicking, which are difficult for Musashi alone. This concept demonstrates that various humanoids, not limited to musculoskeletal humanoids, can mitigate their physical constraints and acquire new abilities by connecting to the environment and driving through wires.</details> | http://arxiv.org/abs/2410.23682v1 |
| SceneComplete: Open-World 3D Scene Completion in Complex Real World   Environments for Robot Manipulation | Aditya Agarwal; Gaurav Singh; Bipasha Sen; Tomás Lozano-Pérez; Leslie Pack Kaelbling | 2024-10-31 | <details><summary>Click to expand</summary>Careful robot manipulation in every-day cluttered environments requires an accurate understanding of the 3D scene, in order to grasp and place objects stably and reliably and to avoid mistakenly colliding with other objects. In general, we must construct such a 3D interpretation of a complex scene based on limited input, such as a single RGB-D image. We describe SceneComplete, a system for constructing a complete, segmented, 3D model of a scene from a single view. It provides a novel pipeline for composing general-purpose pretrained perception modules (vision-language, segmentation, image-inpainting, image-to-3D, and pose-estimation) to obtain high-accuracy results. We demonstrate its accuracy and effectiveness with respect to ground-truth models in a large benchmark dataset and show that its accurate whole-object reconstruction enables robust grasp proposal generation, including for a dexterous hand.</details> | http://arxiv.org/abs/2410.23643v1 |
| SuctionPrompt: Visual-assisted Robotic Picking with a Suction Cup Using   Vision-Language Models and Facile Hardware Design | Tomohiro Motoda; Takahide Kitamura; Ryo Hanai; Yukiyasu Domae | 2024-10-31 | <details><summary>Click to expand</summary>The development of large language models and vision-language models (VLMs) has resulted in the increasing use of robotic systems in various fields. However, the effective integration of these models into real-world robotic tasks is a key challenge. We developed a versatile robotic system called SuctionPrompt that utilizes prompting techniques of VLMs combined with 3D detections to perform product-picking tasks in diverse and dynamic environments. Our method highlights the importance of integrating 3D spatial information with adaptive action planning to enable robots to approach and manipulate objects in novel environments. In the validation experiments, the system accurately selected suction points 75.4%, and achieved a 65.0% success rate in picking common items. This study highlights the effectiveness of VLMs in robotic manipulation tasks, even with simple 3D processing.</details> | http://arxiv.org/abs/2410.23640v1 |
| Tiny Learning-Based MPC for Multirotors: Solver-Aware Learning for   Efficient Embedded Predictive Control | Babak Akbari; Justin Frank; Melissa Greeff | 2024-10-31 | <details><summary>Click to expand</summary>Tiny aerial robots show promise for applications like environmental monitoring and search-and-rescue but face challenges in control due to their limited computing power and complex dynamics. Model Predictive Control (MPC) can achieve agile trajectory tracking and handle constraints. Although current learning-based MPC methods, such as Gaussian Process (GP) MPC, improve control performance by learning residual dynamics, they are computationally demanding, limiting their onboard application on tiny robots. This paper introduces Tiny Learning-Based Model Predictive Control (LB MPC), a novel framework for resource-constrained micro multirotor platforms. By exploiting multirotor dynamics' structure and developing an efficient solver, our approach enables high-rate control at 100 Hz on a Crazyflie 2.1 with a Teensy 4.0 microcontroller. We demonstrate a 23% average improvement in tracking performance over existing embedded MPC methods, achieving the first onboard implementation of learning-based MPC on a tiny multirotor (53 g).</details> | http://arxiv.org/abs/2410.23634v2 |
| EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation   for Electromyography | Jehan Yang; Maxwell Soh; Vivianna Lieu; Douglas J Weber; Zackory Erickson | 2024-10-31 | <details><summary>Click to expand</summary>This paper introduces the first generalization and adaptation benchmark using machine learning for evaluating out-of-distribution performance of electromyography (EMG) classification algorithms. The ability of an EMG classifier to handle inputs drawn from a different distribution than the training distribution is critical for real-world deployment as a control interface. By predicting the user's intended gesture using EMG signals, we can create a wearable solution to control assistive technologies, such as computers, prosthetics, and mobile manipulator robots. This new out-of-distribution benchmark consists of two major tasks that have utility for building robust and adaptable control interfaces: 1) intersubject classification and 2) adaptation using train-test splits for time-series. This benchmark spans nine datasets--the largest collection of EMG datasets in a benchmark. Among these, a new dataset is introduced, featuring a novel, easy-to-wear high-density EMG wearable for data collection. The lack of open-source benchmarks has made comparing accuracy results between papers challenging for the EMG research community. This new benchmark provides researchers with a valuable resource for analyzing practical measures of out-of-distribution performance for EMG datasets. Our code and data from our new dataset can be found at emgbench.github.io.</details> | http://arxiv.org/abs/2410.23625v2 |
| Multi-Robot Pursuit in Parameterized Formation via Imitation Learning | Jinyong Chen; Rui Zhou; Zhaozong Wang; Yunjie Zhang; Guibin Sun | 2024-10-31 | <details><summary>Click to expand</summary>This paper studies the problem of multi-robot pursuit of how to coordinate a group of defending robots to capture a faster attacker before it enters a protected area. Such operation for defending robots is challenging due to the unknown avoidance strategy and higher speed of the attacker, coupled with the limited communication capabilities of defenders. To solve this problem, we propose a parameterized formation controller that allows defending robots to adapt their formation shape using five adjustable parameters. Moreover, we develop an imitation-learning based approach integrated with model predictive control to optimize these shape parameters. We make full use of these two techniques to enhance the capture capabilities of defending robots through ongoing training. Both simulation and experiment are provided to verify the effectiveness and robustness of our proposed controller. Simulation results show that defending robots can rapidly learn an effective strategy for capturing the attacker, and moreover the learned strategy remains effective across varying numbers of defenders. Experiment results on real robot platforms further validated these findings.</details> | http://arxiv.org/abs/2410.23586v1 |
| Distributed Formation Shape Control of Identity-less Robot Swarms | Guibin Sun; Yang Xu; Kexin Liu; Jinhu Lü | 2024-10-31 | <details><summary>Click to expand</summary>Different from most of the formation strategies where robots require unique labels to identify topological neighbors to satisfy the predefined shape constraints, we here study the problem of identity-less distributed shape formation in homogeneous swarms, which is rarely studied in the literature. The absence of identities creates a unique challenge: how to design appropriate target formations and local behaviors that are suitable for identity-less formation shape control. To address this challenge, we propose the following novel results. First, to avoid using unique identities, we propose a dynamic formation description method and solve the formation consensus of robots in a locally distributed manner. Second, to handle identity-less distributed formations, we propose a fully distributed control law for homogeneous swarms based on locally sensed information. While the existing methods are applicable to simple cases where the target formation is stationary, ours can tackle more general maneuvering formations such as translation, rotation, or even shape deformation. Both numerical simulation and flight experiment are presented to verify the effectiveness and robustness of our proposed formation strategy.</details> | http://arxiv.org/abs/2410.23581v1 |
| Dual Agent Learning Based Aerial Trajectory Tracking | Shaswat Garg; Houman Masnavi; Baris Fidan; Farrokh Janabi-Sharifi | 2024-10-31 | <details><summary>Click to expand</summary>This paper presents a novel reinforcement learning framework for trajectory tracking of unmanned aerial vehicles in cluttered environments using a dual-agent architecture. Traditional optimization methods for trajectory tracking face significant computational challenges and lack robustness in dynamic environments. Our approach employs deep reinforcement learning (RL) to overcome these limitations, leveraging 3D pointcloud data to perceive the environment without relying on memory-intensive obstacle representations like occupancy grids. The proposed system features two RL agents: one for predicting UAV velocities to follow a reference trajectory and another for managing collision avoidance in the presence of obstacles. This architecture ensures real-time performance and adaptability to uncertainties. We demonstrate the efficacy of our approach through simulated and real-world experiments, highlighting improvements over state-of-the-art RL and optimization-based methods. Additionally, a curriculum learning paradigm is employed to scale the algorithms to more complex environments, ensuring robust trajectory tracking and obstacle avoidance in both static and dynamic scenarios.</details> | http://arxiv.org/abs/2410.23571v1 |
| Simulating User Agents for Embodied Conversational-AI | Daniel Philipov; Vardhan Dongre; Gokhan Tur; Dilek Hakkani-Tür | 2024-10-31 | <details><summary>Click to expand</summary>Embodied agents designed to assist users with tasks must engage in natural language interactions, interpret instructions, execute actions, and communicate effectively to resolve issues. However, collecting large-scale, diverse datasets of situated human-robot dialogues to train and evaluate such agents is expensive, labor-intensive, and time-consuming. To address this challenge, we propose building a large language model (LLM)-based user agent that can simulate user behavior during interactions with an embodied agent in a virtual environment. Given a user goal (e.g., make breakfast), at each time step, the user agent may observe" the robot actions or speak" to either intervene with the robot or answer questions. Such a user agent assists in improving the scalability and efficiency of embodied dialogues dataset generation and is critical for enhancing and evaluating the robot's interaction and task completion ability, as well as for research in reinforcement learning using AI feedback. We evaluate our user agent's ability to generate human-like behaviors by comparing its simulated dialogues with the TEACh dataset. We perform three experiments: zero-shot prompting to predict dialogue acts, few-shot prompting, and fine-tuning on the TEACh training subset. Results show the LLM-based user agent achieves an F-measure of 42% with zero-shot prompting and 43.4% with few-shot prompting in mimicking human speaking behavior. Through fine-tuning, performance in deciding when to speak remained stable, while deciding what to say improved from 51.1% to 62.5%. These findings showcase the feasibility of the proposed approach for assessing and enhancing the effectiveness of robot task completion through natural language communication.</details> | http://arxiv.org/abs/2410.23535v1 |
| LBurst: Learning-Based Robotic Burst Feature Extraction for 3D   Reconstruction in Low Light | Ahalya Ravendran; Mitch Bryson; Donald G. Dansereau | 2024-10-31 | <details><summary>Click to expand</summary>Drones have revolutionized the fields of aerial imaging, mapping, and disaster recovery. However, the deployment of drones in low-light conditions is constrained by the image quality produced by their on-board cameras. In this paper, we present a learning architecture for improving 3D reconstructions in low-light conditions by finding features in a burst. Our approach enhances visual reconstruction by detecting and describing high quality true features and less spurious features in low signal-to-noise ratio images. We demonstrate that our method is capable of handling challenging scenes in millilux illumination, making it a significant step towards drones operating at night and in extremely low-light applications such as underground mining and search and rescue operations.</details> | http://arxiv.org/abs/2410.23522v1 |
| NUSense: Robust Soft Optical Tactile Sensor | Madina Yergibay; Tleukhan Mussin; Saltanat Seitzhan; Daryn Kenzhebek; Zhanat Kappassov; Harold Soh; Tasbolat Taunyazov | 2024-10-30 | <details><summary>Click to expand</summary>While most tactile sensors rely on measuring pressure, insights from continuum mechanics suggest that measuring shear strain provides critical information for tactile sensing. In this work, we introduce an optical tactile sensing principle based on shear strain detection. A silicone rubber layer, dyed with color inks, is used to quantify the shear magnitude of the sensing layer. This principle was validated using the NUSense camera-based tactile sensor. The wide-angle camera captures the elongation of the soft pad under mechanical load, a phenomenon attributed to the Poisson effect. The physical and optical properties of the inked pad are essential and should ideally remain stable over time. We tested the robustness of the sensor by subjecting the outermost layer to multiple load cycles using a robot arm. Additionally, we discussed potential applications of this sensor in force sensing and contact localization.</details> | http://arxiv.org/abs/2410.23516v1 |
| PACER: Preference-conditioned All-terrain Costmap Generation | Luisa Mao; Garrett Warnell; Peter Stone; Joydeep Biswas | 2024-10-30 | <details><summary>Click to expand</summary>In autonomous robot navigation, terrain cost assignment is typically performed using a semantics-based paradigm in which terrain is first labeled using a pre-trained semantic classifier and costs are then assigned according to a user-defined mapping between label and cost. While this approach is rapidly adaptable to changing user preferences, only preferences over the types of terrain that are already known by the semantic classifier can be expressed. In this paper, we hypothesize that a machine-learning-based alternative to the semantics-based paradigm above will allow for rapid cost assignment adaptation to preferences expressed over new terrains at deployment time without the need for additional training. To investigate this hypothesis, we introduce and study PACER, a novel approach to costmap generation that accepts as input a single birds-eye view (BEV) image of the surrounding area along with a user-specified preference context and generates a corresponding BEV costmap that aligns with the preference context. Using both real and synthetic data along with a combination of proposed training tasks, we find that PACER is able to adapt quickly to new user preferences while also exhibiting better generalization to novel terrains compared to both semantics-based and representation-learning approaches.</details> | http://arxiv.org/abs/2410.23488v1 |
| Design and Motion Analysis of a Reconfigurable Pendulum-Based Rolling   Disk Robot with Magnetic Coupling | Ollie Wiltshire; Seyed Amir Tafrishi | 2024-10-30 | <details><summary>Click to expand</summary>Reconfigurable robots are at the forefront of robotics innovation due to their unmatched versatility and adaptability in addressing various tasks through collaborative operations. This paper explores the design and implementation of a novel pendulum-based magnetic coupling system within a reconfigurable disk robot. Diverging from traditional designs, this system emphasizes enhancing coupling strength while maintaining the compactness of the outer shell. We employ parametric optimization techniques, including magnetic array simulations, to improve coupling performance. Additionally, we conduct a comprehensive analysis of the rolling robot's motion to assess its operational effectiveness in the coupling mechanism. This examination reveals intriguing new motion patterns driven by frictional and sliding effects between the rolling disk modules and the ground. Furthermore, the new setup introduces a novel problem in the area of nonprehensile manipulation.</details> | http://arxiv.org/abs/2410.23464v1 |
| Return Augmented Decision Transformer for Off-Dynamics Reinforcement   Learning | Ruhan Wang; Yu Yang; Zhishuai Liu; Dongruo Zhou; Pan Xu | 2024-10-30 | <details><summary>Click to expand</summary>We study offline off-dynamics reinforcement learning (RL) to utilize data from an easily accessible source domain to enhance policy learning in a target domain with limited data. Our approach centers on return-conditioned supervised learning (RCSL), particularly focusing on the decision transformer (DT), which can predict actions conditioned on desired return guidance and complete trajectory history. Previous works tackle the dynamics shift problem by augmenting the reward in the trajectory from the source domain to match the optimal trajectory in the target domain. However, this strategy can not be directly applicable in RCSL owing to (1) the unique form of the RCSL policy class, which explicitly depends on the return, and (2) the absence of a straightforward representation of the optimal trajectory distribution. We propose the Return Augmented Decision Transformer (RADT) method, where we augment the return in the source domain by aligning its distribution with that in the target domain. We provide the theoretical analysis demonstrating that the RCSL policy learned from RADT achieves the same level of suboptimality as would be obtained without a dynamics shift. We introduce two practical implementations RADT-DARA and RADT-MV respectively. Extensive experiments conducted on D4RL datasets reveal that our methods generally outperform dynamic programming based methods in off-dynamics RL scenarios.</details> | http://arxiv.org/abs/2410.23450v1 |
| Learning for Deformable Linear Object Insertion Leveraging Flexibility   Estimation from Visual Cues | Mingen Li; Changhyun Choi | 2024-10-30 | <details><summary>Click to expand</summary>Manipulation of deformable Linear objects (DLOs), including iron wire, rubber, silk, and nylon rope, is ubiquitous in daily life. These objects exhibit diverse physical properties, such as Young$'$s modulus and bending stiffness.Such diversity poses challenges for developing generalized manipulation policies. However, previous research limited their scope to single-material DLOs and engaged in time-consuming data collection for the state estimation. In this paper, we propose a two-stage manipulation approach consisting of a material property (e.g., flexibility) estimation and policy learning for DLO insertion with reinforcement learning. Firstly, we design a flexibility estimation scheme that characterizes the properties of different types of DLOs. The ground truth flexibility data is collected in simulation to train our flexibility estimation module. During the manipulation, the robot interacts with the DLOs to estimate flexibility by analyzing their visual configurations. Secondly, we train a policy conditioned on the estimated flexibility to perform challenging DLO insertion tasks. Our pipeline trained with diverse insertion scenarios achieves an 85.6% success rate in simulation and 66.67% in real robot experiments. Please refer to our project page: https://lmeee.github.io/DLOInsert/</details> | http://arxiv.org/abs/2410.23428v1 |
| Stepping Out of the Shadows: Reinforcement Learning in Shadow Mode | Philipp Gassert; Matthias Althoff | 2024-10-30 | <details><summary>Click to expand</summary>Reinforcement learning (RL) is not yet competitive for many cyber-physical systems, such as robotics, process automation, and power systems, as training on a system with physical components cannot be accelerated, and simulation models do not exist or suffer from a large simulation-to-reality gap. During the long training time, expensive equipment cannot be used and might even be damaged due to inappropriate actions of the reinforcement learning agent. Our novel approach addresses exactly this problem: We train the reinforcement agent in a so-called shadow mode with the assistance of an existing conventional controller, which does not have to be trained and instantaneously performs reasonably well. In shadow mode, the agent relies on the controller to provide action samples and guidance towards favourable states to learn the task, while simultaneously estimating for which states the learned agent will receive a higher reward than the conventional controller. The RL agent will then control the system for these states and all other regions remain under the control of the existing controller. Over time, the RL agent will take over for an increasing amount of states, while leaving control to the baseline, where it cannot surpass its performance. Thus, we keep regret during training low and improve the performance compared to only using conventional controllers or reinforcement learning. We present and evaluate two mechanisms for deciding whether to use the RL agent or the conventional controller. The usefulness of our approach is demonstrated for a reach-avoid task, for which we are able to effectively train an agent, where standard approaches fail.</details> | http://arxiv.org/abs/2410.23419v1 |
| Estimating Neural Network Robustness via Lipschitz Constant and   Architecture Sensitivity | Abulikemu Abuduweili; Changliu Liu | 2024-10-30 | <details><summary>Click to expand</summary>Ensuring neural network robustness is essential for the safe and reliable operation of robotic learning systems, especially in perception and decision-making tasks within real-world environments. This paper investigates the robustness of neural networks in perception systems, specifically examining their sensitivity to targeted, small-scale perturbations. We identify the Lipschitz constant as a key metric for quantifying and enhancing network robustness. We derive an analytical expression to compute the Lipschitz constant based on neural network architecture, providing a theoretical basis for estimating and improving robustness. Several experiments reveal the relationship between network design, the Lipschitz constant, and robustness, offering practical insights for developing safer, more robust robot learning systems.</details> | http://arxiv.org/abs/2410.23382v1 |
| A Cost-Effective Thermal Imaging Safety Sensor for Industry 5.0 and   Collaborative Robotics | Daniel Barros; Paula Fraga-Lamas; Tiago M. Fernandez-Carames; Sergio Ivan Lopes | 2024-10-30 | <details><summary>Click to expand</summary>The Industry 5.0 paradigm focuses on industrial operator well-being and sustainable manufacturing practices, where humans play a central role, not only during the repetitive and collaborative tasks of the manufacturing process, but also in the management of the factory floor assets. Human factors, such as ergonomics, safety, and well-being, push the human-centric smart factory to efficiently adopt novel technologies while minimizing environmental and social impact. As operations at the factory floor increasingly rely on collaborative robots (CoBots) and flexible manufacturing systems, there is a growing demand for redundant safety mechanisms (i.e., automatic human detection in the proximity of machinery that is under operation). Fostering enhanced process safety for human proximity detection allows for the protection against possible incidents or accidents with the deployed industrial devices and machinery. This paper introduces the design and implementation of a cost-effective thermal imaging Safety Sensor that can be used in the scope of Industry 5.0 to trigger distinct safe mode states in manufacturing processes that rely on collaborative robotics. The proposed Safety Sensor uses a hybrid detection approach and has been evaluated under controlled environmental conditions. The obtained results show a 97% accuracy at low computational cost when using the developed hybrid method to detect the presence of humans in thermal images.</details> | http://arxiv.org/abs/2410.23377v1 |
| Bridging the Human to Robot Dexterity Gap through Object-Oriented   Rewards | Irmak Guzey; Yinlong Dai; Georgy Savva; Raunaq Bhirangi; Lerrel Pinto | 2024-10-30 | <details><summary>Click to expand</summary>Training robots directly from human videos is an emerging area in robotics and computer vision. While there has been notable progress with two-fingered grippers, learning autonomous tasks for multi-fingered robot hands in this way remains challenging. A key reason for this difficulty is that a policy trained on human hands may not directly transfer to a robot hand due to morphology differences. In this work, we present HuDOR, a technique that enables online fine-tuning of policies by directly computing rewards from human videos. Importantly, this reward function is built using object-oriented trajectories derived from off-the-shelf point trackers, providing meaningful learning signals despite the morphology gap and visual differences between human and robot hands. Given a single video of a human solving a task, such as gently opening a music box, HuDOR enables our four-fingered Allegro hand to learn the task with just an hour of online interaction. Our experiments across four tasks show that HuDOR achieves a 4x improvement over baselines. Code and videos are available on our website, https://object-rewards.github.io.</details> | http://arxiv.org/abs/2410.23289v1 |
| DisCo: Distributed Contact-Rich Trajectory Optimization for Forceful   Multi-Robot Collaboration | Ola Shorinwa; Matthew Devlin; Elliot W. Hawkes; Mac Schwager | 2024-10-30 | <details><summary>Click to expand</summary>We present DisCo, a distributed algorithm for contact-rich, multi-robot tasks. DisCo is a distributed contact-implicit trajectory optimization algorithm, which allows a group of robots to optimize a time sequence of forces to objects and to their environment to accomplish tasks such as collaborative manipulation, robot team sports, and modular robot locomotion. We build our algorithm on a variant of the Alternating Direction Method of Multipliers (ADMM), where each robot computes its own contact forces and contact-switching events from a smaller single-robot, contact-implicit trajectory optimization problem, while cooperating with other robots through dual variables, enforcing constraints between robots. Each robot iterates between solving its local problem, and communicating over a wireless mesh network to enforce these consistency constraints with its neighbors, ultimately converging to a coordinated plan for the group. The local problems solved by each robot are significantly less challenging than a centralized problem with all robots' contact forces and switching events, improving the computational efficiency, while also preserving the privacy of some aspects of each robot's operation. We demonstrate the effectiveness of our algorithm in simulations of collaborative manipulation, multi-robot team sports scenarios, and in modular robot locomotion, where DisCo achieves $3$x higher success rates with a 2.5x to 5x faster computation time. Further, we provide results of hardware experiments on a modular truss robot, with three collaborating truss nodes planning individually while working together to produce a punctuated rolling-gate motion of the composite structure. Videos are available on the project page: https://disco-opt.github.io.</details> | http://arxiv.org/abs/2410.23283v1 |
| SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video   Generation | Yining Hong; Beide Liu; Maxine Wu; Yuanhao Zhai; Kai-Wei Chang; Linjie Li; Kevin Lin; Chung-Ching Lin; Jianfeng Wang; Zhengyuan Yang; Yingnian Wu; Lijuan Wang | 2024-10-30 | <details><summary>Click to expand</summary>Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io</details> | http://arxiv.org/abs/2410.23277v2 |
| EMMA: End-to-End Multimodal Model for Autonomous Driving | Jyh-Jing Hwang; Runsheng Xu; Hubert Lin; Wei-Chih Hung; Jingwei Ji; Kristy Choi; Di Huang; Tong He; Paul Covington; Benjamin Sapp; Yin Zhou; James Guo; Dragomir Anguelov; Mingxing Tan | 2024-10-30 | <details><summary>Click to expand</summary>We introduce EMMA, an End-to-end Multimodal Model for Autonomous driving. Built on a multi-modal large language model foundation, EMMA directly maps raw camera sensor data into various driving-specific outputs, including planner trajectories, perception objects, and road graph elements. EMMA maximizes the utility of world knowledge from the pre-trained large language models, by representing all non-sensor inputs (e.g. navigation instructions and ego vehicle status) and outputs (e.g. trajectories and 3D locations) as natural language text. This approach allows EMMA to jointly process various driving tasks in a unified language space, and generate the outputs for each task using task-specific prompts. Empirically, we demonstrate EMMA's effectiveness by achieving state-of-the-art performance in motion planning on nuScenes as well as competitive results on the Waymo Open Motion Dataset (WOMD). EMMA also yields competitive results for camera-primary 3D object detection on the Waymo Open Dataset (WOD). We show that co-training EMMA with planner trajectories, object detection, and road graph tasks yields improvements across all three domains, highlighting EMMA's potential as a generalist model for autonomous driving applications. However, EMMA also exhibits certain limitations: it can process only a small amount of image frames, does not incorporate accurate 3D sensing modalities like LiDAR or radar and is computationally expensive. We hope that our results will inspire further research to mitigate these issues and to further evolve the state of the art in autonomous driving model architectures.</details> | http://arxiv.org/abs/2410.23262v2 |
| Keypoint Abstraction using Large Models for Object-Relative Imitation   Learning | Xiaolin Fang; Bo-Ruei Huang; Jiayuan Mao; Jasmine Shone; Joshua B. Tenenbaum; Tomás Lozano-Pérez; Leslie Pack Kaelbling | 2024-10-30 | <details><summary>Click to expand</summary>Generalization to novel object configurations and instances across diverse tasks and environments is a critical challenge in robotics. Keypoint-based representations have been proven effective as a succinct representation for capturing essential object features, and for establishing a reference frame in action prediction, enabling data-efficient learning of robot skills. However, their manual design nature and reliance on additional human labels limit their scalability. In this paper, we propose KALM, a framework that leverages large pre-trained vision-language models (LMs) to automatically generate task-relevant and cross-instance consistent keypoints. KALM distills robust and consistent keypoints across views and objects by generating proposals using LMs and verifies them against a small set of robot demonstration data. Based on the generated keypoints, we can train keypoint-conditioned policy models that predict actions in keypoint-centric frames, enabling robots to generalize effectively across varying object poses, camera views, and object instances with similar functional shapes. Our method demonstrates strong performance in the real world, adapting to different tasks and environments from only a handful of demonstrations while requiring no additional labels. Website: https://kalm-il.github.io/</details> | http://arxiv.org/abs/2410.23254v1 |
| EMOTION: Expressive Motion Sequence Generation for Humanoid Robots with   In-Context Learning | Peide Huang; Yuhan Hu; Nataliya Nechyporenko; Daehwa Kim; Walter Talbott; Jian Zhang | 2024-10-30 | <details><summary>Click to expand</summary>This paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in humanlike non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of human non-verbal communication. To address this gap, our approach leverages the in-context learning capability of large language models (LLMs) to dynamically generate socially appropriate gesture motion sequences for human-robot interaction. We use this framework to generate 10 different expressive gestures and conduct online user studies comparing the naturalness and understandability of the motions generated by EMOTION and its human-feedback version, EMOTION++, against those by human operators. The results demonstrate that our approach either matches or surpasses human performance in generating understandable and natural robot motions under certain scenarios. We also provide design implications for future research to consider a set of variables when generating expressive robotic gestures.</details> | http://arxiv.org/abs/2410.23234v1 |
| Levels of explanation -- implementation and evaluation of what and when   for different time-sensitive tasks | Shikhar Kumar; Omer Keidar; Yael Edan | 2024-10-30 | <details><summary>Click to expand</summary>In this work, we focused on constructing and evaluating levels of explanation(LOE) that address two basic aspect of HRI: 1. What information should be communicated to the user by the robot? 2. When should the robot communicate this information? For constructing the LOE, we defined two terms, verbosity and explanation patterns, each with two levels (verbosity -- high and low, explanation patterns -- dynamic and static). Based on these parameters, three different LOE (high, medium, and low) were constructed and evaluated in a user study with a telepresence robot. The user study was conducted for a simulated telerobotic healthcare task with two different conditions related to time sensitivity, as evaluated by two different user groups -- one that performed the task within a time limit and the other with no time limit. We found that the high LOE was preferred in terms of adequacy of explanation, number of collisions, number of incorrect movements, and number of clarifications when users performed the experiment in the without time limit condition. We also found that both high and medium LOE did not have significant differences in completion time, the fluency of HRI, and trust in the robot. When users performed the experiment in the with time limit condition, high and medium LOE had better task performances and were preferred to the low LOE in terms of completion time, fluency, adequacy of explanation, trust, number of collisions, number of incorrect movements and number of clarifications. Future directions for advancing LOE are discussed.</details> | http://arxiv.org/abs/2410.23215v1 |
| Kinetix: Investigating the Training of General Agents through Open-Ended   Physics-Based Control Tasks | Michael Matthews; Michael Beukman; Chris Lu; Jakob Foerster | 2024-10-30 | <details><summary>Click to expand</summary>While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge. In this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control. To this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework. Kinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training. Our trained agent exhibits strong physical reasoning capabilities, being able to zero-shot solve unseen human-designed environments. Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agent *tabula rasa*. This includes solving some environments that standard RL training completely fails at. We believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.</details> | http://arxiv.org/abs/2410.23208v1 |
| VisualPredicator: Learning Abstract World Models with Neuro-Symbolic   Predicates for Robot Planning | Yichao Liang; Nishanth Kumar; Hao Tang; Adrian Weller; Joshua B. Tenenbaum; Tom Silver; João F. Henriques; Kevin Ellis | 2024-10-30 | <details><summary>Click to expand</summary>Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.</details> | http://arxiv.org/abs/2410.23156v1 |
| Nested ResNet: A Vision-Based Method for Detecting the Sensing Area of a   Drop-in Gamma Probe | Songyu Xu; Yicheng Hu; Jionglong Su; Daniel Elson; Baoru Huang | 2024-10-30 | <details><summary>Click to expand</summary>Purpose: Drop-in gamma probes are widely used in robotic-assisted minimally invasive surgery (RAMIS) for lymph node detection. However, these devices only provide audio feedback on signal intensity, lacking the visual feedback necessary for precise localisation. Previous work attempted to predict the sensing area location using laparoscopic images, but the prediction accuracy was unsatisfactory. Improvements are needed in the deep learning-based regression approach.   Methods: We introduce a three-branch deep learning framework to predict the sensing area of the probe. Specifically, we utilise the stereo laparoscopic images as input for the main branch and develop a Nested ResNet architecture. The framework also incorporates depth estimation via transfer learning and orientation guidance through probe axis sampling. The combined features from each branch enhanced the accuracy of the prediction.   Results: Our approach has been evaluated on a publicly available dataset, demonstrating superior performance over previous methods. In particular, our method resulted in a 22.10\% decrease in 2D mean error and a 41.67\% reduction in 3D mean error. Additionally, qualitative comparisons further demonstrated the improved precision of our approach.   Conclusion: With extensive evaluation, our solution significantly enhances the accuracy and reliability of sensing area predictions. This advancement enables visual feedback during the use of the drop-in gamma probe in surgery, providing surgeons with more accurate and reliable localisation.}</details> | http://arxiv.org/abs/2410.23154v1 |
| Leader-Follower 3D Formation for Underwater Robots | Di Ni; Hungtang Ko; Radhika Nagpal | 2024-10-30 | <details><summary>Click to expand</summary>The schooling behavior of fish is hypothesized to confer many survival benefits, including foraging success, safety from predators, and energy savings through hydrodynamic interactions when swimming in formation. Underwater robot collectives may be able to achieve similar benefits in future applications, e.g. using formation control to achieve efficient spatial sampling for environmental monitoring. Although many theoretical algorithms exist for multi-robot formation control, they have not been tested in the underwater domain due to the fundamental challenges in underwater communication. Here we introduce a leader-follower strategy for underwater formation control that allows us to realize complex 3D formations, using purely vision-based perception and a reactive control algorithm that is low computation. We use a physical platform, BlueSwarm, to demonstrate for the first time an experimental realization of inline, side-by-side, and staggered swimming 3D formations. More complex formations are studied in a physics-based simulator, providing new insights into the convergence and stability of formations given underwater inertial/drag conditions. Our findings lay the groundwork for future applications of underwater robot swarms in aquatic environments with minimal communication.</details> | http://arxiv.org/abs/2410.23128v1 |
| S3PT: Scene Semantics and Structure Guided Clustering to Boost   Self-Supervised Pre-Training for Autonomous Driving | Maciej K. Wozniak; Hariprasath Govindarajan; Marvin Klingner; Camille Maurice; B Ravi Kiran; Senthil Yogamani | 2024-10-30 | <details><summary>Click to expand</summary>Recent self-supervised clustering-based pre-training techniques like DINO and Cribo have shown impressive results for downstream detection and segmentation tasks. However, real-world applications such as autonomous driving face challenges with imbalanced object class and size distributions and complex scene geometries. In this paper, we propose S3PT a novel scene semantics and structure guided clustering to provide more scene-consistent objectives for self-supervised training. Specifically, our contributions are threefold: First, we incorporate semantic distribution consistent clustering to encourage better representation of rare classes such as motorcycles or animals. Second, we introduce object diversity consistent spatial clustering, to handle imbalanced and diverse object sizes, ranging from large background areas to small objects such as pedestrians and traffic signs. Third, we propose a depth-guided spatial clustering to regularize learning based on geometric information of the scene, thus further refining region separation on the feature level. Our learned representations significantly improve performance in downstream semantic segmentation and 3D object detection tasks on the nuScenes, nuImages, and Cityscapes datasets and show promising domain translation properties.</details> | http://arxiv.org/abs/2410.23085v2 |
| FilMBot: A High-Speed Soft Parallel Robotic Micromanipulator | Jiangkun Yu; Houari Bettahar; Hakan Kandemir; Quan Zhou | 2024-10-30 | <details><summary>Click to expand</summary>Soft robotic manipulators are generally slow despite their great adaptability, resilience, and compliance. This limitation also extends to current soft robotic micromanipulators. Here, we introduce FilMBot, a 3-DOF film-based, electromagnetically actuated, soft kinematic robotic micromanipulator achieving speeds up to 2117 $\deg$/s and 2456 $\deg$/s in $\alpha$ and $\beta$ angular motions, with corresponding linear velocities of 1.61 m/s and 1.92 m/s using a 4-cm needle end-effector, and 1.57 m/s along the Z axis. The robot can reach ~1.50 m/s in path-following tasks, operates at frequencies up to 30 Hz, and remains functional up to 50 Hz. It demonstrates high precision (~6.3 $\mu$m, or ~0.05% of its workspace) in small path-following tasks. The novel combination of the low-stiffness soft kinematic film structure and strong electromagnetic actuation in FilMBot opens new avenues for soft robotics. Furthermore, its simple construction and inexpensive, readily accessible components could broaden the application of micromanipulators beyond current academic and professional users.</details> | http://arxiv.org/abs/2410.23059v2 |
| TumblerBots: Tumbling Robotic sensors for Minimally-invasive Benthic   Monitoring | L. Romanello; A. Teboul; F. Wiesemuller; P. H. Nguyen; M. Kovac; S. F. Armanini | 2024-10-30 | <details><summary>Click to expand</summary>Robotic systems show significant promise for water environmental sensing applications such as water quality monitoring, pollution mapping and biodiversity data collection.   Conventional deployment methods often disrupt fragile ecosystems, preventing depiction of the undisturbed environmental condition. In response to this challenge, we propose a novel framework utilizing a lightweight tumbler system equipped with a sensing unit, deployed via a drone. This design minimizes disruption to the water habitat by maintaining a slow descent. The sensing unit is detached once on the water surface, enabling precise and non-invasive data collection from the benthic zone.   The tumbler is designed to be lightweight and compact, enabling deployment via a drone. The sensing pod, which detaches from the tumbler and descends to the bottom of the water body, is equipped with temperature and pressure sensors, as well as a buoyancy system. The later, activated upon task completion, utilizes a silicon membrane inflated via a chemical reaction. The reaction generates a pressure of 70 kPa, causing the silicon membrane to expand by 30\%, which exceeds the 5.7\% volume increase required for positive buoyancy. The tumblers, made from ecofriendly materials to minimize environmental impact when lost during the mission, were tested for their gliding ratio and descent rate. They exhibit a low descent rate, in the range of 0.8 to 2.5 meters per seconds, which minimizes disturbance to the ecosystem upon water landing. Additionally, the system demonstrated robustness in moderate to strong wind conditions during outdoor tests, validating the overall framework.</details> | http://arxiv.org/abs/2410.23049v1 |
| Neural Attention Field: Emerging Point Relevance in 3D Scenes for   One-Shot Dexterous Grasping | Qianxu Wang; Congyue Deng; Tyler Ga Wei Lum; Yuanpei Chen; Yaodong Yang; Jeannette Bohg; Yixin Zhu; Leonidas Guibas | 2024-10-30 | <details><summary>Click to expand</summary>One-shot transfer of dexterous grasps to novel scenes with object and context variations has been a challenging problem. While distilled feature fields from large vision models have enabled semantic correspondences across 3D scenes, their features are point-based and restricted to object surfaces, limiting their capability of modeling complex semantic feature distributions for hand-object interactions. In this work, we propose the \textit{neural attention field} for representing semantic-aware dense feature fields in the 3D space by modeling inter-point relevance instead of individual point features. Core to it is a transformer decoder that computes the cross-attention between any 3D query point with all the scene points, and provides the query point feature with an attention-based aggregation. We further propose a self-supervised framework for training the transformer decoder from only a few 3D pointclouds without hand demonstrations. Post-training, the attention field can be applied to novel scenes for semantics-aware dexterous grasping from one-shot demonstration. Experiments show that our method provides better optimization landscapes by encouraging the end-effector to focus on task-relevant scene regions, resulting in significant improvements in success rates on real robots compared with the feature-field-based methods.</details> | http://arxiv.org/abs/2410.23039v1 |
| Exploring the Potential of Multi-modal Sensing Framework for Forest   Ecology | Luca Romanello; Tian Lan; Mirko Kovac; Sophie F. Armanini; Basaran Bahadir Kocer | 2024-10-30 | <details><summary>Click to expand</summary>Forests offer essential resources and services to humanity, yet preserving and restoring them presents challenges, particularly due to the limited availability of actionable data, especially in hard-to-reach areas like forest canopies. Accessibility continues to pose a challenge for biologists collecting data in forest environments, often requiring them to invest significant time and energy in climbing trees to place sensors. This operation not only consumes resources but also exposes them to danger. Efforts in robotics have been directed towards accessing the tree canopy using robots. A swarm of drones has showcased autonomous navigation through the canopy, maneuvering with agility and evading tree collisions, all aimed at mapping the area and collecting data. However, relying solely on free-flying drones has proven insufficient for data collection. Flying drones within the canopy generates loud noise, disturbing animals and potentially corrupting the data. Additionally, commercial drones often have limited autonomy for dexterous tasks where aerial physical interaction could be required, further complicating data acquisition efforts. Aerial deployed sensor placement methods such as bio-gliders and sensor shooting have proven effective for data collection within the lower canopy. However, these methods face challenges related to retrieving the data and sensors, often necessitating human intervention.</details> | http://arxiv.org/abs/2410.23033v1 |
| Camber-changing flapping hydrofoils for efficient and environmental-safe   water propulsion system | Luca Romanello; Leonard Hohaus; David-Marian Schmitt; Mirko Kovac; Sophie F. Armanini | 2024-10-30 | <details><summary>Click to expand</summary>This research introduces a novel hydrofoil-based propulsion framework for unmanned aquatic robots, inspired by the undulating locomotion observed in select aquatic species. The proposed system incorporates a camber-modulating mechanism to enhance hydrofoil propulsive force generation and eventually efficiency. Through dynamic simulations, we validate the effectiveness of the camber-adjusting hydrofoil compared to a symmetric counterpart. The results demonstrate a significant improvement in horizontal thrust, emphasizing the potential of the cambering approach to enhance propulsive performance. Additionally, a prototype flipper design is presented, featuring individual control of heave and pitch motions, as well as a camber-adjustment mechanism. The integrated system not only provides efficient water-based propulsion but also offers the capacity for generating vertical forces during take-off maneuvers for seaplanes. The design is tailored to harness wave energy, contributing to the exploration of alternative energy resources. This work advances the understanding of bionic oscillatory principles for aquatic robots and provides a foundation for future developments in environmentally safe and agile underwater exploration.</details> | http://arxiv.org/abs/2410.23032v1 |
| Online Intrinsic Rewards for Decision Making Agents from Large Language   Model Feedback | Qinqing Zheng; Mikael Henaff; Amy Zhang; Aditya Grover; Brandon Amos | 2024-10-30 | <details><summary>Click to expand</summary>Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples; or are limited to reward functions expressible by compact code, which may require source code and have difficulty capturing nuanced semantics; or require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets nor source code. We make our code available at \url{URL} (coming soon).</details> | http://arxiv.org/abs/2410.23022v1 |
| DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale   Synthetic Cluttered Scenes | Jialiang Zhang; Haoran Liu; Danshi Li; Xinqiang Yu; Haoran Geng; Yufei Ding; Jiayi Chen; He Wang | 2024-10-30 | <details><summary>Click to expand</summary>Grasping in cluttered scenes remains highly challenging for dexterous hands due to the scarcity of data. To address this problem, we present a large-scale synthetic benchmark, encompassing 1319 objects, 8270 scenes, and 427 million grasps. Beyond benchmarking, we also propose a novel two-stage grasping method that learns efficiently from data by using a diffusion model that conditions on local geometry. Our proposed generative method outperforms all baselines in simulation experiments. Furthermore, with the aid of test-time-depth restoration, our method demonstrates zero-shot sim-to-real transfer, attaining 90.7% real-world dexterous grasping success rate in cluttered scenes.</details> | http://arxiv.org/abs/2410.23004v1 |
| A Comparison of Prompt Engineering Techniques for Task Planning and   Execution in Service Robotics | Jonas Bode; Bastian Pätzold; Raphael Memmesheimer; Sven Behnke | 2024-10-30 | <details><summary>Click to expand</summary>Recent advances in LLM have been instrumental in autonomous robot control and human-robot interaction by leveraging their vast general knowledge and capabilities to understand and reason across a wide range of tasks and scenarios. Previous works have investigated various prompt engineering techniques for improving the performance of \glspl{LLM} to accomplish tasks, while others have proposed methods that utilize LLMs to plan and execute tasks based on the available functionalities of a given robot platform. In this work, we consider both lines of research by comparing prompt engineering techniques and combinations thereof within the application of high-level task planning and execution in service robotics. We define a diverse set of tasks and a simple set of functionalities in simulation, and measure task completion accuracy and execution time for several state-of-the-art models.</details> | http://arxiv.org/abs/2410.22997v1 |
| PDSR: Efficient UAV Deployment for Swift and Accurate Post-Disaster   Search and Rescue | Alaa Awad Abdellatif; Ali Elmancy; Amr Mohamed; Ahmed Massoud; Wadha Lebda; Khalid K. Naji | 2024-10-30 | <details><summary>Click to expand</summary>This paper introduces a comprehensive framework for Post-Disaster Search and Rescue (PDSR), aiming to optimize search and rescue operations leveraging Unmanned Aerial Vehicles (UAVs). The primary goal is to improve the precision and availability of sensing capabilities, particularly in various catastrophic scenarios. Central to this concept is the rapid deployment of UAV swarms equipped with diverse sensing, communication, and intelligence capabilities, functioning as an integrated system that incorporates multiple technologies and approaches for efficient detection of individuals buried beneath rubble or debris following a disaster. Within this framework, we propose architectural solution and address associated challenges to ensure optimal performance in real-world disaster scenarios. The proposed framework aims to achieve complete coverage of damaged areas significantly faster than traditional methods using a multi-tier swarm architecture. Furthermore, integrating multi-modal sensing data with machine learning for data fusion could enhance detection accuracy, ensuring precise identification of survivors.</details> | http://arxiv.org/abs/2410.22982v1 |
| Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices   with Hierarchical Heatmaps and Feature Propagation | Kaiqin Yang. Yixiang Dai; Guijin Wang; Siang Chen | 2024-10-30 | <details><summary>Click to expand</summary>6-DoF grasp detection is critically important for the advancement of intelligent embodied systems, as it provides feasible robot poses for object grasping. Various methods have been proposed to detect 6-DoF grasps through the extraction of 3D geometric features from RGBD or point cloud data. However, most of these approaches encounter challenges during real robot deployment due to their significant computational demands, which can be particularly problematic for mobile robot platforms, especially those reliant on edge computing devices. This paper presents an Efficient End-to-End Grasp Detection Network (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap representations. E3GNet effectively identifies high-quality and diverse grasps in cluttered real-world environments. Benefiting from our end-to-end methodology and efficient network design, our approach surpasses previous methods in model inference efficiency and achieves real-time 6-Dof grasp detection on edge devices. Furthermore, real-world experiments validate the effectiveness of our method, achieving a satisfactory 94% object grasping success rate.</details> | http://arxiv.org/abs/2410.22980v1 |
| GPTR: Gaussian Process Trajectory Representation for Continuous-Time   Motion Estimation | Thien-Minh Nguyen; Ziyu Cao; Kailai Li; Shenghai Yuan; Lihua Xie | 2024-10-30 | <details><summary>Click to expand</summary>Continuous-time trajectory representation has gained significant popularity in recent years, as it offers an elegant formulation that allows the fusion of a larger number of sensors and sensing modalities, overcoming limitations of traditional discrete-time frameworks. To bolster the adoption of the continuous-time paradigm, we propose a so-called Gaussian Process Trajectory Representation (GPTR) framework for continuous-time motion estimation (CTME) tasks. Our approach stands out by employing a third-order random jerk model, featuring closed-form expressions for both rotational and translational state derivatives. This model provides smooth, continuous trajectory representations that are crucial for precise estimation of complex motion. To support the wider robotics and computer vision communities, we have made the source code for GPTR available as a light-weight header-only library. This format was chosen for its ease of integration, allowing developers to incorporate GPTR into existing systems without needing extensive code modifications. Moreover, we also provide a set of optimization examples with LiDAR, camera, IMU, UWB factors, and closed-form analytical Jacobians under the proposed GP framework. Our experiments demonstrate the efficacy and efficiency of GP-based trajectory representation in various motion estimation tasks, and the examples can serve as the prototype to help researchers quickly develop future applications such as batch optimization, calibration, sensor fusion, trajectory planning, etc., with continuous-time trajectory representation. Our project is accessible at https://github.com/brytsknguyen/gptr .</details> | http://arxiv.org/abs/2410.22931v3 |
| An Efficient Representation of Whole-body Model Predictive Control for   Online Compliant Dual-arm Mobile Manipulation | Wenqian Du; Ran Long; João Moura; Jiayi Wang; Saeid Samadi; Sethu Vijayakumar | 2024-10-30 | <details><summary>Click to expand</summary>Dual-arm mobile manipulators can transport and manipulate large-size objects with simple end-effectors. To interact with dynamic environments with strict safety and compliance requirements, achieving whole-body motion planning online while meeting various hard constraints for such highly redundant mobile manipulators poses a significant challenge. We tackle this challenge by presenting an efficient representation of whole-body motion trajectories within our bilevel model-based predictive control (MPC) framework. We utilize B\'ezier-curve parameterization to represent the optimized collision-free trajectories of two collaborating end-effectors in the first MPC, facilitating fast long-horizon object-oriented motion planning in SE(3) while considering approximated feasibility constraints. This approach is further applied to parameterize whole-body trajectories in the second MPC for whole-body motion generation with predictive admittance control in a relatively short horizon while satisfying whole-body hard constraints. This representation enables two MPCs with continuous properties, thereby avoiding inaccurate model-state transition and dense decision-variable settings in existing MPCs using the discretization method. It strengthens the online execution of the bilevel MPC framework in high-dimensional space and facilitates the generation of consistent commands for our hybrid position/velocity-controlled robot. The simulation comparisons and real-world experiments demonstrate the efficiency and robustness of this approach in various scenarios for static and dynamic obstacle avoidance, and compliant interaction control with the manipulated object and external disturbances.</details> | http://arxiv.org/abs/2410.22910v1 |
| Human-inspired Grasping Strategies of Fresh Fruits and Vegetables   Applied to Robotic Manipulation | Romeo Orsolino; Mykhaylo Marfeychuk; Mariana de Paula Assis Fonseca; Mario Baggetta; Wesley Wimshurst; Francesco Porta; Morgan Clarke; Giovanni Berselli; Jelizaveta Konstantinova | 2024-10-30 | <details><summary>Click to expand</summary>Robotic manipulation of fresh fruits and vegetables, including the grasping of multiple loose items, has a strong industrial need but it still is a challenging task for robotic manipulation. This paper outlines the distinctive manipulation strategies used by humans to pick loose fruits and vegetables with the aim to better adopt them for robotic manipulation of diverse items. In this work we present a first version of a robotic setup designed to pick different single or multiple fresh items, featuring multi-fingered compliant robotic gripper. We analyse human grasping strategies from the perspective of industrial Key Performance Indicators (KPIs) used in the logistic sector. The robotic system was validated using the same KPIs, as well as taking into account human performance and strategies. This paper lays the foundation for future development of the robotic demonstrator for fresh fruit and vegetable intelligent manipulation, and outlines the need for generic approaches to handle the complexity of the task.</details> | http://arxiv.org/abs/2410.22893v1 |
| Non-contact Dexterous Micromanipulation with Multiple Optoelectronic   Robots | Yongyi Jia; Shu Miao; Ao Wang; Caiding Ni; Lin Feng; Xiaowo Wang; Xiang Li | 2024-10-30 | <details><summary>Click to expand</summary>Micromanipulation systems leverage automation and robotic technologies to improve the precision, repeatability, and efficiency of various tasks at the microscale. However, current approaches are typically limited to specific objects or tasks, which necessitates the use of custom tools and specialized grasping methods. This paper proposes a novel non-contact micromanipulation method based on optoelectronic technologies. The proposed method utilizes repulsive dielectrophoretic forces generated in the optoelectronic field to drive a microrobot, enabling the microrobot to push the target object in a cluttered environment without physical contact. The non-contact feature can minimize the risks of potential damage, contamination, or adhesion while largely improving the flexibility of manipulation. The feature enables the use of a general tool for indirect object manipulation, eliminating the need for specialized tools. A series of simulation studies and real-world experiments -- including non-contact trajectory tracking, obstacle avoidance, and reciprocal avoidance between multiple microrobots -- are conducted to validate the performance of the proposed method. The proposed formulation provides a general and dexterous solution for a range of objects and tasks at the micro scale.</details> | http://arxiv.org/abs/2410.22848v1 |
| Grasping Force Estimation for Markerless Visuotactile Sensors | Julio Castaño-Amoros; Pablo Gil | 2024-10-30 | <details><summary>Click to expand</summary>Tactile sensors have been used for force estimation in the past, especially Vision-Based Tactile Sensors (VBTS) have recently become a new trend due to their high spatial resolution and low cost. In this work, we have designed and implemented several approaches to estimate the normal grasping force using different types of markerless visuotactile representations obtained from VBTS. Our main goal is to determine the most appropriate visuotactile representation, based on a performance analysis during robotic grasping tasks. Our proposal has been tested on the dataset generated with our DIGIT sensors and another one obtained using GelSight Mini sensors from another state-of-the-art work. We have also tested the generalization capabilities of our best approach, called RGBmod. The results led to two main conclusions. First, the RGB visuotactile representation is a better input option than the depth image or a combination of the two for estimating normal grasping forces. Second, RGBmod achieved a good performance when tested on 10 unseen everyday objects in real-world scenarios, achieving an average relative error of 0.125 +- 0.153. Furthermore, we show that our proposal outperforms other works in the literature that use RGB and depth information for the same task.</details> | http://arxiv.org/abs/2410.22825v1 |
| Enhancing Tool Manipulation of An Aerial Vehicle with A Dynamically   Displacing Center-of-Mass | Tong Hui; Matteo Fumagalli | 2024-10-30 | <details><summary>Click to expand</summary>As aerial robots gain traction in industrial applications, there is growing interest in enhancing their physical interaction capabilities. Pushing tasks performed by aerial manipulators have been successfully demonstrated in contact-based inspections. However, more complex industrial applications require these systems to support higher-DoF (Degree of Freedom) manipulators and generate larger forces while pushing (e.g., drilling, grinding). This paper builds on our previous work, where we introduced an aerial vehicle with a dynamically displacing CoM (Center of Mass) to improve force exertion during interactions. We propose a novel approach to further enhance this system's force generation by optimizing its CoM location during interactions. Additionally, we study the case of this aerial vehicle equipped with a 2-DoF manipulation arm to extend the system's functionality in tool-based tasks. The effectiveness of the proposed methods is validated through simulations, demonstrating the potential of this system for advanced aerial manipulation in practical settings.</details> | http://arxiv.org/abs/2410.22816v1 |
| SoftCTRL: Soft conservative KL-control of Transformer Reinforcement   Learning for Autonomous Driving | Minh Tri Huynh; Duc Dung Nguyen | 2024-10-30 | <details><summary>Click to expand</summary>In recent years, motion planning for urban self-driving cars (SDV) has become a popular problem due to its complex interaction of road components. To tackle this, many methods have relied on large-scale, human-sampled data processed through Imitation learning (IL). Although effective, IL alone cannot adequately handle safety and reliability concerns. Combining IL with Reinforcement learning (RL) by adding KL divergence between RL and IL policy to the RL loss can alleviate IL's weakness but suffer from over-conservation caused by covariate shift of IL. To address this limitation, we introduce a method that combines IL with RL using an implicit entropy-KL control that offers a simple way to reduce the over-conservation characteristic. In particular, we validate different challenging simulated urban scenarios from the unseen dataset, indicating that although IL can perform well in imitation tasks, our proposed method significantly improves robustness (over 17\% reduction in failures) and generates human-like driving behavior.</details> | http://arxiv.org/abs/2410.22752v1 |
| Robotic State Recognition with Image-to-Text Retrieval Task of   Pre-Trained Vision-Language Model and Black-Box Optimization | Kento Kawaharazuka; Yoshiki Obinata; Naoaki Kanazawa; Kei Okada; Masayuki Inaba | 2024-10-30 | <details><summary>Click to expand</summary>State recognition of the environment and objects, such as the open/closed state of doors and the on/off of lights, is indispensable for robots that perform daily life support and security tasks. Until now, state recognition methods have been based on training neural networks from manual annotations, preparing special sensors for the recognition, or manually programming to extract features from point clouds or raw images. In contrast, we propose a robotic state recognition method using a pre-trained vision-language model, which is capable of Image-to-Text Retrieval (ITR) tasks. We prepare several kinds of language prompts in advance, calculate the similarity between these prompts and the current image by ITR, and perform state recognition. By applying the optimal weighting to each prompt using black-box optimization, state recognition can be performed with higher accuracy. Experiments show that this theory enables a variety of state recognitions by simply preparing multiple prompts without retraining neural networks or manual programming. In addition, since only prompts and their weights need to be prepared for each recognizer, there is no need to prepare multiple models, which facilitates resource management. It is possible to recognize the open/closed state of transparent doors, the state of whether water is running or not from a faucet, and even the qualitative state of whether a kitchen is clean or not, which have been challenging so far, through language.</details> | http://arxiv.org/abs/2410.22707v1 |
| MiniTac: An Ultra-Compact 8 mm Vision-Based Tactile Sensor for Enhanced   Palpation in Robot-Assisted Minimally Invasive Surgery | Wanlin Li; Zihang Zhao; Leiyao Cui; Weiyi Zhang; Hangxin Liu; Li-An Li; Yixin Zhu | 2024-10-30 | <details><summary>Click to expand</summary>Robot-assisted minimally invasive surgery (RAMIS) provides substantial benefits over traditional open and laparoscopic methods. However, a significant limitation of RAMIS is the surgeon's inability to palpate tissues, a crucial technique for examining tissue properties and detecting abnormalities, restricting the widespread adoption of RAMIS. To overcome this obstacle, we introduce MiniTac, a novel vision-based tactile sensor with an ultra-compact cross-sectional diameter of 8 mm, designed for seamless integration into mainstream RAMIS devices, particularly the Da Vinci surgical systems. MiniTac features a novel mechanoresponsive photonic elastomer membrane that changes color distribution under varying contact pressures. This color change is captured by an embedded miniature camera, allowing MiniTac to detect tumors both on the tissue surface and in deeper layers typically obscured from endoscopic view. MiniTac's efficacy has been rigorously tested on both phantoms and ex-vivo tissues. By leveraging advanced mechanoresponsive photonic materials, MiniTac represents a significant advancement in integrating tactile sensing into RAMIS, potentially expanding its applicability to a wider array of clinical scenarios that currently rely on traditional surgical approaches.</details> | http://arxiv.org/abs/2410.22691v1 |
| Multi-Task Interactive Robot Fleet Learning with Visual World Models | Huihan Liu; Yu Zhang; Vaarij Betala; Evan Zhang; James Liu; Crystal Ding; Yuke Zhu | 2024-10-30 | <details><summary>Click to expand</summary>Recent advancements in large-scale multi-task robot learning offer the potential for deploying robot fleets in household and industrial settings, enabling them to perform diverse tasks across various environments. However, AI-enabled robots often face challenges with generalization and robustness when exposed to real-world variability and uncertainty. We introduce Sirius-Fleet, a multi-task interactive robot fleet learning framework to address these challenges. Sirius-Fleet monitors robot performance during deployment and involves humans to correct the robot's actions when necessary. We employ a visual world model to predict the outcomes of future actions and build anomaly predictors to predict whether they will likely result in anomalies. As the robot autonomy improves, the anomaly predictors automatically adapt their prediction criteria, leading to fewer requests for human intervention and gradually reducing human workload over time. Evaluations on large-scale benchmarks demonstrate Sirius-Fleet's effectiveness in improving multi-task policy performance and monitoring accuracy. We demonstrate Sirius-Fleet's performance in both RoboCasa in simulation and Mutex in the real world, two diverse, large-scale multi-task benchmarks. More information is available on the project website: https://ut-austin-rpl.github.io/sirius-fleet</details> | http://arxiv.org/abs/2410.22689v1 |
| IM-GIV: an effective integrity monitoring scheme for tightly-coupled   GNSS/INS/Vision integration based on factor graph optimization | Yunong Tian; Tuan Li; Haitao Jiang; Zhipeng Wang; Chuang Shi | 2024-10-30 | <details><summary>Click to expand</summary>Global Navigation Satellite System/Inertial Navigation System (GNSS/INS)/Vision integration based on factor graph optimization (FGO) has recently attracted extensive attention in navigation and robotics community. Integrity monitoring (IM) capability is required when FGO-based integrated navigation system is used for safety-critical applications. However, traditional researches on IM of integrated navigation system are mostly based on Kalman filter. It is urgent to develop effective IM scheme for FGO-based GNSS/INS/Vision integration. In this contribution, the position error bounding formula to ensure the integrity of the GNSS/INS/Vision integration based on FGO is designed and validated for the first time. It can be calculated by the linearized equations from the residuals of GNSS pseudo-range, IMU pre-integration and visual measurements. The specific position error bounding is given in the case of GNSS, INS and visual measurement faults. Field experiments were conducted to evaluate and validate the performance of the proposed position error bounding. Experimental results demonstrate that the proposed position error bounding for the GNSS/INS/Vision integration based on FGO can correctly fit the position error against different fault modes, and the availability of integrity in six fault modes is 100% after correct and timely fault exclusion.</details> | http://arxiv.org/abs/2410.22672v1 |
| $\textbf{EMOS}$: $\textbf{E}$mbodiment-aware Heterogeneous   $\textbf{M}$ulti-robot $\textbf{O}$perating $\textbf{S}$ystem with LLM Agents | Junting Chen; Checheng Yu; Xunzhe Zhou; Tianqi Xu; Yao Mu; Mengkang Hu; Wenqi Shao; Yikai Wang; Guohao Li; Lin Shao | 2024-10-30 | <details><summary>Click to expand</summary>Heterogeneous multi-robot systems (HMRS) have emerged as a powerful approach for tackling complex tasks that single robots cannot manage alone. Current large-language-model-based multi-agent systems (LLM-based MAS) have shown success in areas like software development and operating systems, but applying these systems to robot control presents unique challenges. In particular, the capabilities of each agent in a multi-robot system are inherently tied to the physical composition of the robots, rather than predefined roles. To address this issue, we introduce a novel multi-agent framework designed to enable effective collaboration among heterogeneous robots with varying embodiments and capabilities, along with a new benchmark named Habitat-MAS. One of our key designs is $\textit{Robot Resume}$: Instead of adopting human-designed role play, we propose a self-prompted approach, where agents comprehend robot URDF files and call robot kinematics tools to generate descriptions of their physics capabilities to guide their behavior in task planning and action execution. The Habitat-MAS benchmark is designed to assess how a multi-agent framework handles tasks that require embodiment-aware reasoning, which includes 1) manipulation, 2) perception, 3) navigation, and 4) comprehensive multi-floor object rearrangement. The experimental results indicate that the robot's resume and the hierarchical design of our multi-agent system are essential for the effective operation of the heterogeneous multi-robot system within this intricate problem context.</details> | http://arxiv.org/abs/2410.22662v1 |
| An Overtaking Trajectory Planning Framework Based on Spatio-temporal   Topology and Reachable Set Analysis Ensuring Time Efficiency | Wule Mao; Zhouheng Li; Lei Xie; Hongye Su | 2024-10-30 | <details><summary>Click to expand</summary>Generating overtaking trajectories in high-speed scenarios presents significant challenges and is typically addressed through hierarchical planning methods. However, this method has two primary drawbacks. First, heuristic algorithms can only provide a single initial solution, which may lead to local optima and consequently diminish the quality of the solution. Second, the time efficiency of trajectory refinement based on numerical optimization is insufficient. To overcome these limitations, this paper proposes an overtaking trajectory planning framework based on spatio-temporal topology and reachable set analysis (SROP), to improve trajectory quality and time efficiency. Specifically, this paper introduces topological classes to describe trajectories representing different overtaking behaviors, which support the spatio-temporal topological search method employed by the upper-layer planner to identify diverse initial paths. This approach helps prevent getting stuck in local optima, enhancing the overall solution quality by considering multiple initial solutions from distinct topologies. Moreover, the reachable set method is integrated into the lower-layer planner for parallel trajectory evaluation. This method enhances planning efficiency by decoupling vehicle model constraints from the optimization process, enabling parallel computation while ensuring control feasibility. Simulation results show that the proposed method improves the smoothness of generated trajectories by 66.8% compared to state-of-the-art methods, highlighting its effectiveness in enhancing trajectory quality. Additionally, this method reduces computation time by 62.9%, demonstrating its efficiency.</details> | http://arxiv.org/abs/2410.22643v1 |
| Symbolic Graph Inference for Compound Scene Understanding | FNU Aryan; Simon Stepputtis; Sarthak Bhagat; Joseph Campbell; Kwonjoon Lee; Hossein Nourkhiz Mahjoub; Katia Sycara | 2024-10-30 | <details><summary>Click to expand</summary>Scene understanding is a fundamental capability needed in many domains, ranging from question-answering to robotics. Unlike recent end-to-end approaches that must explicitly learn varying compositions of the same scene, our method reasons over their constituent objects and analyzes their arrangement to infer a scene's meaning. We propose a novel approach that reasons over a scene's scene- and knowledge-graph, capturing spatial information while being able to utilize general domain knowledge in a joint graph search. Empirically, we demonstrate the feasibility of our method on the ADE20K dataset and compare it to current scene understanding approaches.</details> | http://arxiv.org/abs/2410.22626v1 |
| Solving Minimum-Cost Reach Avoid using Reinforcement Learning | Oswin So; Cheng Ge; Chuchu Fan | 2024-10-29 | <details><summary>Click to expand</summary>Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.</details> | http://arxiv.org/abs/2410.22600v1 |
| Continuous-Time Line-of-Sight Constrained Trajectory Planning for   6-Degree of Freedom Systems | Christopher R. Hayner; John M. Carson III; Behçet Açıkmeşe; Karen Leung | 2024-10-29 | <details><summary>Click to expand</summary>Perception algorithms are ubiquitous in modern autonomy stacks, providing necessary environmental information to operate in the real world. Many of these algorithms depend on the visibility of keypoints, which must remain within the robot's line-of-sight (LoS), for reliable operation. This paper tackles the challenge of maintaining LoS on such keypoints during robot movement. We propose a novel method that addresses these issues by ensuring applicability to various sensor footprints, adaptability to arbitrary nonlinear dynamics, and constant enforcement of LoS throughout the robot's path. Through our experiments, we show that the proposed approach achieves significantly reduced LoS violation and runtime compared to existing state-of-the-art methods in several representative and challenging scenarios.</details> | http://arxiv.org/abs/2410.22596v1 |
| Pre-Trained Vision Models as Perception Backbones for Safety Filters in   Autonomous Driving | Yuxuan Yang; Hussein Sibai | 2024-10-29 | <details><summary>Click to expand</summary>End-to-end vision-based autonomous driving has achieved impressive success, but safety remains a major concern. The safe control problem has been addressed in low-dimensional settings using safety filters, e.g., those based on control barrier functions. Designing safety filters for vision-based controllers in the high-dimensional settings of autonomous driving can similarly alleviate the safety problem, but is significantly more challenging. In this paper, we address this challenge by using frozen pre-trained vision representation models as perception backbones to design vision-based safety filters, inspired by these models' success as backbones of robotic control policies. We empirically evaluate the offline performance of four common pre-trained vision models in this context. We try three existing methods for training safety filters for black-box dynamics, as the dynamics over representation spaces are not known. We use the DeepAccident dataset that consists of action-annotated videos from multiple cameras on vehicles in CARLA simulating real accident scenarios. Our results show that the filters resulting from our approach are competitive with the ones that are given the ground truth state of the ego vehicle and its environment.</details> | http://arxiv.org/abs/2410.22585v1 |
| Analytical Solution for Inverse Kinematics | Serdar Kalaycioglu; Anton de Ruiter; Ethan Fung; Harrison Zhang; Haipeng Xie | 2024-10-29 | <details><summary>Click to expand</summary>This paper introduces a closed-form analytical solution for the inverse kinematics (IK) of a 6 Degrees of Freedom (DOF) serial robotic manipulator arm, configured with six revolute joints and utilized within the Lunar Exploration Rover System (LERS). As a critical asset for conducting precise operations in the demanding lunar environment, this robotic arm relies on the IK solution to determine joint parameters required for precise end-effector positioning, essential for tasks such as sample collection, infrastructure assembly, and equipment deployment. By applying geometric principles, the proposed method offers a highly efficient and accurate approach to solving the IK problem, significantly reducing computational demands compared to traditional numerical methods. This advancement not only enhances real-time operational capabilities but is also optimized for space robotics, where precision and speed are critical. Additionally, the paper explores the integration of the LERS robotic system, underscoring the importance of this work in supporting autonomous lunar exploration within the ARTEMIS program and future missions</details> | http://arxiv.org/abs/2410.22582v1 |
| Energy-Aware Multi-Agent Reinforcement Learning for Collaborative   Execution in Mission-Oriented Drone Networks | Ying Li; Changling Li; Jiyao Chen; Christine Roinou | 2024-10-29 | <details><summary>Click to expand</summary>Mission-oriented drone networks have been widely used for structural inspection, disaster monitoring, border surveillance, etc. Due to the limited battery capacity of drones, mission execution strategy impacts network performance and mission completion. However, collaborative execution is a challenging problem for drones in such a dynamic environment as it also involves efficient trajectory design. We leverage multi-agent reinforcement learning (MARL) to manage the challenge in this study, letting each drone learn to collaboratively execute tasks and plan trajectories based on its current status and environment. Simulation results show that the proposed collaborative execution model can successfully complete the mission at least 80% of the time, regardless of task locations and lengths, and can even achieve a 100% success rate when the task density is not way too sparse. To the best of our knowledge, our work is one of the pioneer studies on leveraging MARL on collaborative execution for mission-oriented drone networks; the unique value of this work lies in drone battery level driving our model design.</details> | http://arxiv.org/abs/2410.22578v1 |
| Intelligent Mobility System with Integrated Motion Planning and Control   Utilizing Infrastructure Sensor Nodes | Yufeng Yang; Minghao Ning; Shucheng Huang; Ehsan Hashemi; Amir Khajepour | 2024-10-29 | <details><summary>Click to expand</summary>This paper introduces a framework for an indoor autonomous mobility system that can perform patient transfers and materials handling. Unlike traditional systems that rely on onboard perception sensors, the proposed approach leverages a global perception and localization (PL) through Infrastructure Sensor Nodes (ISNs) and cloud computing technology. Using the global PL, an integrated Model Predictive Control (MPC)-based local planning and tracking controller augmented with Artificial Potential Field (APF) is developed, enabling reliable and efficient motion planning and obstacle avoidance ability while tracking predefined reference motions. Simulation results demonstrate the effectiveness of the proposed MPC controller in smoothly navigating around both static and dynamic obstacles. The proposed system has the potential to extend to intelligent connected autonomous vehicles, such as electric or cargo transport vehicles with four-wheel independent drive/steering (4WID-4WIS) configurations.</details> | http://arxiv.org/abs/2410.22527v1 |
| Heterogeneous Team Coordination on Partially Observable Graphs with   Realistic Communication | Yanlin Zhou; Manshi Limbu; Xuan Wang; Daigo Shishika; Xuesu Xiao | 2024-10-29 | <details><summary>Click to expand</summary>Team Coordination on Graphs with Risky Edges (\textsc{tcgre}) is a recently proposed problem, in which robots find paths to their goals while considering possible coordination to reduce overall team cost. However, \textsc{tcgre} assumes that the \emph{entire} environment is available to a \emph{homogeneous} robot team with \emph{ubiquitous} communication. In this paper, we study an extended version of \textsc{tcgre}, called \textsc{hpr-tcgre}, with three relaxations: Heterogeneous robots, Partial observability, and Realistic communication. To this end, we form a new combinatorial optimization problem on top of \textsc{tcgre}. After analysis, we divide it into two sub-problems, one for robots moving individually, another for robots in groups, depending on their communication availability. Then, we develop an algorithm that exploits real-time partial maps to solve local shortest path(s) problems, with a A*-like sub-goal(s) assignment mechanism that explores potential coordination opportunities for global interests. Extensive experiments indicate that our algorithm is able to produce team coordination behaviors in order to reduce overall cost even with our three relaxations.</details> | http://arxiv.org/abs/2410.22482v1 |
| Designing robot swarms: a puzzle, a problem, and a mess | David Garzón Ramos; Sabine Hauert | 2024-10-29 | <details><summary>Click to expand</summary>Framing an issue as a puzzle, problem, or mess is an illustrative approach to characterizing the issue's complexity within organizational theory and systems thinking. We use this approach to characterize the issue of designing collective behaviors for robot swarms and discuss how various research goals have shaped the current state of the field. We contextualize our discussion at these three levels by highlighting relevant literature. Our aim is to emphasize key challenges that arise in the development of robot swarms for real-world applications and to motivate further work on promising research directions.</details> | http://arxiv.org/abs/2410.22478v1 |
| How Artists Improvise and Provoke Robotics | Steve Benford; Rachael Garrett; Eike Schneiders; Paul Tennent; Alan Chamberlain; Juan Avila; Pat Brundell; Simon Castle-Green | 2024-10-29 | <details><summary>Click to expand</summary>We explore transdisciplinary collaborations between artists and roboticists across a portfolio of artworks. Brendan Walker's Broncomatic was a breath controlled mechanical rodeo bull ride. Blast Theory's Cat Royale deployed a robot arm to play with a family of three cats for twelve days. Different Bodies is a prototype improvised dance performance in which dancers with disabilities physically manipulate two mirrored robot arms. We reflect on these to explore how artists shape robotics research through the two key strategies of improvisation and provocation. Artists are skilled at improvising extended robot experiences that surface opportunities for technology-focused design, but which also require researchers to improvise their research processes. Artists may provoke audiences into reflecting on the societal implications of robots, but at the same time challenge the established techno-centric concepts, methods and underlying epistemology of robotics research.</details> | http://arxiv.org/abs/2410.22462v1 |
| Local Policies Enable Zero-shot Long-horizon Manipulation | Murtaza Dalal; Min Liu; Walter Talbott; Chen Chen; Deepak Pathak; Jian Zhang; Ruslan Salakhutdinov | 2024-10-29 | <details><summary>Click to expand</summary>Sim2real for robotic manipulation is difficult due to the challenges of simulating complex contacts and generating realistic task distributions. To tackle the latter problem, we introduce ManipGen, which leverages a new class of policies for sim2real transfer: local policies. Locality enables a variety of appealing properties including invariances to absolute robot and object pose, skill ordering, and global scene configuration. We combine these policies with foundation models for vision, language and motion planning and demonstrate SOTA zero-shot performance of our method to Robosuite benchmark tasks in simulation (97%). We transfer our local policies from simulation to reality and observe they can solve unseen long-horizon manipulation tasks with up to 8 stages with significant pose, object and scene configuration variation. ManipGen outperforms SOTA approaches such as SayCan, OpenVLA, LLMTrajGen and VoxPoser across 50 real-world manipulation tasks by 36%, 76%, 62% and 60% respectively. Video results at https://mihdalal.github.io/manipgen/</details> | http://arxiv.org/abs/2410.22332v1 |
| Robots Pre-train Robots: Manipulation-Centric Robotic Representation   from Large-Scale Robot Datasets | Guangqi Jiang; Yifei Sun; Tao Huang; Huanyu Li; Yongyuan Liang; Huazhe Xu | 2024-10-29 | <details><summary>Click to expand</summary>The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). Interestingly, we find that the "manipulation centricity" is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose Manipulation Centric Representation (MCR), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with a behavior cloning (BC)-like actor loss to predict actions during pre-training, along with a time contrastive loss. Empirical results across 4 simulation domains with 20 tasks verify that MCR outperforms the strongest baseline method by 14.8%. Moreover, MCR boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by 76.9%. Project website: https://robots-pretrain-robots.github.io/.</details> | http://arxiv.org/abs/2410.22325v2 |
| A Large Recurrent Action Model: xLSTM enables Fast Inference for   Robotics Tasks | Thomas Schmied; Thomas Adler; Vihang Patil; Maximilian Beck; Korbinian Pöppel; Johannes Brandstetter; Günter Klambauer; Razvan Pascanu; Sepp Hochreiter | 2024-10-29 | <details><summary>Click to expand</summary>In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.</details> | http://arxiv.org/abs/2410.22391v1 |
| An Efficient Approach to Generate Safe Drivable Space by   LiDAR-Camera-HDmap Fusion | Minghao Ning; Ahmad Reza Alghooneh; Chen Sun; Ruihe Zhang; Pouya Panahandeh; Steven Tuer; Ehsan Hashemi; Amir Khajepour | 2024-10-29 | <details><summary>Click to expand</summary>In this paper, we propose an accurate and robust perception module for Autonomous Vehicles (AVs) for drivable space extraction. Perception is crucial in autonomous driving, where many deep learning-based methods, while accurate on benchmark datasets, fail to generalize effectively, especially in diverse and unpredictable environments. Our work introduces a robust easy-to-generalize perception module that leverages LiDAR, camera, and HD map data fusion to deliver a safe and reliable drivable space in all weather conditions. We present an adaptive ground removal and curb detection method integrated with HD map data for enhanced obstacle detection reliability. Additionally, we propose an adaptive DBSCAN clustering algorithm optimized for precipitation noise, and a cost-effective LiDAR-camera frustum association that is resilient to calibration discrepancies. Our comprehensive drivable space representation incorporates all perception data, ensuring compatibility with vehicle dimensions and road regulations. This approach not only improves generalization and efficiency, but also significantly enhances safety in autonomous vehicle operations. Our approach is tested on a real dataset and its reliability is verified during the daily (including harsh snowy weather) operation of our autonomous shuttle, WATonoBus</details> | http://arxiv.org/abs/2410.22314v1 |
| Senna: Bridging Large Vision-Language Models and End-to-End Autonomous   Driving | Bo Jiang; Shaoyu Chen; Bencheng Liao; Xingyu Zhang; Wei Yin; Qian Zhang; Chang Huang; Wenyu Liu; Xinggang Wang | 2024-10-29 | <details><summary>Click to expand</summary>End-to-end autonomous driving demonstrates strong planning capabilities with large-scale data but still struggles in complex, rare scenarios due to limited commonsense. In contrast, Large Vision-Language Models (LVLMs) excel in scene understanding and reasoning. The path forward lies in merging the strengths of both approaches. Previous methods using LVLMs to predict trajectories or control signals yield suboptimal results, as LVLMs are not well-suited for precise numerical predictions. This paper presents Senna, an autonomous driving system combining an LVLM (Senna-VLM) with an end-to-end model (Senna-E2E). Senna decouples high-level planning from low-level trajectory prediction. Senna-VLM generates planning decisions in natural language, while Senna-E2E predicts precise trajectories. Senna-VLM utilizes a multi-image encoding approach and multi-view prompts for efficient scene understanding. Besides, we introduce planning-oriented QAs alongside a three-stage training strategy, which enhances Senna-VLM's planning performance while preserving commonsense. Extensive experiments on two datasets show that Senna achieves state-of-the-art planning performance. Notably, with pre-training on a large-scale dataset DriveX and fine-tuning on nuScenes, Senna significantly reduces average planning error by 27.12% and collision rate by 33.33% over model without pre-training. We believe Senna's cross-scenario generalization and transferability are essential for achieving fully autonomous driving. Code and models will be released at https://github.com/hustvl/Senna.</details> | http://arxiv.org/abs/2410.22313v1 |
| Environment as Policy: Learning to Race in Unseen Tracks | Hongze Wang; Jiaxu Xing; Nico Messikommer; Davide Scaramuzza | 2024-10-29 | <details><summary>Click to expand</summary>Reinforcement learning (RL) has achieved outstanding success in complex robot control tasks, such as drone racing, where the RL agents have outperformed human champions in a known racing track. However, these agents fail in unseen track configurations, always requiring complete retraining when presented with new track layouts. This work aims to develop RL agents that generalize effectively to novel track configurations without retraining. The naive solution of training directly on a diverse set of track layouts can overburden the agent, resulting in suboptimal policy learning as the increased complexity of the environment impairs the agent's ability to learn to fly. To enhance the generalizability of the RL agent, we propose an adaptive environment-shaping framework that dynamically adjusts the training environment based on the agent's performance. We achieve this by leveraging a secondary RL policy to design environments that strike a balance between being challenging and achievable, allowing the agent to adapt and improve progressively. Using our adaptive environment shaping, one single racing policy efficiently learns to race in diverse challenging tracks. Experimental results validated in both simulation and the real world show that our method enables drones to successfully fly complex and unseen race tracks, outperforming existing environment-shaping techniques. Project page: http://rpg.ifi.uzh.ch/env_as_policy/index.html</details> | http://arxiv.org/abs/2410.22308v1 |
| Multi-Object 3D Grounding with Dynamic Modules and Language-Informed   Spatial Attention | Haomeng Zhang; Chiao-An Yang; Raymond A. Yeh | 2024-10-29 | <details><summary>Click to expand</summary>Multi-object 3D Grounding involves locating 3D boxes based on a given query phrase from a point cloud. It is a challenging and significant task with numerous applications in visual understanding, human-computer interaction, and robotics. To tackle this challenge, we introduce D-LISA, a two-stage approach incorporating three innovations. First, a dynamic vision module that enables a variable and learnable number of box proposals. Second, a dynamic camera positioning that extracts features for each proposal. Third, a language-informed spatial attention module that better reasons over the proposals to output the final prediction. Empirically, experiments show that our method outperforms the state-of-the-art methods on multi-object 3D grounding by 12.8% (absolute) and is competitive in single-object 3D grounding.</details> | http://arxiv.org/abs/2410.22306v1 |
| LipKernel: Lipschitz-Bounded Convolutional Neural Networks via   Dissipative Layers | Patricia Pauli; Ruigang Wang; Ian Manchester; Frank Allgöwer | 2024-10-29 | <details><summary>Click to expand</summary>We propose a novel layer-wise parameterization for convolutional neural networks (CNNs) that includes built-in robustness guarantees by enforcing a prescribed Lipschitz bound. Each layer in our parameterization is designed to satisfy a linear matrix inequality (LMI), which in turn implies dissipativity with respect to a specific supply rate. Collectively, these layer-wise LMIs ensure Lipschitz boundedness for the input-output mapping of the neural network, yielding a more expressive parameterization than through spectral bounds or orthogonal layers. Our new method LipKernel directly parameterizes dissipative convolution kernels using a 2-D Roesser-type state space model. This means that the convolutional layers are given in standard form after training and can be evaluated without computational overhead. In numerical experiments, we show that the run-time using our method is orders of magnitude faster than state-of-the-art Lipschitz-bounded networks that parameterize convolutions in the Fourier domain, making our approach particularly attractive for improving robustness of learning-based real-time perception or control in robotics, autonomous vehicles, or automation systems. We focus on CNNs, and in contrast to previous works, our approach accommodates a wide variety of layers typically used in CNNs, including 1-D and 2-D convolutional layers, maximum and average pooling layers, as well as strided and dilated convolutions and zero padding. However, our approach naturally extends beyond CNNs as we can incorporate any layer that is incrementally dissipative.</details> | http://arxiv.org/abs/2410.22258v1 |
| CaStL: Constraints as Specifications through LLM Translation for   Long-Horizon Task and Motion Planning | Weihang Guo; Zachary Kingston; Lydia E. Kavraki | 2024-10-29 | <details><summary>Click to expand</summary>Large Language Models (LLMs) have demonstrated remarkable ability in long-horizon Task and Motion Planning (TAMP) by translating clear and straightforward natural language problems into formal specifications such as the Planning Domain Definition Language (PDDL). However, real-world problems are often ambiguous and involve many complex constraints. In this paper, we introduce Constraints as Specifications through LLMs (CaStL), a framework that identifies constraints such as goal conditions, action ordering, and action blocking from natural language in multiple stages. CaStL translates these constraints into PDDL and Python scripts, which are solved using an custom PDDL solver. Tested across three PDDL domains, CaStL significantly improves constraint handling and planning success rates from natural language specification in complex scenarios.</details> | http://arxiv.org/abs/2410.22225v1 |
| EnvoDat: A Large-Scale Multisensory Dataset for Robotic Spatial   Awareness and Semantic Reasoning in Heterogeneous Environments | Linus Nwankwo; Bjoern Ellensohn; Vedant Dave; Peter Hofer; Jan Forstner; Marlene Villneuve; Robert Galler; Elmar Rueckert | 2024-10-29 | <details><summary>Click to expand</summary>To ensure the efficiency of robot autonomy under diverse real-world conditions, a high-quality heterogeneous dataset is essential to benchmark the operating algorithms' performance and robustness. Current benchmarks predominantly focus on urban terrains, specifically for on-road autonomous driving, leaving multi-degraded, densely vegetated, dynamic and feature-sparse environments, such as underground tunnels, natural fields, and modern indoor spaces underrepresented. To fill this gap, we introduce EnvoDat, a large-scale, multi-modal dataset collected in diverse environments and conditions, including high illumination, fog, rain, and zero visibility at different times of the day. Overall, EnvoDat contains 26 sequences from 13 scenes, 10 sensing modalities, over 1.9TB of data, and over 89K fine-grained polygon-based annotations for more than 82 object and terrain classes. We post-processed EnvoDat in different formats that support benchmarking SLAM and supervised learning algorithms, and fine-tuning multimodal vision models. With EnvoDat, we contribute to environment-resilient robotic autonomy in areas where the conditions are extremely challenging. The datasets and other relevant resources can be accessed through https://linusnep.github.io/EnvoDat/.</details> | http://arxiv.org/abs/2410.22200v1 |
| 4D-based Robot Navigation Using Relativistic Image Processing | Simone Müller; Dieter Kranzlmüller | 2024-10-29 | <details><summary>Click to expand</summary>Machine perception is an important prerequisite for safe interaction and locomotion in dynamic environments. This requires not only the timely perception of surrounding geometries and distances but also the ability to react to changing situations through predefined, learned but also reusable skill endings of a robot so that physical damage or bodily harm can be avoided. In this context, 4D perception offers the possibility of predicting one's own position and changes in the environment over time. In this paper, we present a 4D-based approach to robot navigation using relativistic image processing. Relativistic image processing handles the temporal-related sensor information in a tensor model within a constructive 4D space. 4D-based navigation expands the causal understanding and the resulting interaction radius of a robot through the use of visual and sensory 4D information.</details> | http://arxiv.org/abs/2410.22087v1 |
| PACA: Perspective-Aware Cross-Attention Representation for Zero-Shot   Scene Rearrangement | Shutong Jin; Ruiyu Wang; Kuangyi Chen; Florian T. Pokorny | 2024-10-29 | <details><summary>Click to expand</summary>Scene rearrangement, like table tidying, is a challenging task in robotic manipulation due to the complexity of predicting diverse object arrangements. Web-scale trained generative models such as Stable Diffusion can aid by generating natural scenes as goals. To facilitate robot execution, object-level representations must be extracted to match the real scenes with the generated goals and to calculate object pose transformations. Current methods typically use a multi-step design that involves separate models for generation, segmentation, and feature encoding, which can lead to a low success rate due to error accumulation. Furthermore, they lack control over the viewing perspectives of the generated goals, restricting the tasks to 3-DoF settings. In this paper, we propose PACA, a zero-shot pipeline for scene rearrangement that leverages perspective-aware cross-attention representation derived from Stable Diffusion. Specifically, we develop a representation that integrates generation, segmentation, and feature encoding into a single step to produce object-level representations. Additionally, we introduce perspective control, thus enabling the matching of 6-DoF camera views and extending past approaches that were limited to 3-DoF top-down views. The efficacy of our method is demonstrated through its zero-shot performance in real robot experiments across various scenes, achieving an average matching accuracy and execution success rate of 87% and 67%, respectively.</details> | http://arxiv.org/abs/2410.22059v1 |
| On the Synthesis of Reactive Collision-Free Whole-Body Robot Motions: A   Complementarity-based Approach | Haowen Yao; Riddhiman Laha; Anirban Sinha; Jonas Hall; Luis F. C. Figueredo; Nilanjan Chakraborty; Sami Haddadin | 2024-10-29 | <details><summary>Click to expand</summary>This paper is about generating motion plans for high degree-of-freedom systems that account for collisions along the entire body. A particular class of mathematical programs with complementarity constraints become useful in this regard. Optimization-based planners can tackle confined-space trajectory planning while being cognizant of robot constraints. However, introducing obstacles in this setting transforms the formulation into a non-convex problem (oftentimes with ill-posed bilinear constraints), which is non-trivial in a real-time setting. To this end, we present the FLIQC (Fast LInear Quadratic Complementarity based) motion planner. Our planner employs a novel motion model that captures the entire rigid robot as well as the obstacle geometry and ensures non-penetration between the surfaces due to the imposed constraint. We perform thorough comparative studies with the state-of-the-art, which demonstrate improved performance. Extensive simulation and hardware experiments validate our claim of generating continuous and reactive motion plans at 1 kHz for modern collaborative robots with constant minimal parameters.</details> | http://arxiv.org/abs/2410.22049v1 |
| A Degree of Flowability for Virtual Tubes | Quan Quan; Shuhan Huang; Kai-Yuan Cai | 2024-10-29 | <details><summary>Click to expand</summary>With the rapid development of robotics swarm technology, there are more tasks that require the swarm to pass through complicated environments safely and efficiently. Virtual tube technology is a novel way to achieve this goal. Virtual tubes are free spaces connecting two places that provide safety boundaries and direction of motion for swarm robotics. How to determine the design quality of a virtual tube is a fundamental problem. For such a purpose, this paper presents a degree of flowability (DOF) for two-dimensional virtual tubes according to a minimum energy principle. After that, methods to calculate DOF are proposed with a feasibility analysis. Simulations of swarm robotics in different kinds of two-dimensional virtual tubes are performed to demonstrate the effectiveness of the proposed method of calculating DOF.</details> | http://arxiv.org/abs/2410.22031v2 |
| Neurofeedback-Driven 6-DOF Robotic Arm: Integration of Brain-Computer   Interface with Arduino for Advanced Control | Ihab A. Satam; Róbert Szabolcsi | 2024-10-29 | <details><summary>Click to expand</summary>Brain computer interface (BCI) applications in robotics are becoming more famous and famous. People with disabilities are facing a real-time problem of doing simple activities such as grasping, handshaking etc. in order to aid with this problem, the use of brain signals to control actuators is showing a great importance. The Emotive Insight, a Brain-Computer Interface (BCI) device, is utilized in this project to collect brain signals and transform them into commands for controlling a robotic arm using an Arduino controller. The Emotive Insight captures brain signals, which are subsequently analyzed using Emotive software and connected with Arduino code. The HITI Brain software integrates these devices, allowing for smooth communication between brain activity and the robotic arm. This system demonstrates how brain impulses may be utilized to control external devices directly. The results showed that the system is applicable efficiently to robotic arms and also for prosthetic arms with Multi Degree of Freedom. In addition to that, the system can be used for other actuators such as bikes, mobile robots, wheelchairs etc.</details> | http://arxiv.org/abs/2410.22008v1 |
| Component Modularized Design of Musculoskeletal Humanoid Platform   Musashi to Investigate Learning Control Systems | Kento Kawaharazuka; Shogo Makino; Kei Tsuzuki; Moritaka Onitsuka; Yuya Nagamatsu; Koki Shinjo; Tasuku Makabe; Yuki Asano; Kei Okada; Koji Kawasaki; Masayuki Inaba | 2024-10-29 | <details><summary>Click to expand</summary>To develop Musashi as a musculoskeletal humanoid platform to investigate learning control systems, we aimed for a body with flexible musculoskeletal structure, redundant sensors, and easily reconfigurable structure. For this purpose, we develop joint modules that can directly measure joint angles, muscle modules that can realize various muscle routes, and nonlinear elastic units with soft structures, etc. Next, we develop MusashiLarm, a musculoskeletal platform composed of only joint modules, muscle modules, generic bone frames, muscle wire units, and a few attachments. Finally, we develop Musashi, a musculoskeletal humanoid platform which extends MusashiLarm to the whole body design, and conduct several basic experiments and learning control experiments to verify the effectiveness of its concept.</details> | http://arxiv.org/abs/2410.22000v1 |
| ActiveSplat: High-Fidelity Scene Reconstruction through Active Gaussian   Splatting | Yuetao Li; Zijia Kuang; Ting Li; Guyue Zhou; Shaohui Zhang; Zike Yan | 2024-10-29 | <details><summary>Click to expand</summary>We propose ActiveSplat, an autonomous high-fidelity reconstruction system leveraging Gaussian splatting. Taking advantage of efficient and realistic rendering, the system establishes a unified framework for online mapping, viewpoint selection, and path planning. The key to ActiveSplat is a hybrid map representation that integrates both dense information about the environment and a sparse abstraction of the workspace. Therefore, the system leverages sparse topology for efficient viewpoint sampling and path planning, while exploiting view-dependent dense prediction for viewpoint selection, facilitating efficient decision-making with promising accuracy and completeness. A hierarchical planning strategy based on the topological map is adopted to mitigate repetitive trajectories and improve local granularity given limited budgets, ensuring high-fidelity reconstruction with photorealistic view synthesis. Extensive experiments and ablation studies validate the efficacy of the proposed method in terms of reconstruction accuracy, data coverage, and exploration efficiency. Project page: https://li-yuetao.github.io/ActiveSplat/.</details> | http://arxiv.org/abs/2410.21955v1 |
| Reliable Semantic Understanding for Real World Zero-shot Object Goal   Navigation | Halil Utku Unlu; Shuaihang Yuan; Congcong Wen; Hao Huang; Anthony Tzes; Yi Fang | 2024-10-29 | <details><summary>Click to expand</summary>We introduce an innovative approach to advancing semantic understanding in zero-shot object goal navigation (ZS-OGN), enhancing the autonomy of robots in unfamiliar environments. Traditional reliance on labeled data has been a limitation for robotic adaptability, which we address by employing a dual-component framework that integrates a GLIP Vision Language Model for initial detection and an InstructionBLIP model for validation. This combination not only refines object and environmental recognition but also fortifies the semantic interpretation, pivotal for navigational decision-making. Our method, rigorously tested in both simulated and real-world settings, exhibits marked improvements in navigation precision and reliability.</details> | http://arxiv.org/abs/2410.21926v1 |
| Precise and Dexterous Robotic Manipulation via Human-in-the-Loop   Reinforcement Learning | Jianlan Luo; Charles Xu; Jeffrey Wu; Sergey Levine | 2024-10-29 | <details><summary>Click to expand</summary>Reinforcement learning (RL) holds great promise for enabling autonomous acquisition of complex robotic manipulation skills, but realizing this potential in real-world settings has been challenging. We present a human-in-the-loop vision-based RL system that demonstrates impressive performance on a diverse set of dexterous manipulation tasks, including dynamic manipulation, precision assembly, and dual-arm coordination. Our approach integrates demonstrations and human corrections, efficient RL algorithms, and other system-level design choices to learn policies that achieve near-perfect success rates and fast cycle times within just 1 to 2.5 hours of training. We show that our method significantly outperforms imitation learning baselines and prior RL approaches, with an average 2x improvement in success rate and 1.8x faster execution. Through extensive experiments and analysis, we provide insights into the effectiveness of our approach, demonstrating how it learns robust, adaptive policies for both reactive and predictive control strategies. Our results suggest that RL can indeed learn a wide range of complex vision-based manipulation policies directly in the real world within practical training times. We hope this work will inspire a new generation of learned robotic manipulation techniques, benefiting both industrial applications and research advancements. Videos and code are available at our project website https://hil-serl.github.io/.</details> | http://arxiv.org/abs/2410.21845v1 |
| First-in-human spinal cord tumor imaging with fast adaptive focus   tracking robotic-OCT | Bin He; Yuzhe Ying; Yejiong Shi; Zhe Meng; Zichen Yin; Zhengyu Chen; Zhangwei Hu; Ruizhi Xue; Linkai Jing; Yang Lu; Zhenxing Sun; Weitao Man; Youtu Wu; Dan Lei; Ning Zhang; Guihuai Wang; Ping Xue | 2024-10-29 | <details><summary>Click to expand</summary>Current surgical procedures for spinal cord tumors lack in vivo high-resolution, high-speed multifunctional imaging systems, posing challenges for precise tumor resection and intraoperative decision-making. This study introduces the Fast Adaptive Focus Tracking Robotic Optical Coherence Tomography (FACT-ROCT) system,designed to overcome these obstacles by providing real-time, artifact-free multifunctional imaging of spinal cord tumors during surgery. By integrating cross-scanning, adaptive focus tracking and robotics, the system addresses motion artifacts and resolution degradation from tissue movement, achieving wide-area, high-resolution imaging. We conducted intraoperative imaging on 21 patients, including 13 with spinal gliomas and 8 with other tumors. This study marks the first demonstration of OCT in situ imaging of human spinal cord tumors, providing micrometer-scale in vivo structural images and demonstrating FACT-ROCT's potential to differentiate various tumor types in real-time. Analysis of the attenuation coefficients of spinal gliomas revealed increased heterogeneity with higher malignancy grades. So, we proposed the standard deviation of the attenuation coefficient as a physical marker, achieving over 90% accuracy in distinguishing high- from low-grade gliomas intraoperatively at a threshold. FACT-ROCT even enabled extensive in vivo microvascular imaging of spinal cord tumors, covering 70 mm * 13 mm * 10 mm within 2 minutes. Quantitative vascular tortuosity comparisons confirmed greater tortuosity in higher-grade tumors. The ability to perform extensive vascular imaging and real-time tumor grading during surgery provides critical information for surgical strategy, such as minimizing intraoperative bleeding and optimizing tumor resection while preserving functional tissue.</details> | http://arxiv.org/abs/2410.21809v2 |
| Robot Policy Learning with Temporal Optimal Transport Reward | Yuwei Fu; Haichao Zhang; Di Wu; Wei Xu; Benoit Boulet | 2024-10-29 | <details><summary>Click to expand</summary>Reward specification is one of the most tricky problems in Reinforcement Learning, which usually requires tedious hand engineering in practice. One promising approach to tackle this challenge is to adopt existing expert video demonstrations for policy learning. Some recent work investigates how to learn robot policies from only a single/few expert video demonstrations. For example, reward labeling via Optimal Transport (OT) has been shown to be an effective strategy to generate a proxy reward by measuring the alignment between the robot trajectory and the expert demonstrations. However, previous work mostly overlooks that the OT reward is invariant to temporal order information, which could bring extra noise to the reward signal. To address this issue, in this paper, we introduce the Temporal Optimal Transport (TemporalOT) reward to incorporate temporal order information for learning a more accurate OT-based proxy reward. Extensive experiments on the Meta-world benchmark tasks validate the efficacy of the proposed method. Code is available at: https://github.com/fuyw/TemporalOT</details> | http://arxiv.org/abs/2410.21795v2 |
| DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial   Information for Dynamics Model Learning | Zhen Zhang; Xiangyu Chu; Yunxi Tang; K. W. Samuel Au | 2024-10-29 | <details><summary>Click to expand</summary>This work proposes DOFS, a pilot dataset of 3D deformable objects (DOs) (e.g., elasto-plastic objects) with full spatial information (i.e., top, side, and bottom information) using a novel and low-cost data collection platform with a transparent operating plane. The dataset consists of active manipulation action, multi-view RGB-D images, well-registered point clouds, 3D deformed mesh, and 3D occupancy with semantics, using a pinching strategy with a two-parallel-finger gripper. In addition, we trained a neural network with the down-sampled 3D occupancy and action as input to model the dynamics of an elasto-plastic object. Our dataset and all CADs of the data collection system will be released soon on our website.</details> | http://arxiv.org/abs/2410.21758v1 |
| EI-Nexus: Towards Unmediated and Flexible Inter-Modality Local Feature   Extraction and Matching for Event-Image Data | Zhonghua Yi; Hao Shi; Qi Jiang; Kailun Yang; Ze Wang; Diyang Gu; Yufan Zhang; Kaiwei Wang | 2024-10-29 | <details><summary>Click to expand</summary>Event cameras, with high temporal resolution and high dynamic range, have limited research on the inter-modality local feature extraction and matching of event-image data. We propose EI-Nexus, an unmediated and flexible framework that integrates two modality-specific keypoint extractors and a feature matcher. To achieve keypoint extraction across viewpoint and modality changes, we bring Local Feature Distillation (LFD), which transfers the viewpoint consistency from a well-learned image extractor to the event extractor, ensuring robust feature correspondence. Furthermore, with the help of Context Aggregation (CA), a remarkable enhancement is observed in feature matching. We further establish the first two inter-modality feature matching benchmarks, MVSEC-RPE and EC-RPE, to assess relative pose estimation on event-image data. Our approach outperforms traditional methods that rely on explicit modal transformation, offering more unmediated and adaptable feature extraction and matching, achieving better keypoint similarity and state-of-the-art results on the MVSEC-RPE and EC-RPE benchmarks. The source code and benchmarks will be made publicly available at https://github.com/ZhonghuaYi/EI-Nexus_official.</details> | http://arxiv.org/abs/2410.21743v1 |
| Enhancing Safety and Robustness of Vision-Based Controllers via   Reachability Analysis | Kaustav Chakraborty; Aryaman Gupta; Somil Bansal | 2024-10-29 | <details><summary>Click to expand</summary>Autonomous systems, such as self-driving cars and drones, have made significant strides in recent years by leveraging visual inputs and machine learning for decision-making and control. Despite their impressive performance, these vision-based controllers can make erroneous predictions when faced with novel or out-of-distribution inputs. Such errors can cascade into catastrophic system failures and compromise system safety. In this work, we compute Neural Reachable Tubes, which act as parameterized approximations of Backward Reachable Tubes to stress-test the vision-based controllers and mine their failure modes. The identified failures are then used to enhance the system safety through both offline and online methods. The online approach involves training a classifier as a run-time failure monitor to detect closed-loop, system-level failures, subsequently triggering a fallback controller that robustly handles these detected failures to preserve system safety. For the offline approach, we improve the original controller via incremental training using a carefully augmented failure dataset, resulting in a more robust controller that is resistant to the known failure modes. In either approach, the system is safeguarded against shortcomings that transcend the vision-based controller and pertain to the closed-loop safety of the overall system. We validate the proposed approaches on an autonomous aircraft taxiing task that involves using a vision-based controller to guide the aircraft towards the centerline of the runway. Our results show the efficacy of the proposed algorithms in identifying and handling system-level failures, outperforming methods that rely on controller prediction error or uncertainty quantification for identifying system failures.</details> | http://arxiv.org/abs/2410.21736v2 |
| A Time and Place to Land: Online Learning-Based Distributed MPC for   Multirotor Landing on Surface Vessel in Waves | Jess Stephenson; William S. Stewart; Melissa Greeff | 2024-10-29 | <details><summary>Click to expand</summary>Landing a multirotor unmanned aerial vehicle (UAV) on an uncrewed surface vessel (USV) extends the operational range and offers recharging capabilities for maritime and limnology applications, such as search-and-rescue and environmental monitoring. However, autonomous UAV landings on USVs are challenging due to the unpredictable tilt and motion of the vessel caused by waves. This movement introduces spatial and temporal uncertainties, complicating safe, precise landings. Existing autonomous landing techniques on unmanned ground vehicles (UGVs) rely on shared state information, often causing time delays due to communication limits. This paper introduces a learning-based distributed Model Predictive Control (MPC) framework for autonomous UAV landings on USVs in wave-like conditions. Each vehicle's MPC optimizes for an artificial goal and input, sharing only the goal with the other vehicle. These goals are penalized by coupling and platform tilt costs, learned as a Gaussian Process (GP). We validate our framework in comprehensive indoor experiments using a custom-designed platform attached to a UGV to simulate USV tilting motion. Our approach achieves a 53% increase in landing success compared to an approach that neglects the impact of tilt motion on landing.</details> | http://arxiv.org/abs/2410.21674v1 |
| Constrained Nonlinear Kaczmarz Projection on Intersections of Manifolds   for Coordinated Multi-Robot Mobile Manipulation | Akshaya Agrawal; Parker Mayer; Zachary Kingston; Geoffrey A. Hollinger | 2024-10-29 | <details><summary>Click to expand</summary>Cooperative manipulation tasks impose various structure-, task-, and robot-specific constraints on mobile manipulators. However, current methods struggle to model and solve these myriad constraints simultaneously. We propose a twofold solution: first, we model constraints as a family of manifolds amenable to simultaneous solving. Second, we introduce the constrained nonlinear Kaczmarz (cNKZ) projection technique to produce constraint-satisfying solutions. Experiments show that cNKZ dramatically outperforms baseline approaches, which cannot find solutions at all. We integrate cNKZ with a sampling-based motion planning algorithm to generate complex, coordinated motions for 3 to 6 mobile manipulators (18--36 DoF), with cNKZ solving up to 80 nonlinear constraints simultaneously and achieving up to a 92% success rate in cluttered environments. We also demonstrate our approach on hardware using three Turtlebot3 Waffle Pi robots with OpenMANIPULATOR-X arms.</details> | http://arxiv.org/abs/2410.21630v1 |
| Identifying Selections for Unsupervised Subtask Discovery | Yiwen Qiu; Yujia Zheng; Kun Zhang | 2024-10-28 | <details><summary>Click to expand</summary>When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a $\textit{selection}$ mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at https://anonymous.4open.science/r/Identifying\_Selections\_for\_Unsupervised\_Subtask\_Discovery/README.md.</details> | http://arxiv.org/abs/2410.21616v1 |
| NYC-Event-VPR: A Large-Scale High-Resolution Event-Based Visual Place   Recognition Dataset in Dense Urban Environments | Taiyi Pan; Junyang He; Chao Chen; Yiming Li; Chen Feng | 2024-10-28 | <details><summary>Click to expand</summary>Visual place recognition (VPR) enables autonomous robots to identify previously visited locations, which contributes to tasks like simultaneous localization and mapping (SLAM). VPR faces challenges such as accurate image neighbor retrieval and appearance change in scenery. Event cameras, also known as dynamic vision sensors, are a new sensor modality for VPR and offer a promising solution to the challenges with their unique attributes: high temporal resolution (1MHz clock), ultra-low latency (in {\mu}s), and high dynamic range (>120dB). These attributes make event cameras less susceptible to motion blur and more robust in variable lighting conditions, making them suitable for addressing VPR challenges. However, the scarcity of event-based VPR datasets, partly due to the novelty and cost of event cameras, hampers their adoption. To fill this data gap, our paper introduces the NYC-Event-VPR dataset to the robotics and computer vision communities, featuring the Prophesee IMX636 HD event sensor (1280x720 resolution), combined with RGB camera and GPS module. It encompasses over 13 hours of geotagged event data, spanning 260 kilometers across New York City, covering diverse lighting and weather conditions, day/night scenarios, and multiple visits to various locations. Furthermore, our paper employs three frameworks to conduct generalization performance assessments, promoting innovation in event-based VPR and its integration into robotics applications.</details> | http://arxiv.org/abs/2410.21615v1 |
| Adaptive Self-Calibration for Minimalistic Collective Perception by   Imperfect Robot Swarms | Khai Yi Chin; Carlo Pinciroli | 2024-10-28 | <details><summary>Click to expand</summary>Collective perception is a fundamental problem in swarm robotics, often cast as best-of-$n$ decision-making. Past studies involve robots with perfect sensing or with small numbers of faulty robots. We previously addressed these limitations by proposing an algorithm, here referred to as Minimalistic Collective Perception (MCP) [arxiv:2209.12858], to reach correct decisions despite the entire swarm having severely damaged sensors. However, this algorithm assumes that sensor accuracy is known, which may be infeasible in reality. In this paper, we eliminate this assumption to (i) investigate the decline of estimation performance and (ii) introduce an Adaptive Sensor Degradation Filter (ASDF) to mitigate the decline. We combine the MCP algorithm and a hypothesis test to enable adaptive self-calibration of robots' assumed sensor accuracy. We validate our approach across several parameters of interest. Our findings show that estimation performance by a swarm with correctly known accuracy is superior to that by a swarm unaware of its accuracy. However, the ASDF drastically mitigates the damage, even reaching the performance levels of robots aware a priori of their correct accuracy.</details> | http://arxiv.org/abs/2410.21546v1 |
| Denoising Diffusion Planner: Learning Complex Paths from Low-Quality   Demonstrations | Michiel Nikken; Nicolò Botteghi; Weasley Roozing; Federico Califano | 2024-10-28 | <details><summary>Click to expand</summary>Denoising Diffusion Probabilistic Models (DDPMs) are powerful generative deep learning models that have been very successful at image generation, and, very recently, in path planning and control. In this paper, we investigate how to leverage the generalization and conditional-sampling capabilities of DDPMs to generate complex paths for a robotic end effector. We show that training a DDPM with synthetical and low-quality demonstrations is sufficient for generating nontrivial paths reaching arbitrary targets and avoiding obstacles. Additionally, we investigate different strategies for conditional sampling combining classifier-free and classifier-guided approaches. Eventually, we deploy the DDPM in a receding-horizon control scheme to enhance its planning capabilities. The Denoising Diffusion Planner is experimentally validated through various experiments on a Franka Emika Panda robot.</details> | http://arxiv.org/abs/2410.21497v1 |
| TALE-teller: Tendon-Actuated Linked Element Robotic Testbed for   Investigating Tail Functions | Margaret J. Zhang; Anvay A. Pradhan; Zachary Brei; Xiangyun Bu; Xiang Ye; Saima Jamal; Chae Woo Lim; Xiaonan Huang; Talia Y. Moore | 2024-10-28 | <details><summary>Click to expand</summary>Tails serve various functions in both robotics and biology, including expression, grasping, and defense. The vertebrate tails associated with these functions exhibit diverse patterns of vertebral lengths, but the precise mechanisms linking form to function have not yet been established. Vertebrate tails are complex musculoskeletal structures, making both direct experimentation and computational modeling challenging. This paper presents Tendon-Actuated Linked-Element (TALE), a modular robotic test bed to explore how tail morphology influences function. By varying 3D printed bones, silicone joints, and tendon configurations, TALE can match the morphology of extant, extinct, and even theoretical tails. We first characterized the stiffness of our joint design empirically and in simulation before testing the hypothesis that tails with different vertebral proportions curve differently. We then compared the maximum bending state of two common vertebrate proportions and one theoretical morphology. Uniform bending of joints with different vertebral proportions led to substantial differences in the location of the tail tip, suggesting a significant influence on overall tail function. Future studies can introduce more complex morphologies to establish the mechanisms of diverse tail functions. With this foundational knowledge, we will isolate the key features underlying tail function to inform the design for robotic tails. Images and videos can be found on TALE's project page: https://www.embirlab.com/tale.</details> | http://arxiv.org/abs/2410.21445v1 |
| Learning State Conditioned Linear Mappings for Low-Dimensional Control   of Robotic Manipulators | Michael Przystupa; Kerrick Johnstonbaugh; Zichen Zhang; Laura Petrich; Masood Dehghan; Faezeh Haghverd; Martin Jagersand | 2024-10-28 | <details><summary>Click to expand</summary>Identifying an appropriate task space that simplifies control solutions is important for solving robotic manipulation problems. One approach to this problem is learning an appropriate low-dimensional action space. Linear and nonlinear action mapping methods have trade-offs between simplicity on the one hand and the ability to express motor commands outside of a single low-dimensional subspace on the other. We propose that learning local linear action representations that adapt based on the current configuration of the robot achieves both of these benefits. Our state-conditioned linear maps ensure that for any given state, the high-dimensional robotic actuations are linear in the low-dimensional action. As the robot state evolves, so do the action mappings, ensuring the ability to represent motions that are immediately necessary. These local linear representations guarantee desirable theoretical properties by design, and we validate these findings empirically through two user studies. Results suggest state-conditioned linear maps outperform conditional autoencoder and PCA baselines on a pick-and-place task and perform comparably to mode switching in a more complex pouring task.</details> | http://arxiv.org/abs/2410.21441v1 |
| Large Language Models for Manufacturing | Yiwei Li; Huaqin Zhao; Hanqi Jiang; Yi Pan; Zhengliang Liu; Zihao Wu; Peng Shu; Jie Tian; Tianze Yang; Shaochen Xu; Yanjun Lyu; Parker Blenk; Jacob Pence; Jason Rupram; Eliza Banu; Ninghao Liu; Linbing Wang; Wenzhan Song; Xiaoming Zhai; Kenan Song; Dajiang Zhu; Beiwen Li; Xianqiao Wang; Tianming Liu | 2024-10-28 | <details><summary>Click to expand</summary>The rapid advances in Large Language Models (LLMs) have the potential to transform manufacturing industry, offering new opportunities to optimize processes, improve efficiency, and drive innovation. This paper provides a comprehensive exploration of the integration of LLMs into the manufacturing domain, focusing on their potential to automate and enhance various aspects of manufacturing, from product design and development to quality control, supply chain optimization, and talent management. Through extensive evaluations across multiple manufacturing tasks, we demonstrate the remarkable capabilities of state-of-the-art LLMs, such as GPT-4V, in understanding and executing complex instructions, extracting valuable insights from vast amounts of data, and facilitating knowledge sharing. We also delve into the transformative potential of LLMs in reshaping manufacturing education, automating coding processes, enhancing robot control systems, and enabling the creation of immersive, data-rich virtual environments through the industrial metaverse. By highlighting the practical applications and emerging use cases of LLMs in manufacturing, this paper aims to provide a valuable resource for professionals, researchers, and decision-makers seeking to harness the power of these technologies to address real-world challenges, drive operational excellence, and unlock sustainable growth in an increasingly competitive landscape.</details> | http://arxiv.org/abs/2410.21418v1 |
| Deploying Ten Thousand Robots: Scalable Imitation Learning for Lifelong   Multi-Agent Path Finding | He Jiang; Yutong Wang; Rishi Veerapaneni; Tanishq Duhan; Guillaume Sartoretti; Jiaoyang Li | 2024-10-28 | <details><summary>Click to expand</summary>Lifelong Multi-Agent Path Finding (LMAPF) is a variant of MAPF where agents are continually assigned new goals, necessitating frequent re-planning to accommodate these dynamic changes. Recently, this field has embraced learning-based methods, which reactively generate single-step actions based on individual local observations. However, it is still challenging for them to match the performance of the best search-based algorithms, especially in large-scale settings. This work proposes an imitation-learning-based LMAPF solver that introduces a novel communication module and systematic single-step collision resolution and global guidance techniques. Our proposed solver, Scalable Imitation Learning for LMAPF (SILLM), inherits the fast reasoning speed of learning-based methods and the high solution quality of search-based methods with the help of modern GPUs. Across six large-scale maps with up to 10,000 agents and varying obstacle structures, SILLM surpasses the best learning- and search-based baselines, achieving average throughput improvements of 137.7% and 16.0%, respectively. Furthermore, SILLM also beats the winning solution of the 2023 League of Robot Runners, an international LMAPF competition sponsored by Amazon Robotics. Finally, we validated SILLM with 10 real robots and 100 virtual robots in a mockup warehouse environment.</details> | http://arxiv.org/abs/2410.21415v1 |
| Exploring reinforcement learning for incident response in autonomous   military vehicles | Henrik Madsen; Gudmund Grov; Federico Mancini; Magnus Baksaas; Åvald Åslaugson Sommervoll | 2024-10-28 | <details><summary>Click to expand</summary>Unmanned vehicles able to conduct advanced operations without human intervention are being developed at a fast pace for many purposes. Not surprisingly, they are also expected to significantly change how military operations can be conducted. To leverage the potential of this new technology in a physically and logically contested environment, security risks are to be assessed and managed accordingly. Research on this topic points to autonomous cyber defence as one of the capabilities that may be needed to accelerate the adoption of these vehicles for military purposes. Here, we pursue this line of investigation by exploring reinforcement learning to train an agent that can autonomously respond to cyber attacks on unmanned vehicles in the context of a military operation. We first developed a simple simulation environment to quickly prototype and test some proof-of-concept agents for an initial evaluation. This agent was then applied to a more realistic simulation environment and finally deployed on an actual unmanned ground vehicle for even more realism. A key contribution of our work is demonstrating that reinforcement learning is a viable approach to train an agent that can be used for autonomous cyber defence on a real unmanned ground vehicle, even when trained in a simple simulation environment.</details> | http://arxiv.org/abs/2410.21407v1 |
| Investigating the Benefits of Nonlinear Action Maps in Data-Driven   Teleoperation | Michael Przystupa; Gauthier Gidel; Matthew E. Taylor; Martin Jagersand; Justus Piater; Samuele Tosatto | 2024-10-28 | <details><summary>Click to expand</summary>As robots become more common for both able-bodied individuals and those living with a disability, it is increasingly important that lay people be able to drive multi-degree-of-freedom platforms with low-dimensional controllers. One approach is to use state-conditioned action mapping methods to learn mappings between low-dimensional controllers and high DOF manipulators -- prior research suggests these mappings can simplify the teleoperation experience for users. Recent works suggest that neural networks predicting a local linear function are superior to the typical end-to-end multi-layer perceptrons because they allow users to more easily undo actions, providing more control over the system. However, local linear models assume actions exist on a linear subspace and may not capture nuanced actions in training data. We observe that the benefit of these mappings is being an odd function concerning user actions, and propose end-to-end nonlinear action maps which achieve this property. Unfortunately, our experiments show that such modifications offer minimal advantages over previous solutions. We find that nonlinear odd functions behave linearly for most of the control space, suggesting architecture structure improvements are not the primary factor in data-driven teleoperation. Our results suggest other avenues, such as data augmentation techniques and analysis of human behavior, are necessary for action maps to become practical in real-world applications, such as in assistive robotics to improve the quality of life of people living with w disability.</details> | http://arxiv.org/abs/2410.21406v1 |
| One-Step Diffusion Policy: Fast Visuomotor Policies via Diffusion   Distillation | Zhendong Wang; Zhaoshuo Li; Ajay Mandlekar; Zhenjia Xu; Jiaojiao Fan; Yashraj Narang; Linxi Fan; Yuke Zhu; Yogesh Balaji; Mingyuan Zhou; Ming-Yu Liu; Yu Zeng | 2024-10-28 | <details><summary>Click to expand</summary>Diffusion models, praised for their success in generative tasks, are increasingly being applied to robotics, demonstrating exceptional performance in behavior cloning. However, their slow generation process stemming from iterative denoising steps poses a challenge for real-time applications in resource-constrained robotics setups and dynamically changing environments. In this paper, we introduce the One-Step Diffusion Policy (OneDP), a novel approach that distills knowledge from pre-trained diffusion policies into a single-step action generator, significantly accelerating response times for robotic control tasks. We ensure the distilled generator closely aligns with the original policy distribution by minimizing the Kullback-Leibler (KL) divergence along the diffusion chain, requiring only $2\%$-$10\%$ additional pre-training cost for convergence. We evaluated OneDP on 6 challenging simulation tasks as well as 4 self-designed real-world tasks using the Franka robot. The results demonstrate that OneDP not only achieves state-of-the-art success rates but also delivers an order-of-magnitude improvement in inference speed, boosting action prediction frequency from 1.5 Hz to 62 Hz, establishing its potential for dynamic and computationally constrained robotic applications. We share the project page at https://research.nvidia.com/labs/dir/onedp/.</details> | http://arxiv.org/abs/2410.21257v1 |
| Capacity-Aware Planning and Scheduling in Budget-Constrained Monotonic   MDPs: A Meta-RL Approach | Manav Vora; Ilan Shomorony; Melkior Ornik | 2024-10-28 | <details><summary>Click to expand</summary>Many real-world sequential repair problems can be effectively modeled using monotonic Markov Decision Processes (MDPs), where the system state stochastically decreases and can only be increased by performing a restorative action. This work addresses the problem of solving multi-component monotonic MDPs with both budget and capacity constraints. The budget constraint limits the total number of restorative actions and the capacity constraint limits the number of restorative actions that can be performed simultaneously. While prior methods dealt with budget constraints, including capacity constraints in prior methods leads to an exponential increase in computational complexity as the number of components in the MDP grows. We propose a two-step planning approach to address this challenge. First, we partition the components of the multi-component MDP into groups, where the number of groups is determined by the capacity constraint. We achieve this partitioning by solving a Linear Sum Assignment Problem (LSAP). Each group is then allocated a fraction of the total budget proportional to its size. This partitioning effectively decouples the large multi-component MDP into smaller subproblems, which are computationally feasible because the capacity constraint is simplified and the budget constraint can be addressed using existing methods. Subsequently, we use a meta-trained PPO agent to obtain an approximately optimal policy for each group. To validate our approach, we apply it to the problem of scheduling repairs for a large group of industrial robots, constrained by a limited number of repair technicians and a total repair budget. Our results demonstrate that the proposed method outperforms baseline approaches in terms of maximizing the average uptime of the robot swarm, particularly for large swarm sizes.</details> | http://arxiv.org/abs/2410.21249v1 |
| HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots | Tairan He; Wenli Xiao; Toru Lin; Zhengyi Luo; Zhenjia Xu; Zhenyu Jiang; Jan Kautz; Changliu Liu; Guanya Shi; Xiaolong Wang; Linxi Fan; Yuke Zhu | 2024-10-28 | <details><summary>Click to expand</summary>Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.</details> | http://arxiv.org/abs/2410.21229v1 |
| User-Centered Design of Socially Assistive Robotic Combined with   Non-Immersive Virtual Reality-based Dyadic Activities for Older Adults   Residing in Long Term Care Facilities | Ritam Ghosh; Nibraas Khan; Miroslava Migovich; Judith A. Tate; Cathy Maxwell; Emily Latshaw; Paul Newhouse; Douglas W. Scharre; Alai Tan; Kelley Colopietro; Lorraine C. Mion; Nilanjan Sarkar | 2024-10-28 | <details><summary>Click to expand</summary>Apathy impairs the quality of life for older adults and their care providers. While few pharmacological remedies exist, current non-pharmacologic approaches are resource intensive. To address these concerns, this study utilizes a user-centered design (UCD) process to develop and test a set of dyadic activities that provide physical, cognitive, and social stimuli to older adults residing in long-term care (LTC) communities. Within the design, a novel framework that combines socially assistive robots and non-immersive virtual reality (SAR-VR) emphasizing human-robot interaction (HRI) and human-computer interaction (HCI) is utilized with the roles of the robots being coach and entertainer. An interdisciplinary team of engineers, nurses, and physicians collaborated with an advisory panel comprising LTC activity coordinators, staff, and residents to prototype the activities. The study resulted in four virtual activities: three with the humanoid robot, Nao, and one with the animal robot, Aibo. Fourteen participants tested the acceptability of the different components of the system and provided feedback at different stages of development. Participant approval increased significantly over successive iterations of the system highlighting the importance of stakeholder feedback. Five LTC staff members successfully set up the system with minimal help from the researchers, demonstrating the usability of the system for caregivers. Rationale for activity selection, design changes, and both quantitative and qualitative results on the acceptability and usability of the system have been presented. The paper discusses the challenges encountered in developing activities for older adults in LTCs and underscores the necessity of the UCD process to address them.</details> | http://arxiv.org/abs/2410.21197v1 |
| Efficiency Optimization of a Two-link Planar Robotic Arm | Meysam Fathizadeh; Hanz Richter | 2024-10-28 | <details><summary>Click to expand</summary>Energy consumption optimization of a two-link planar robotic arm is considered with the system's efficiency being the target for optimization. A new formulation of thermodynamic principles within the framework of dynamical systems is used. This approach is applied by considering cyclic motions for the robotic arm and analyzing the cyclic averaged energies while the robotic arm is tasked with going from point A to point B in the task space while resisting an external force. The energy transfer rate between the links is classified into positive and negative and the results combined with the averaged energy quantities, are used to address the optimization problem while adhering to the constraints imposed by the second law of thermodynamics in its new formulation.</details> | http://arxiv.org/abs/2410.21185v1 |
| Synthetica: Large Scale Synthetic Data for Robot Perception | Ritvik Singh; Jingzhou Liu; Karl Van Wyk; Yu-Wei Chao; Jean-Francois Lafleche; Florian Shkurti; Nathan Ratliff; Ankur Handa | 2024-10-28 | <details><summary>Click to expand</summary>Vision-based object detectors are a crucial basis for robotics applications as they provide valuable information about object localisation in the environment. These need to ensure high reliability in different lighting conditions, occlusions, and visual artifacts, all while running in real-time. Collecting and annotating real-world data for these networks is prohibitively time consuming and costly, especially for custom assets, such as industrial objects, making it untenable for generalization to in-the-wild scenarios. To this end, we present Synthetica, a method for large-scale synthetic data generation for training robust state estimators. This paper focuses on the task of object detection, an important problem which can serve as the front-end for most state estimation problems, such as pose estimation. Leveraging data from a photorealistic ray-tracing renderer, we scale up data generation, generating 2.7 million images, to train highly accurate real-time detection transformers. We present a collection of rendering randomization and training-time data augmentation techniques conducive to robust sim-to-real performance for vision tasks. We demonstrate state-of-the-art performance on the task of object detection while having detectors that run at 50-100Hz which is 9 times faster than the prior SOTA. We further demonstrate the usefulness of our training methodology for robotics applications by showcasing a pipeline for use in the real world with custom objects for which there do not exist prior datasets. Our work highlights the importance of scaling synthetic data generation for robust sim-to-real transfer while achieving the fastest real-time inference speeds. Videos and supplementary information can be found at this URL: https://sites.google.com/view/synthetica-vision.</details> | http://arxiv.org/abs/2410.21153v1 |
| coVoxSLAM: GPU Accelerated Globally Consistent Dense SLAM | Emiliano Höss; Pablo De Cristóforis | 2024-10-28 | <details><summary>Click to expand</summary>A dense SLAM system is essential for mobile robots, as it provides localization and allows navigation, path planning, obstacle avoidance, and decision-making in unstructured environments. Due to increasing computational demands the use of GPUs in dense SLAM is expanding. In this work, we present coVoxSLAM, a novel GPU-accelerated volumetric SLAM system that takes full advantage of the parallel processing power of the GPU to build globally consistent maps even in large-scale environments. It was deployed on different platforms (discrete and embedded GPU) and compared with the state of the art. The results obtained using public datasets show that coVoxSLAM delivers a significant performance improvement considering execution times while maintaining accurate localization. The presented system is available as open-source on GitHub https://github.com/lrse-uba/coVoxSLAM.</details> | http://arxiv.org/abs/2410.21149v1 |
| Magnetic Milli-spinner for Robotic Endovascular Surgery | Shuai Wu; Sophie Leanza; Lu Lu; Yilong Chang; Qi Li; Diego Stone; Ruike Renee Zhao | 2024-10-28 | <details><summary>Click to expand</summary>Vascular diseases such as thrombosis, atherosclerosis, and aneurysm, which can lead to blockage of blood flow or blood vessel rupture, are common and life-threatening. Conventional minimally invasive treatments utilize catheters, or long tubes, to guide small devices or therapeutic agents to targeted regions for intervention. Unfortunately, catheters suffer from difficult and unreliable navigation in narrow, winding vessels such as those found in the brain. Magnetically actuated untethered robots, which have been extensively explored as an alternative, are promising for navigation in complex vasculatures and vascular disease treatments. Most current robots, however, cannot swim against high flows or are inadequate in treating certain conditions. Here, we introduce a multifunctional and magnetically actuated milli-spinner robot for rapid navigation and performance of various treatments in complicated vasculatures. The milli-spinner, with a unique hollow structure including helical fins and slits for propulsion, generates a distinct flow field upon spinning. The milli-spinner is the fastest-ever untethered magnetic robot for movement in tubular environments, easily achieving speeds of 23 cm/s, demonstrating promise as an untethered medical device for effective navigation in blood vessels and robotic treatment of numerous vascular diseases.</details> | http://arxiv.org/abs/2410.21112v1 |
| Multiple power laws and scaling relation in exploratory locomotion of   the snail Tegula nigerrima | Katsushi Kagaya; Tomoyuki Nakano; Ryo Nakayama | 2024-10-28 | <details><summary>Click to expand</summary>One of goals in soft robotics is to achive spontaneous behavior like real organisms. To gain a clue to achieve this, we examined the long (16-hour) spontaneous exploratory locomotion of snails. The active forager snail, Tegula nigerrima, from an intertidal rocky shore was selected to test the general hypothesis that nervous systems are inherently near a critical state, which is self-organized to drive spontaneous animal behavior. This hypothesis, known as the critical brain hypothesis, was originally proposed for vertebrate species, but it might be applicable to other invertebrate species as well. We first investigated the power spectra of the speed of locomotion of the snails ($N=39$). The spectra showed $1/{f^\alpha}$ fluctuation, which is one of the signatures of self-organized criticality. The $\alpha$ was estimated to be about 0.9. We further examined whether the spatial and temporal quantities show multiple power-laws and scaling relations, which are rigorous criteria of criticality. Although the satisfaction of these criteria is limited to a truncated region and provides limited evidence to demonstrate the aspect of self-organization, the multiple power-laws and the scaling relations were overall satisfied. Therefore, these results additionally support the generality of the critical brain hypothesis.</details> | http://arxiv.org/abs/2410.21090v1 |
| Predictive Reachability for Embodiment Selection in Mobile Manipulation   Behaviors | Xiaoxu Feng; Takato Horii; Takayuki Nagai | 2024-10-28 | <details><summary>Click to expand</summary>Mobile manipulators require coordinated control between navigation and manipulation to accomplish tasks. Typically, coordinated mobile manipulation behaviors have base navigation to approach the goal followed by arm manipulation to reach the desired pose. Selecting the embodiment between the base and arm can be determined based on reachability. Previous methods evaluate reachability by computing inverse kinematics and activate arm motions once solutions are identified. In this study, we introduce a new approach called predictive reachability that decides reachability based on predicted arm motions. Our model utilizes a hierarchical policy framework built upon a world model. The world model allows the prediction of future trajectories and the evaluation of reachability. The hierarchical policy selects the embodiment based on the predicted reachability and plans accordingly. Unlike methods that require prior knowledge about robots and environments for inverse kinematics, our method only relies on image-based observations. We evaluate our approach through basic reaching tasks across various environments. The results demonstrate that our method outperforms previous model-based approaches in both sample efficiency and performance, while enabling more reasonable embodiment selection based on predictive reachability.</details> | http://arxiv.org/abs/2410.21059v1 |
| LiP-LLM: Integrating Linear Programming and dependency graph with Large   Language Models for multi-robot task planning | Kazuma Obata; Tatsuya Aoki; Takato Horii; Tadahiro Taniguchi; Takayuki Nagai | 2024-10-28 | <details><summary>Click to expand</summary>This study proposes LiP-LLM: integrating linear programming and dependency graph with large language models (LLMs) for multi-robot task planning. In order for multiple robots to perform tasks more efficiently, it is necessary to manage the precedence dependencies between tasks. Although multi-robot decentralized and centralized task planners using LLMs have been proposed, none of these studies focus on precedence dependencies from the perspective of task efficiency or leverage traditional optimization methods. It addresses key challenges in managing dependencies between skills and optimizing task allocation. LiP-LLM consists of three steps: skill list generation and dependency graph generation by LLMs, and task allocation using linear programming. The LLMs are utilized to generate a comprehensive list of skills and to construct a dependency graph that maps the relationships and sequential constraints among these skills. To ensure the feasibility and efficiency of skill execution, the skill list is generated by calculated likelihood, and linear programming is used to optimally allocate tasks to each robot. Experimental evaluations in simulated environments demonstrate that this method outperforms existing task planners, achieving higher success rates and efficiency in executing complex, multi-robot tasks. The results indicate the potential of combining LLMs with optimization techniques to enhance the capabilities of multi-robot systems in executing coordinated tasks accurately and efficiently. In an environment with two robots, a maximum success rate difference of 0.82 is observed in the language instruction group with a change in the object name.</details> | http://arxiv.org/abs/2410.21040v1 |
| Exploring the Reliability of Foundation Model-Based Frontier Selection   in Zero-Shot Object Goal Navigation | Shuaihang Yuan; Halil Utku Unlu; Hao Huang; Congcong Wen; Anthony Tzes; Yi Fang | 2024-10-28 | <details><summary>Click to expand</summary>In this paper, we present a novel method for reliable frontier selection in Zero-Shot Object Goal Navigation (ZS-OGN), enhancing robotic navigation systems with foundation models to improve commonsense reasoning in indoor environments. Our approach introduces a multi-expert decision framework to address the nonsensical or irrelevant reasoning often seen in foundation model-based systems. The method comprises two key components: Diversified Expert Frontier Analysis (DEFA) and Consensus Decision Making (CDM). DEFA utilizes three expert models: furniture arrangement, room type analysis, and visual scene reasoning, while CDM aggregates their outputs, prioritizing unanimous or majority consensus for more reliable decisions. Demonstrating state-of-the-art performance on the RoboTHOR and HM3D datasets, our method excels at navigating towards untrained objects or goals and outperforms various baselines, showcasing its adaptability to dynamic real-world conditions and superior generalization capabilities.</details> | http://arxiv.org/abs/2410.21037v1 |
| On the Black-box Explainability of Object Detection Models for Safe and   Trustworthy Industrial Applications | Alain Andres; Aitor Martinez-Seras; Ibai Laña; Javier Del Ser | 2024-10-28 | <details><summary>Click to expand</summary>In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence (XAI) methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic XAI methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP), which uses segmentation-based mask generation. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in the same scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used.</details> | http://arxiv.org/abs/2411.00818v1 |
| Reference-Free Formula Drift with Reinforcement Learning: From Driving   Data to Tire Energy-Inspired, Real-World Policies | Franck Djeumou; Michael Thompson; Makoto Suminaka; John Subosits | 2024-10-28 | <details><summary>Click to expand</summary>The skill to drift a car--i.e., operate in a state of controlled oversteer like professional drivers--could give future autonomous cars maximum flexibility when they need to retain control in adverse conditions or avoid collisions. We investigate real-time drifting strategies that put the car where needed while bypassing expensive trajectory optimization. To this end, we design a reinforcement learning agent that builds on the concept of tire energy absorption to autonomously drift through changing and complex waypoint configurations while safely staying within track bounds. We achieve zero-shot deployment on the car by training the agent in a simulation environment built on top of a neural stochastic differential equation vehicle model learned from pre-collected driving data. Experiments on a Toyota GR Supra and Lexus LC 500 show that the agent is capable of drifting smoothly through varying waypoint configurations with tracking error as low as 10 cm while stably pushing the vehicles to sideslip angles of up to 63{\deg}.</details> | http://arxiv.org/abs/2410.20990v1 |
| Empowering Autonomous Shuttles with Next-Generation Infrastructure | Sven Ochs; Melih Yazgan; Rupert Polley; Albert Schotschneider; Stefan Orf; Marc Uecker; Maximilian Zipfl; Julian Burger; Abhishek Vivekanandan; Jennifer Amritzer; Marc René Zofka; J. Marius Zöllner | 2024-10-28 | <details><summary>Click to expand</summary>As cities strive to address urban mobility challenges, combining autonomous transportation technologies with intelligent infrastructure presents an opportunity to transform how people move within urban environments. Autonomous shuttles are particularly suited for adaptive and responsive public transport for the first and last mile, connecting with smart infrastructure to enhance urban transit. This paper presents the concept, implementation, and evaluation of a proof-of-concept deployment of an autonomous shuttle integrated with smart infrastructure at a public fair. The infrastructure includes two perception-equipped bus stops and a connected pedestrian intersection, all linked through a central communication and control hub. Our key contributions include the development of a comprehensive system architecture for "smart" bus stops, the integration of multiple urban locations into a cohesive smart transport ecosystem, and the creation of adaptive shuttle behavior for automated driving. Additionally, we publish an open source dataset and a Vehicle-to-X (V2X) driver to support further research. Finally, we offer an outlook on future research directions and potential expansions of the demonstrated technologies and concepts.</details> | http://arxiv.org/abs/2410.20989v1 |
| BEVPose: Unveiling Scene Semantics through Pose-Guided Multi-Modal BEV   Alignment | Mehdi Hosseinzadeh; Ian Reid | 2024-10-28 | <details><summary>Click to expand</summary>In the field of autonomous driving and mobile robotics, there has been a significant shift in the methods used to create Bird's Eye View (BEV) representations. This shift is characterised by using transformers and learning to fuse measurements from disparate vision sensors, mainly lidar and cameras, into a 2D planar ground-based representation. However, these learning-based methods for creating such maps often rely heavily on extensive annotated data, presenting notable challenges, particularly in diverse or non-urban environments where large-scale datasets are scarce. In this work, we present BEVPose, a framework that integrates BEV representations from camera and lidar data, using sensor pose as a guiding supervisory signal. This method notably reduces the dependence on costly annotated data. By leveraging pose information, we align and fuse multi-modal sensory inputs, facilitating the learning of latent BEV embeddings that capture both geometric and semantic aspects of the environment. Our pretraining approach demonstrates promising performance in BEV map segmentation tasks, outperforming fully-supervised state-of-the-art methods, while necessitating only a minimal amount of annotated data. This development not only confronts the challenge of data efficiency in BEV representation learning but also broadens the potential for such techniques in a variety of domains, including off-road and indoor environments.</details> | http://arxiv.org/abs/2410.20969v1 |
| VLMimic: Vision Language Models are Visual Imitation Learner for   Fine-grained Actions | Guanyan Chen; Meiling Wang; Te Cui; Yao Mu; Haoyang Lu; Tianxing Zhou; Zicai Peng; Mengxiao Hu; Haizhou Li; Yuan Li; Yi Yang; Yufeng Yue | 2024-10-28 | <details><summary>Click to expand</summary>Visual imitation learning (VIL) provides an efficient and intuitive strategy for robotic systems to acquire novel skills. Recent advancements in Vision Language Models (VLMs) have demonstrated remarkable performance in vision and language reasoning capabilities for VIL tasks. Despite the progress, current VIL methods naively employ VLMs to learn high-level plans from human videos, relying on pre-defined motion primitives for executing physical interactions, which remains a major bottleneck. In this work, we present VLMimic, a novel paradigm that harnesses VLMs to directly learn even fine-grained action levels, only given a limited number of human videos. Specifically, VLMimic first grounds object-centric movements from human videos, and learns skills using hierarchical constraint representations, facilitating the derivation of skills with fine-grained action levels from limited human videos. These skills are refined and updated through an iterative comparison strategy, enabling efficient adaptation to unseen environments. Our extensive experiments exhibit that our VLMimic, using only 5 human videos, yields significant improvements of over 27% and 21% in RLBench and real-world manipulation tasks, and surpasses baselines by over 37% in long-horizon tasks.</details> | http://arxiv.org/abs/2410.20927v3 |
| Combining Deep Reinforcement Learning with a Jerk-Bounded Trajectory   Generator for Kinematically Constrained Motion Planning | Seyed Adel Alizadeh Kolagar; Mehdi Heydari Shahna; Jouni Mattila | 2024-10-28 | <details><summary>Click to expand</summary>Deep reinforcement learning (DRL) is emerging as a promising method for adaptive robotic motion and complex task automation, effectively addressing the limitations of traditional control methods. However, ensuring safety throughout both the learning process and policy deployment remains a key challenge due to the risky exploration inherent in DRL, as well as the discrete nature of actions taken at intervals. These discontinuities, despite being part of a continuous action space, can lead to abrupt changes between successive actions, causing instability and unsafe intermediate states. To address these challenges, this paper proposes an integrated framework that combines DRL with a jerk-bounded trajectory generator (JBTG) and a robust low-level control strategy, significantly enhancing the safety, stability, and reliability of robotic manipulators. The low-level controller ensures the precise execution of DRL-generated commands, while the JBTG refines these motions to produce smooth, continuous trajectories that prevent abrupt or unsafe actions. The framework also includes pre-calculated safe velocity zones for smooth braking, preventing joint limit violations and ensuring compliance with kinematic constraints. This approach not only guarantees the robustness and safety of the robotic system but also optimizes motion control, making it suitable for practical applications. The effectiveness of the proposed framework is demonstrated through its application to a highly complex heavy-duty manipulator.</details> | http://arxiv.org/abs/2410.20907v1 |
| Active Causal Structure Learning with Latent Variables: Towards Learning   to Detour in Autonomous Robots | Pablo de los Riscos; Fernando Corbacho | 2024-10-28 | <details><summary>Click to expand</summary>Artificial General Intelligence (AGI) Agents and Robots must be able to cope with everchanging environments and tasks. They must be able to actively construct new internal causal models of their interactions with the environment when new structural changes take place in the environment. Thus, we claim that active causal structure learning with latent variables (ACSLWL) is a necessary component to build AGI agents and robots. This paper describes how a complex planning and expectation-based detour behavior can be learned by ACSLWL when, unexpectedly, and for the first time, the simulated robot encounters a sort of transparent barrier in its pathway towards its target. ACSWL consists of acting in the environment, discovering new causal relations, constructing new causal models, exploiting the causal models to maximize its expected utility, detecting possible latent variables when unexpected observations occur, and constructing new structures-internal causal models and optimal estimation of the associated parameters, to be able to cope efficiently with the new encountered situations. That is, the agent must be able to construct new causal internal models that transform a previously unexpected and inefficient (sub-optimal) situation, into a predictable situation with an optimal operating plan.</details> | http://arxiv.org/abs/2410.20894v1 |
| Origami crawlers: exploring a single origami vertex for complex path   navigation | Davood Farhadi; Laura Pernigoni; David Melancon; Katia Bertoldi | 2024-10-28 | <details><summary>Click to expand</summary>The ancient art of origami, traditionally used to transform simple sheets into intricate objects, also holds potential for diverse engineering applications, such as shape morphing and robotics. In this study, we demonstrate that one of the most basic origami structures (i.e., a rigid, foldable degree-four vertex) can be engineered to create a crawler capable of navigating complex paths using only a single input. Through a combination of experimental studies and modeling, we show that modifying the geometry of a degree four vertex enables sheets to move either in a straight line or turn. Furthermore, we illustrate how leveraging the nonlinearities in folding allows the design of crawlers that can switch between moving straight and turning. Remarkably, these crawling modes can be controlled by adjusting the range of the actuation folding angle. Our study opens avenues for simple machines that can follow intricate trajectories with minimal actuation.</details> | http://arxiv.org/abs/2410.20818v1 |
| Adversarial Constrained Policy Optimization: Improving Constrained   Reinforcement Learning by Adapting Budgets | Jianmina Ma; Jingtian Ji; Yue Gao | 2024-10-28 | <details><summary>Click to expand</summary>Constrained reinforcement learning has achieved promising progress in safety-critical fields where both rewards and constraints are considered. However, constrained reinforcement learning methods face challenges in striking the right balance between task performance and constraint satisfaction and it is prone for them to get stuck in over-conservative or constraint violating local minima. In this paper, we propose Adversarial Constrained Policy Optimization (ACPO), which enables simultaneous optimization of reward and the adaptation of cost budgets during training. Our approach divides original constrained problem into two adversarial stages that are solved alternately, and the policy update performance of our algorithm can be theoretically guaranteed. We validate our method through experiments conducted on Safety Gymnasium and quadruped locomotion tasks. Results demonstrate that our algorithm achieves better performances compared to commonly used baselines.</details> | http://arxiv.org/abs/2410.20786v1 |
| Narrow Passage Path Planning using Collision Constraint Interpolation | Minji Lee; Jeongmin Lee; Dongjun Lee | 2024-10-28 | <details><summary>Click to expand</summary>Narrow passage path planning is a prevalent problem from industrial to household sites, often facing difficulties in finding feasible paths or requiring excessive computational resources. Given that deep penetration into the environment can cause optimization failure, we propose a framework to ensure feasibility throughout the process using a series of subproblems tailored for narrow passage problem. We begin by decomposing the environment into convex objects and initializing collision constraints with a subset of these objects. By continuously interpolating the collision constraints through the process of sequentially introducing remaining objects, our proposed framework generates subproblems that guide the optimization toward solving the narrow passage problem. Several examples are presented to demonstrate how the proposed framework addresses narrow passage path planning problems.</details> | http://arxiv.org/abs/2410.20697v1 |
| Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for   Robotic Guidance of People with Visual Impairments | Sangmim Song; Sarath Kodagoda; Amal Gunatilake; Marc G. Carmichael; Karthick Thiyagarajan; Jodi Martin | 2024-10-28 | <details><summary>Click to expand</summary>Navigation presents a significant challenge for persons with visual impairments (PVI). While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations. Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation. In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments. Our approach features a novel text-based topological map that enables the LLM to plan global paths using a simplified environmental representation, focusing on straight paths and right-angle turns to facilitate navigation. Additionally, we utilize the LLM's commonsense reasoning for hazard detection and personalized path planning based on user preferences. Simulated experiments demonstrate the system's efficacy in guiding PVI, underscoring its potential as a significant advancement in assistive technology. The results highlight Guide-LLM's ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field.</details> | http://arxiv.org/abs/2410.20666v1 |
| Generating and Optimizing Topologically Distinct Guesses for Mobile   Manipulator Path Planning | Rufus Cheuk Yin Wong; Mayank Sewlia; Adrian Wiltz; Dimos V. Dimarogonas | 2024-10-27 | <details><summary>Click to expand</summary>Optimal path planning often suffers from getting stuck in a local optimum. This is often the case for mobile manipulators due to nonconvexities induced by obstacles and robot kinematics. This paper attempts to circumvent this issue by proposing a pipeline to obtain multiple distinct local optima. By evaluating and selecting the optimum among multiple distinct local optima, it is likely to obtain a closer approximation of the global optimum. We demonstrate this capability in optimal path planning of nonholonomic mobile manipulators in the presence of obstacles and subject to end effector path constraints. The nonholomicity, obstacles, and end effector path constraints often cause direct optimal path planning approaches to get stuck in local optima. We demonstrate that our pipeline is able to circumvent this issue and produce a final local optimum that is close to the global optimum.</details> | http://arxiv.org/abs/2410.20635v1 |
| Towards an LLM-Based Speech Interface for Robot-Assisted Feeding | Jessie Yuan; Janavi Gupta; Akhil Padmanabha; Zulekha Karachiwalla; Carmel Majidi; Henny Admoni; Zackory Erickson | 2024-10-27 | <details><summary>Click to expand</summary>Physically assistive robots present an opportunity to significantly increase the well-being and independence of individuals with motor impairments or other forms of disability who are unable to complete activities of daily living (ADLs). Speech interfaces, especially ones that utilize Large Language Models (LLMs), can enable individuals to effectively and naturally communicate high-level commands and nuanced preferences to robots. In this work, we demonstrate an LLM-based speech interface for a commercially available assistive feeding robot. Our system is based on an iteratively designed framework, from the paper "VoicePilot: Harnessing LLMs as Speech Interfaces for Physically Assistive Robots," that incorporates human-centric elements for integrating LLMs as interfaces for robots. It has been evaluated through a user study with 11 older adults at an independent living facility. Videos are located on our project website: https://sites.google.com/andrew.cmu.edu/voicepilot/.</details> | http://arxiv.org/abs/2410.20624v1 |
| Sensor Fusion for Autonomous Indoor UAV Navigation in Confined Spaces | Alice James; Avishkar Seth; Endrowednes Kuantama; Subhas Mukhopadhyay; Richard Han | 2024-10-27 | <details><summary>Click to expand</summary>In this paper, we address the challenge of navigating through unknown indoor environments using autonomous aerial robots within confined spaces. The core of our system involves the integration of key sensor technologies, including depth sensing from the ZED 2i camera, IMU data, and LiDAR measurements, facilitated by the Robot Operating System (ROS) and RTAB-Map. Through custom designed experiments, we demonstrate the robustness and effectiveness of this approach. Our results showcase a promising navigation accuracy, with errors as low as 0.4 meters, and mapping quality characterized by a Root Mean Square Error (RMSE) of just 0.13 m. Notably, this performance is achieved while maintaining energy efficiency and balanced resource allocation, addressing a crucial concern in UAV applications. Flight tests further underscore the precision of our system in maintaining desired flight orientations, with a remarkable error rate of only 0.1%. This work represents a significant stride in the development of autonomous indoor UAV navigation systems, with potential applications in search and rescue, facility inspection, and environmental monitoring within GPS-denied indoor environments.</details> | http://arxiv.org/abs/2410.20599v1 |
| Aerodynamics and Sensing Analysis for Efficient Drone-Based Parcel   Delivery | Avishkar Seth; Alice James; Endrowednes Kuantama; Subhas Mukhopadhyay; Richard Han | 2024-10-27 | <details><summary>Click to expand</summary>In an era of rapid urbanization and e-commerce growth, efficient parcel delivery methods are crucial. This paper presents a detailed study of the aerodynamics and sensing analysis of drones for parcel delivery. Utilizing Computational Fluid Dynamics (CFD), the study offers a comprehensive airflow analysis, revealing the aerodynamic forces affecting drone stability due to payload capacity. A multidisciplinary approach is employed, integrating mechanical design, control theory, and sensing systems to address the complex issue of parcel positioning. The experimental validation section rigorously tests different size payloads and their positions and impact on drones with maximum thrusts of 2000 gf. The findings prove the drone's capacity to lift a large payload that covers up to 50 percent of the propeller, thereby contributing to optimizing drone designs and sustainable parcel delivery systems. It has been observed that the drone can lift a large payload smoothly when placed above the drone, with an error rate as low as 0.1 percent for roll, pitch, and yaw. This work paved the way for more versatile, real-world applications of drone technology, setting a new standard in the field.</details> | http://arxiv.org/abs/2410.20584v1 |
| Comparing the Consistency of User Studies Conducted in Simulations and   Laboratory Settings | Jonathan Hümmer; Dominik Riedelbauch; Dominik Henrich | 2024-10-27 | <details><summary>Click to expand</summary>Human-robot collaboration enables highly adaptive co-working. The variety of resulting workflows makes it difficult to measure metrics as, e.g. makespans or idle times for multiple systems and tasks in a comparable manner. This issue can be addressed with virtual commissioning, where arbitrary numbers of non-deterministic human-robot workflows in assembly tasks can be simulated. To this end, data-driven models of human decisions are needed. Gathering the required large corpus of data with on-site user studies is quite time-consuming. In comparison, simulation-based studies (e.g., by crowdsourcing) would allow us to access a large pool of study participants with less effort. To rely on respective study results, human action sequences observed in a browser-based simulation environment must be shown to match those gathered in a laboratory setting. To this end, this work aims to understand to what extent cooperative assembly work in a simulated environment differs from that in an on-site laboratory setting. We show how a simulation environment can be aligned with a laboratory setting in which a robot and a human perform pick-and-place tasks together. A user study (N=29) indicates that participants' assembly decisions and perception of the situation are consistent across these different environments.</details> | http://arxiv.org/abs/2410.20549v1 |
| Uncertainty-Aware Decision-Making and Planning for Autonomous Forced   Merging | Jian Zhou; Yulong Gao; Björn Olofsson; Erik Frisk | 2024-10-27 | <details><summary>Click to expand</summary>In this paper, we develop an uncertainty-aware decision-making and motion-planning method for an autonomous ego vehicle in forced merging scenarios, considering the motion uncertainty of surrounding vehicles. The method dynamically captures the uncertainty of surrounding vehicles by online estimation of their acceleration bounds, enabling a reactive but rapid understanding of the uncertainty characteristics of the surrounding vehicles. By leveraging these estimated bounds, a non-conservative forward occupancy of surrounding vehicles is predicted over a horizon, which is incorporated in both the decision-making process and the motion-planning strategy, to enhance the resilience and safety of the planned reference trajectory. The method successfully fulfills the tasks in challenging forced merging scenarios, and the properties are illustrated by comparison with several alternative approaches.</details> | http://arxiv.org/abs/2410.20514v1 |
| Referring Human Pose and Mask Estimation in the Wild | Bo Miao; Mingtao Feng; Zijie Wu; Mohammed Bennamoun; Yongsheng Gao; Ajmal Mian | 2024-10-27 | <details><summary>Click to expand</summary>We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild, where either a text or positional prompt specifies the person of interest in an image. This new task holds significant potential for human-centric applications such as assistive robotics and sports analysis. In contrast to previous works, R-HPM (i) ensures high-quality, identity-aware results corresponding to the referred person, and (ii) simultaneously predicts human pose and mask for a comprehensive representation. To achieve this, we introduce a large-scale dataset named RefHuman, which substantially extends the MS COCO dataset with additional text and positional prompt annotations. RefHuman includes over 50,000 annotated instances in the wild, each equipped with keypoint, mask, and prompt annotations. To enable prompt-conditioned estimation, we propose the first end-to-end promptable approach named UniPHD for R-HPM. UniPHD extracts multimodal representations and employs a proposed pose-centric hierarchical decoder to process (text or positional) instance queries and keypoint queries, producing results specific to the referred person. Extensive experiments demonstrate that UniPHD produces quality results based on user-friendly prompts and achieves top-tier performance on RefHuman val and MS COCO val2017. Data and Code: https://github.com/bo-miao/RefHuman</details> | http://arxiv.org/abs/2410.20508v1 |
| Trust-Aware Assistance Seeking in Human-Supervised Autonomy | Dong Hae Mangalindan; Ericka Rovira; Vaibhav Srivastava | 2024-10-27 | <details><summary>Click to expand</summary>Our goal is to model and experimentally assess trust evolution to predict future beliefs and behaviors of human-robot teams in dynamic environments. Research suggests that maintaining trust among team members in a human-robot team is vital for successful team performance. Research suggests that trust is a multi-dimensional and latent entity that relates to past experiences and future actions in a complex manner. Employing a human-robot collaborative task, we design an optimal assistance-seeking strategy for the robot using a POMDP framework. In the task, the human supervises an autonomous mobile manipulator collecting objects in an environment. The supervisor's task is to ensure that the robot safely executes its task. The robot can either choose to attempt to collect the object or seek human assistance. The human supervisor actively monitors the robot's activities, offering assistance upon request, and intervening if they perceive the robot may fail. In this setting, human trust is the hidden state, and the primary objective is to optimize team performance. We execute two sets of human-robot interaction experiments. The data from the first experiment are used to estimate POMDP parameters, which are used to compute an optimal assistance-seeking policy evaluated in the second experiment. The estimated POMDP parameters reveal that, for most participants, human intervention is more probable when trust is low, particularly in high-complexity tasks. Our estimates suggest that the robot's action of asking for assistance in high-complexity tasks can positively impact human trust. Our experimental results show that the proposed trust-aware policy is better than an optimal trust-agnostic policy. By comparing model estimates of human trust, obtained using only behavioral data, with the collected self-reported trust values, we show that model estimates are isomorphic to self-reported responses.</details> | http://arxiv.org/abs/2410.20496v1 |
| Efficient Diversity-based Experience Replay for Deep Reinforcement   Learning | Kaiyan Zhao; Yiming Wang; Yuyang Chen; Xiaoguang Niu; Yan Li; Leong Hou U | 2024-10-27 | <details><summary>Click to expand</summary>Deep Reinforcement Learning (DRL) has achieved remarkable success in solving complex decision-making problems by combining the representation capabilities of deep learning with the decision-making power of reinforcement learning. However, learning in sparse reward environments remains challenging due to insufficient feedback to guide the optimization of agents, especially in real-life environments with high-dimensional states. To tackle this issue, experience replay is commonly introduced to enhance learning efficiency through past experiences. Nonetheless, current methods of experience replay, whether based on uniform or prioritized sampling, frequently struggle with suboptimal learning efficiency and insufficient utilization of samples. This paper proposes a novel approach, diversity-based experience replay (DBER), which leverages the deterministic point process to prioritize diverse samples in state realizations. We conducted extensive experiments on Robotic Manipulation tasks in MuJoCo, Atari games, and realistic in-door environments in Habitat. The results show that our method not only significantly improves learning efficiency but also demonstrates superior performance in sparse reward environments with high-dimensional states, providing a simple yet effective solution for this field.</details> | http://arxiv.org/abs/2410.20487v1 |
| A Deconfounding Framework for Human Behavior Prediction: Enhancing   Robotic Systems in Dynamic Environments | Wentao Gao; Cheng Zhou | 2024-10-27 | <details><summary>Click to expand</summary>Accurate prediction of human behavior is crucial for effective human-robot interaction (HRI) systems, especially in dynamic environments where real-time decisions are essential. This paper addresses the challenge of forecasting future human behavior using multivariate time series data from wearable sensors, which capture various aspects of human movement. The presence of hidden confounding factors in this data often leads to biased predictions, limiting the reliability of traditional models. To overcome this, we propose a robust predictive model that integrates deconfounding techniques with advanced time series prediction methods, enhancing the model's ability to isolate true causal relationships and improve prediction accuracy. Evaluation on real-world datasets demonstrates that our approach significantly outperforms traditional methods, providing a more reliable foundation for responsive and adaptive HRI systems.</details> | http://arxiv.org/abs/2410.20423v1 |
| Trajectory Reconstruction through a Gaussian Adaptive Selective Outlier   Rejecting Smoother | Arslan Majal; Aamir Hussain Chughtai | 2024-10-27 | <details><summary>Click to expand</summary>Trajectory Reconstruction (TR) is vital for accurately mapping movement patterns and validating analyses, especially in fields like robotics, biomechanics, and environmental tracking, where data might be missing or affected by outliers. Improving Trajectory estimation by employing Gaussian smoothing techniques in the presence of non-Gaussian noise is the subject of this work. We consider the case where data is collected from independent sensors. A variational Bayesian (VB) based Unscented Raunch-Tung-Striebel smoothing (URTSS) scheme is proposed which adopts a vectorized weighing mechanism for the measurement covariance matrix to selectively remove contaminated measurements at each time step. To improve our outlier mitigation, we model our outlier characteristics as a Gamma distribution and dynamically learn the parameters of this distribution from data. We verify the performance of our proposed smoother by a range of simulations and experimental data. We also propose a robustness criterion for smoothers based on the Kullback-Leibler (KL) divergence and show that our proposed method complies with this criterion.</details> | http://arxiv.org/abs/2410.20411v1 |
| A CT-guided Control Framework of a Robotic Flexible Endoscope for the   Diagnosis of the Maxillary Sinusitis | Puchen Zhu; Huayu Zhang; Xin Ma; Xiaoyin Zheng; Xuchen Wang; Kwok Wai Samuel Au | 2024-10-27 | <details><summary>Click to expand</summary>Flexible endoscopes are commonly adopted in narrow and confined anatomical cavities due to their higher reachability and dexterity. However, prolonged and unintuitive manipulation of these endoscopes leads to an increased workload on surgeons and risks of collision. To address these challenges, this paper proposes a CT-guided control framework for the diagnosis of maxillary sinusitis by using a robotic flexible endoscope. In the CT-guided control framework, a feasible path to the target position in the maxillary sinus cavity for the robotic flexible endoscope is designed. Besides, an optimal control scheme is proposed to autonomously control the robotic flexible endoscope to follow the feasible path. This greatly improves the efficiency and reduces the workload for surgeons. Several experiments were conducted based on a widely utilized sinus phantom, and the results showed that the robotic flexible endoscope can accurately and autonomously follow the feasible path and reach the target position in the maxillary sinus cavity. The results also verified the feasibility of the CT-guided control framework, which contributes an effective approach to early diagnosis of sinusitis in the future.</details> | http://arxiv.org/abs/2410.20374v1 |
| Dynamics as Prompts: In-Context Learning for Sim-to-Real System   Identifications | Xilun Zhang; Shiqi Liu; Peide Huang; William Jongwon Han; Yiqi Lyu; Mengdi Xu; Ding Zhao | 2024-10-27 | <details><summary>Click to expand</summary>Sim-to-real transfer remains a significant challenge in robotics due to the discrepancies between simulated and real-world dynamics. Traditional methods like Domain Randomization often fail to capture fine-grained dynamics, limiting their effectiveness for precise control tasks. In this work, we propose a novel approach that dynamically adjusts simulation environment parameters online using in-context learning. By leveraging past interaction histories as context, our method adapts the simulation environment dynamics to real-world dynamics without requiring gradient updates, resulting in faster and more accurate alignment between simulated and real-world performance. We validate our approach across two tasks: object scooping and table air hockey. In the sim-to-sim evaluations, our method significantly outperforms the baselines on environment parameter estimation by 80% and 42% in the object scooping and table air hockey setups, respectively. Furthermore, our method achieves at least 70% success rate in sim-to-real transfer on object scooping across three different objects. By incorporating historical interaction data, our approach delivers efficient and smooth system identification, advancing the deployment of robots in dynamic real-world scenarios. Demos are available on our project page: https://sim2real-capture.github.io/</details> | http://arxiv.org/abs/2410.20357v1 |
| SEEV: Synthesis with Efficient Exact Verification for ReLU Neural   Barrier Functions | Hongchao Zhang; Zhizhen Qin; Sicun Gao; Andrew Clark | 2024-10-27 | <details><summary>Click to expand</summary>Neural Control Barrier Functions (NCBFs) have shown significant promise in enforcing safety constraints on nonlinear autonomous systems. State-of-the-art exact approaches to verifying safety of NCBF-based controllers exploit the piecewise-linear structure of ReLU neural networks, however, such approaches still rely on enumerating all of the activation regions of the network near the safety boundary, thus incurring high computation cost. In this paper, we propose a framework for Synthesis with Efficient Exact Verification (SEEV). Our framework consists of two components, namely (i) an NCBF synthesis algorithm that introduces a novel regularizer to reduce the number of activation regions at the safety boundary, and (ii) a verification algorithm that exploits tight over-approximations of the safety conditions to reduce the cost of verifying each piecewise-linear segment. Our simulations show that SEEV significantly improves verification efficiency while maintaining the CBF quality across various benchmark systems and neural network structures. Our code is available at https://github.com/HongchaoZhang-HZ/SEEV.</details> | http://arxiv.org/abs/2410.20326v1 |
| An Optimization-Based Inverse Kinematics Solver for Continuum   Manipulators in Intricate Environments | Yinan Sun; Sai Wang | 2024-10-27 | <details><summary>Click to expand</summary>Continuum manipulators have gained significant attention as a promising alternative to rigid manipulators, offering notable advantages in terms of flexibility and adaptability within intricate workspace. However, the broader application of high degree-of-freedom (DoF) continuum manipulators in intricate environments with multiple obstacles necessitates the development of an efficient inverse kinematics (IK) solver specifically tailored for such scenarios. Existing IK methods face challenges in terms of computational cost and solution guarantees for high DoF continuum manipulators, particularly within intricate workspace that obstacle avoidance is needed. To address these challenges, we have developed a novel IK solver for continuum manipulators that incorporates obstacle avoidance and other constraints like length, orientation, etc., in intricate environments, drawing inspiration from optimization-based path planning methods. Through simulations, our proposed method showcases superior flexibility, efficiency with increasing DoF, and robust performance within highly unstructured workspace, achieved with acceptable latency.</details> | http://arxiv.org/abs/2410.20311v1 |
| HIRO: Heuristics Informed Robot Online Path Planning Using Pre-computed   Deterministic Roadmaps | Xi Huang; Gergely Sóti; Hongyi Zhou; Christoph Ledermann; Björn Hein; Torsten Kröger | 2024-10-26 | <details><summary>Click to expand</summary>With the goal of efficiently computing collision-free robot motion trajectories in dynamically changing environments, we present results of a novel method for Heuristics Informed Robot Online Path Planning (HIRO). Dividing robot environments into static and dynamic elements, we use the static part for initializing a deterministic roadmap, which provides a lower bound of the final path cost as informed heuristics for fast path-finding. These heuristics guide a search tree to explore the roadmap during runtime. The search tree examines the edges using a fuzzy collision checking concerning the dynamic environment. Finally, the heuristics tree exploits knowledge fed back from the fuzzy collision checking module and updates the lower bound for the path cost. As we demonstrate in real-world experiments, the closed-loop formed by these three components significantly accelerates the planning procedure. An additional backtracking step ensures the feasibility of the resulting paths. Experiments in simulation and the real world show that HIRO can find collision-free paths considerably faster than baseline methods with and without prior knowledge of the environment.</details> | http://arxiv.org/abs/2410.20279v1 |
| Planning with Learned Subgoals Selected by Temporal Information | Xi Huang; Gergely Sóti; Christoph Ledermann; Björn Hein; Torsten Kröger | 2024-10-26 | <details><summary>Click to expand</summary>Path planning in a changing environment is a challenging task in robotics, as moving objects impose time-dependent constraints. Recent planning methods primarily focus on the spatial aspects, lacking the capability to directly incorporate time constraints. In this paper, we propose a method that leverages a generative model to decompose a complex planning problem into small manageable ones by incrementally generating subgoals given the current planning context. Then, we take into account the temporal information and use learned time estimators based on different statistic distributions to examine and select the generated subgoal candidates. Experiments show that planning from the current robot state to the selected subgoal can satisfy the given time-dependent constraints while being goal-oriented.</details> | http://arxiv.org/abs/2410.20272v1 |
| Learning Approximated Maximal Safe Sets via Hypernetworks for MPC-Based   Local Motion Planning | Bojan Derajić; Mohamed-Khalil Bouzidi; Sebastian Bernhard; Wolfgang Hönig | 2024-10-26 | <details><summary>Click to expand</summary>This paper presents a novel learning-based approach for online estimation of maximal safe sets for local motion planning tasks in mobile robotics. We leverage the idea of hypernetworks to achieve good generalization properties and real-time performance simultaneously. As the source of supervision, we employ the Hamilton-Jacobi (HJ) reachability analysis, allowing us to consider general nonlinear dynamics and arbitrary constraints. We integrate our model into a model predictive control (MPC) local planner as a safety constraint and compare the performance with relevant baselines in realistic 3D simulations for different environments and robot dynamics. The results show the advantages of our approach in terms of a significantly higher success rate: 2 to 18 percent over the best baseline, while achieving real-time performance.</details> | http://arxiv.org/abs/2410.20267v1 |
| EfficientEQA: An Efficient Approach for Open Vocabulary Embodied   Question Answering | Kai Cheng; Zhengyuan Li; Xingpeng Sun; Byung-Cheol Min; Amrit Singh Bedi; Aniket Bera | 2024-10-26 | <details><summary>Click to expand</summary>Embodied Question Answering (EQA) is an essential yet challenging task for robotic home assistants. Recent studies have shown that large vision-language models (VLMs) can be effectively utilized for EQA, but existing works either focus on video-based question answering without embodied exploration or rely on closed-form choice sets. In real-world scenarios, a robotic agent must efficiently explore and accurately answer questions in open-vocabulary settings. To address these challenges, we propose a novel framework called EfficientEQA for open-vocabulary EQA, which enables efficient exploration and accurate answering. In EfficientEQA, the robot actively explores unknown environments using Semantic-Value-Weighted Frontier Exploration, a strategy that prioritizes exploration based on semantic importance provided by calibrated confidence from black-box VLMs to quickly gather relevant information. To generate accurate answers, we employ Retrieval-Augmented Generation (RAG), which utilizes BLIP to retrieve useful images from accumulated observations and VLM reasoning to produce responses without relying on predefined answer choices. Additionally, we detect observations that are highly relevant to the question as outliers, allowing the robot to determine when it has sufficient information to stop exploring and provide an answer. Experimental results demonstrate the effectiveness of our approach, showing an improvement in answering accuracy by over 15% and efficiency, measured in running steps, by over 20% compared to state-of-the-art methods.</details> | http://arxiv.org/abs/2410.20263v1 |
| Discovering Robotic Interaction Modes with Discrete Representation   Learning | Liquan Wang; Ankit Goyal; Haoping Xu; Animesh Garg | 2024-10-26 | <details><summary>Click to expand</summary>Human actions manipulating articulated objects, such as opening and closing a drawer, can be categorized into multiple modalities we define as interaction modes. Traditional robot learning approaches lack discrete representations of these modes, which are crucial for empirical sampling and grounding. In this paper, we present ActAIM2, which learns a discrete representation of robot manipulation interaction modes in a purely unsupervised fashion, without the use of expert labels or simulator-based privileged information. Utilizing novel data collection methods involving simulator rollouts, ActAIM2 consists of an interaction mode selector and a low-level action predictor. The selector generates discrete representations of potential interaction modes with self-supervision, while the predictor outputs corresponding action trajectories. Our method is validated through its success rate in manipulating articulated objects and its robustness in sampling meaningful actions from the discrete representation. Extensive experiments demonstrate ActAIM2's effectiveness in enhancing manipulability and generalizability over baselines and ablation studies. For videos and additional results, see our website: https://actaim2.github.io/.</details> | http://arxiv.org/abs/2410.20258v1 |
| That was not what I was aiming at! Differentiating human intent and   outcome in a physically dynamic throwing task | Vidullan Surendran; Alan R. Wagner | 2024-10-26 | <details><summary>Click to expand</summary>Recognising intent in collaborative human robot tasks can improve team performance and human perception of robots. Intent can differ from the observed outcome in the presence of mistakes which are likely in physically dynamic tasks. We created a dataset of 1227 throws of a ball at a target from 10 participants and observed that 47% of throws were mistakes with 16% completely missing the target. Our research leverages facial images capturing the person's reaction to the outcome of a throw to predict when the resulting throw is a mistake and then we determine the actual intent of the throw. The approach we propose for outcome prediction performs 38% better than the two-stream architecture used previously for this task on front-on videos. In addition, we propose a 1-D CNN model which is used in conjunction with priors learned from the frequency of mistakes to provide an end-to-end pipeline for outcome and intent recognition in this throwing task.</details> | http://arxiv.org/abs/2410.20256v1 |
| Overcoming the Sim-to-Real Gap: Leveraging Simulation to Learn to   Explore for Real-World RL | Andrew Wagenmaker; Kevin Huang; Liyiming Ke; Byron Boots; Kevin Jamieson; Abhishek Gupta | 2024-10-26 | <details><summary>Click to expand</summary>In order to mitigate the sample complexity of real-world reinforcement learning, common practice is to first train a policy in a simulator where samples are cheap, and then deploy this policy in the real world, with the hope that it generalizes effectively. Such \emph{direct sim2real} transfer is not guaranteed to succeed, however, and in cases where it fails, it is unclear how to best utilize the simulator. In this work, we show that in many regimes, while direct sim2real transfer may fail, we can utilize the simulator to learn a set of \emph{exploratory} policies which enable efficient exploration in the real world. In particular, in the setting of low-rank MDPs, we show that coupling these exploratory policies with simple, practical approaches -- least-squares regression oracles and naive randomized exploration -- yields a polynomial sample complexity in the real world, an exponential improvement over direct sim2real transfer, or learning without access to a simulator. To the best of our knowledge, this is the first evidence that simulation transfer yields a provable gain in reinforcement learning in settings where direct sim2real transfer fails. We validate our theoretical results on several realistic robotic simulators and a real-world robotic sim2real task, demonstrating that transferring exploratory policies can yield substantial gains in practice as well.</details> | http://arxiv.org/abs/2410.20254v1 |
| FRTree Planner: Robot Navigation in Cluttered and Unknown Environments   with Tree of Free Regions | Yulin Li; Zhicheng Song; Chunxin Zheng; Zhihai Bi; Kai Chen; Michael Yu Wang; Jun Ma | 2024-10-26 | <details><summary>Click to expand</summary>In this work, we present FRTree planner, a novel robot navigation framework that leverages a tree structure of free regions, specifically designed for navigation in cluttered and unknown environments with narrow passages. The framework continuously incorporates real-time perceptive information to identify distinct navigation options and dynamically expands the tree toward explorable and traversable directions. This dynamically constructed tree incrementally encodes the geometric and topological information of the collision-free space, enabling efficient selection of the intermediate goals, navigating around dead-end situations, and avoidance of dynamic obstacles without a prior map. Crucially, our method performs a comprehensive analysis of the geometric relationship between free regions and the robot during online replanning. In particular, the planner assesses the accessibility of candidate passages based on the robot's geometries, facilitating the effective selection of the most viable intermediate goals through accessible narrow passages while minimizing unnecessary detours. By combining the free region information with a bi-level trajectory optimization tailored for robots with specific geometries, our approach generates robust and adaptable obstacle avoidance strategies in confined spaces. Through extensive simulations and real-world experiments, FRTree demonstrates its superiority over benchmark methods in generating safe, efficient motion plans through highly cluttered and unknown terrains with narrow gaps.</details> | http://arxiv.org/abs/2410.20230v1 |
| Neural Fields in Robotics: A Survey | Muhammad Zubair Irshad; Mauro Comi; Yen-Chen Lin; Nick Heppert; Abhinav Valada; Rares Ambrus; Zsolt Kira; Jonathan Tremblay | 2024-10-26 | <details><summary>Click to expand</summary>Neural Fields have emerged as a transformative approach for 3D scene representation in computer vision and robotics, enabling accurate inference of geometry, 3D semantics, and dynamics from posed 2D data. Leveraging differentiable rendering, Neural Fields encompass both continuous implicit and explicit neural representations enabling high-fidelity 3D reconstruction, integration of multi-modal sensor data, and generation of novel viewpoints. This survey explores their applications in robotics, emphasizing their potential to enhance perception, planning, and control. Their compactness, memory efficiency, and differentiability, along with seamless integration with foundation and generative models, make them ideal for real-time applications, improving robot adaptability and decision-making. This paper provides a thorough review of Neural Fields in robotics, categorizing applications across various domains and evaluating their strengths and limitations, based on over 200 papers. First, we present four key Neural Fields frameworks: Occupancy Networks, Signed Distance Fields, Neural Radiance Fields, and Gaussian Splatting. Second, we detail Neural Fields' applications in five major robotics domains: pose estimation, manipulation, navigation, physics, and autonomous driving, highlighting key works and discussing takeaways and open challenges. Finally, we outline the current limitations of Neural Fields in robotics and propose promising directions for future research. Project page: https://robonerf.github.io</details> | http://arxiv.org/abs/2410.20220v1 |
| Velocity-History-Based Soft Actor-Critic Tackling IROS'24 Competition   "AI Olympics with RealAIGym" | Tim Lukas Faust; Habib Maraqten; Erfan Aghadavoodi; Boris Belousov; Jan Peters | 2024-10-26 | <details><summary>Click to expand</summary>The ``AI Olympics with RealAIGym'' competition challenges participants to stabilize chaotic underactuated dynamical systems with advanced control algorithms. In this paper, we present a novel solution submitted to IROS'24 competition, which builds upon Soft Actor-Critic (SAC), a popular model-free entropy-regularized Reinforcement Learning (RL) algorithm. We add a `context' vector to the state, which encodes the immediate history via a Convolutional Neural Network (CNN) to counteract the unmodeled effects on the real system. Our method achieves high performance scores and competitive robustness scores on both tracks of the competition: Pendubot and Acrobot.</details> | http://arxiv.org/abs/2410.20096v1 |
| Towards Robust Algorithms for Surgical Phase Recognition via Digital   Twin-based Scene Representation | Hao Ding; Yuqian Zhang; Hongchao Shu; Xu Lian; Ji Woong Kim; Axel Krieger; Mathias Unberath | 2024-10-26 | <details><summary>Click to expand</summary>Purpose: Surgical phase recognition (SPR) is an integral component of surgical data science, enabling high-level surgical analysis. End-to-end trained neural networks that predict surgical phase directly from videos have shown excellent performance on benchmarks. However, these models struggle with robustness due to non-causal associations in the training set, resulting in poor generalizability. Our goal is to improve model robustness to variations in the surgical videos by leveraging the digital twin (DT) paradigm -- an intermediary layer to separate high-level analysis (SPR) from low-level processing (geometric understanding). This approach takes advantage of the recent vision foundation models that ensure reliable low-level scene understanding to craft DT-based scene representations that support various high-level tasks.   Methods: We present a DT-based framework for SPR from videos. The framework employs vision foundation models to extract representations. We embed the representation in place of raw video inputs in the state-of-the-art Surgformer model. The framework is trained on the Cholec80 dataset and evaluated on out-of-distribution (OOD) and corrupted test samples.   Results: Contrary to the vulnerability of the baseline model, our framework demonstrates strong robustness on both OOD and corrupted samples, with a video-level accuracy of 51.1 on the challenging CRCD dataset, 96.0 on an internal robotics training dataset, and 64.4 on a highly corrupted Cholec80 test set.   Conclusion: Our findings lend support to the thesis that DT-based scene representations are effective in enhancing model robustness. Future work will seek to improve the feature informativeness, automate feature extraction, and incorporate interpretability for a more comprehensive framework.</details> | http://arxiv.org/abs/2410.20026v1 |
| GHIL-Glue: Hierarchical Control with Filtered Subgoal Images | Kyle B. Hatch; Ashwin Balakrishna; Oier Mees; Suraj Nair; Seohong Park; Blake Wulfe; Masha Itkina; Benjamin Eysenbach; Sergey Levine; Thomas Kollar; Benjamin Burchfiel | 2024-10-26 | <details><summary>Click to expand</summary>Image and video generative models that are pre-trained on Internet-scale data can greatly increase the generalization capacity of robot learning systems. These models can function as high-level planners, generating intermediate subgoals for low-level goal-conditioned policies to reach. However, the performance of these systems can be greatly bottlenecked by the interface between generative models and low-level controllers. For example, generative models may predict photorealistic yet physically infeasible frames that confuse low-level policies. Low-level policies may also be sensitive to subtle visual artifacts in generated goal images. This paper addresses these two facets of generalization, providing an interface to effectively "glue together" language-conditioned image or video prediction models with low-level goal-conditioned policies. Our method, Generative Hierarchical Imitation Learning-Glue (GHIL-Glue), filters out subgoals that do not lead to task progress and improves the robustness of goal-conditioned policies to generated subgoals with harmful visual artifacts. We find in extensive experiments in both simulated and real environments that GHIL-Glue achieves a 25% improvement across several hierarchical models that leverage generative subgoals, achieving a new state-of-the-art on the CALVIN simulation benchmark for policies using observations from a single RGB camera. GHIL-Glue also outperforms other generalist robot policies across 3/4 language-conditioned manipulation tasks testing zero-shot generalization in physical experiments.</details> | http://arxiv.org/abs/2410.20018v1 |
| A-MFST: Adaptive Multi-Flow Sparse Tracker for Real-Time Tissue Tracking   Under Occlusion | Yuxin Chen; Zijian Wu; Adam Schmidt; Septimiu E. Salcudean | 2024-10-25 | <details><summary>Click to expand</summary>Purpose: Tissue tracking is critical for downstream tasks in robot-assisted surgery. The Sparse Efficient Neural Depth and Deformation (SENDD) model has previously demonstrated accurate and real-time sparse point tracking, but struggled with occlusion handling. This work extends SENDD to enhance occlusion detection and tracking consistency while maintaining real-time performance. Methods: We use the Segment Anything Model2 (SAM2) to detect and mask occlusions by surgical tools, and we develop and integrate into SENDD an Adaptive Multi-Flow Sparse Tracker (A-MFST) with forward-backward consistency metrics, to enhance occlusion and uncertainty estimation. A-MFST is an unsupervised variant of the Multi-Flow Dense Tracker (MFT). Results: We evaluate our approach on the STIR dataset and demonstrate a significant improvement in tracking accuracy under occlusion, reducing average tracking errors by 12 percent in Mean Endpoint Error (MEE) and showing a 6 percent improvement in the averaged accuracy over thresholds of 4, 8, 16, 32, and 64 pixels. The incorporation of forward-backward consistency further improves the selection of optimal tracking paths, reducing drift and enhancing robustness. Notably, these improvements were achieved without compromising the model's real-time capabilities. Conclusions: Using A-MFST and SAM2, we enhance SENDD's ability to track tissue in real time under instrument and tissue occlusions.</details> | http://arxiv.org/abs/2410.19996v1 |
| On-Robot Reinforcement Learning with Goal-Contrastive Rewards | Ondrej Biza; Thomas Weng; Lingfeng Sun; Karl Schmeckpeper; Tarik Kelestemur; Yecheng Jason Ma; Robert Platt; Jan-Willem van de Meent; Lawson L. S. Wong | 2024-10-25 | <details><summary>Click to expand</summary>Reinforcement Learning (RL) has the potential to enable robots to learn from their own actions in the real world. Unfortunately, RL can be prohibitively expensive, in terms of on-robot runtime, due to inefficient exploration when learning from a sparse reward signal. Designing dense reward functions is labour-intensive and requires domain expertise. In our work, we propose GCR (Goal-Contrastive Rewards), a dense reward function learning method that can be trained on passive video demonstrations. By using videos without actions, our method is easier to scale, as we can use arbitrary videos. GCR combines two loss functions, an implicit value loss function that models how the reward increases when traversing a successful trajectory, and a goal-contrastive loss that discriminates between successful and failed trajectories. We perform experiments in simulated manipulation environments across RoboMimic and MimicGen tasks, as well as in the real world using a Franka arm and a Spot quadruped. We find that GCR leads to a more-sample efficient RL, enabling model-free RL to solve about twice as many tasks as our baseline reward learning methods. We also demonstrate positive cross-embodiment transfer from videos of people and of other robots performing a task. Appendix: \url{https://tinyurl.com/gcr-appendix-2}.</details> | http://arxiv.org/abs/2410.19989v1 |
| Implementación de Navegación en Plataforma Robótica Móvil Basada   en ROS y Gazebo | Angel Da Silva; Santiago Fernández; Braian Vidal; Hiago Sodre; Pablo Moraes; Christopher Peters; Sebastian Barcelona; Vincent Sandin; William Moraes; Ahilen Mazondo; Brandon Macedo; Nathalie Assunção; Bruna de Vargas; André Kelbouscas; Ricardo Grando | 2024-10-25 | <details><summary>Click to expand</summary>This research focused on utilizing ROS2 and Gazebo for simulating the TurtleBot3 robot, with the aim of exploring autonomous navigation capabilities. While the study did not achieve full autonomous navigation, it successfully established the connection between ROS2 and Gazebo and enabled manual simulation of the robot's movements. The primary objective was to understand how these tools can be integrated to support autonomous functions, providing valuable insights into the development process. The results of this work lay the groundwork for future research into autonomous robotics. The topic is particularly engaging for both teenagers and adults interested in discovering how robots function independently and the underlying technology involved. This research highlights the potential for further advancements in autonomous systems and serves as a stepping stone for more in-depth studies in the field.</details> | http://arxiv.org/abs/2410.19972v1 |
| Hybrid Iterative Linear Quadratic Estimation: Optimal Estimation for   Hybrid Systems | J. Joe Payne; James Zhu; Nathan J. Kong; Aaron M. Johnson | 2024-10-25 | <details><summary>Click to expand</summary>In this paper we present Hybrid iterative Linear Quadratic Estimation (HiLQE), an optimization based offline state estimation algorithm for hybrid dynamical systems. We utilize the saltation matrix, a first order approximation of the variational update through an event driven hybrid transition, to calculate gradient information through hybrid events in the backward pass of an iterative linear quadratic optimization over state estimates. This enables accurate computation of the value function approximation at each timestep. Additionally, the forward pass in the iterative algorithm is augmented with hybrid dynamics in the rollout. A reference extension method is used to account for varying impact times when comparing states for the feedback gain in noise calculation. The proposed method is demonstrated on an ASLIP hopper system with position measurements. In comparison to the Salted Kalman Filter (SKF), the algorithm presented here achieves a maximum of 63.55% reduction in estimation error magnitude over all state dimensions near impact events.</details> | http://arxiv.org/abs/2410.19958v1 |
| Evolving Neural Networks Reveal Emergent Collective Behavior from   Minimal Agent Interactions | Guilherme S. Y. Giardini; John F. Hardy II; Carlo R. da Cunha | 2024-10-25 | <details><summary>Click to expand</summary>Understanding the mechanisms behind emergent behaviors in multi-agent systems is critical for advancing fields such as swarm robotics and artificial intelligence. In this study, we investigate how neural networks evolve to control agents' behavior in a dynamic environment, focusing on the relationship between the network's complexity and collective behavior patterns. By performing quantitative and qualitative analyses, we demonstrate that the degree of network non-linearity correlates with the complexity of emergent behaviors. Simpler behaviors, such as lane formation and laminar flow, are characterized by more linear network operations, while complex behaviors like swarming and flocking show highly non-linear neural processing. Moreover, specific environmental parameters, such as moderate noise, broader field of view, and lower agent density, promote the evolution of non-linear networks that drive richer, more intricate collective behaviors. These results highlight the importance of tuning evolutionary conditions to induce desired behaviors in multi-agent systems, offering new pathways for optimizing coordination in autonomous swarms. Our findings contribute to a deeper understanding of how neural mechanisms influence collective dynamics, with implications for the design of intelligent, self-organizing systems.</details> | http://arxiv.org/abs/2410.19718v1 |
| DA-VIL: Adaptive Dual-Arm Manipulation with Reinforcement Learning and   Variable Impedance Control | Md Faizal Karim; Shreya Bollimuntha; Mohammed Saad Hashmi; Autrio Das; Gaurav Singh; Srinath Sridhar; Arun Kumar Singh; Nagamanikandan Govindan; K Madhava Krishna | 2024-10-25 | <details><summary>Click to expand</summary>Dual-arm manipulation is an area of growing interest in the robotics community. Enabling robots to perform tasks that require the coordinated use of two arms, is essential for complex manipulation tasks such as handling large objects, assembling components, and performing human-like interactions. However, achieving effective dual-arm manipulation is challenging due to the need for precise coordination, dynamic adaptability, and the ability to manage interaction forces between the arms and the objects being manipulated. We propose a novel pipeline that combines the advantages of policy learning based on environment feedback and gradient-based optimization to learn controller gains required for the control outputs. This allows the robotic system to dynamically modulate its impedance in response to task demands, ensuring stability and dexterity in dual-arm operations. We evaluate our pipeline on a trajectory-tracking task involving a variety of large, complex objects with different masses and geometries. The performance is then compared to three other established methods for controlling dual-arm robots, demonstrating superior results.</details> | http://arxiv.org/abs/2410.19712v1 |
| IPPON: Common Sense Guided Informative Path Planning for Object Goal   Navigation | Kaixian Qu; Jie Tan; Tingnan Zhang; Fei Xia; Cesar Cadena; Marco Hutter | 2024-10-25 | <details><summary>Click to expand</summary>Navigating efficiently to an object in an unexplored environment is a critical skill for general-purpose intelligent robots. Recent approaches to this object goal navigation problem have embraced a modular strategy, integrating classical exploration algorithms-notably frontier exploration-with a learned semantic mapping/exploration module. This paper introduces a novel informative path planning and 3D object probability mapping approach. The mapping module computes the probability of the object of interest through semantic segmentation and a Bayes filter. Additionally, it stores probabilities for common objects, which semantically guides the exploration based on common sense priors from a large language model. The planner terminates when the current viewpoint captures enough voxels identified with high confidence as the object of interest. Although our planner follows a zero-shot approach, it achieves state-of-the-art performance as measured by the Success weighted by Path Length (SPL) and Soft SPL in the Habitat ObjectNav Challenge 2023, outperforming other works by more than 20%. Furthermore, we validate its effectiveness on real robots. Project webpage: https://ippon-paper.github.io/</details> | http://arxiv.org/abs/2410.19697v1 |
| MILES: Making Imitation Learning Easy with Self-Supervision | Georgios Papagiannis; Edward Johns | 2024-10-25 | <details><summary>Click to expand</summary>Data collection in imitation learning often requires significant, laborious human supervision, such as numerous demonstrations, and/or frequent environment resets for methods that incorporate reinforcement learning. In this work, we propose an alternative approach, MILES: a fully autonomous, self-supervised data collection paradigm, and we show that this enables efficient policy learning from just a single demonstration and a single environment reset. MILES autonomously learns a policy for returning to and then following the single demonstration, whilst being self-guided during data collection, eliminating the need for additional human interventions. We evaluated MILES across several real-world tasks, including tasks that require precise contact-rich manipulation such as locking a lock with a key. We found that, under the constraints of a single demonstration and no repeated environment resetting, MILES significantly outperforms state-of-the-art alternatives like imitation learning methods that leverage reinforcement learning. Videos of our experiments and code can be found on our webpage: www.robot-learning.uk/miles.</details> | http://arxiv.org/abs/2410.19693v1 |
| Soft Finger Grasp Force and Contact State Estimation from Tactile   Sensors | Hun Jang; Joonbum Bae; Kevin Haninger | 2024-10-25 | <details><summary>Click to expand</summary>Soft robotic fingers can improve adaptability in grasping and manipulation, compensating for geometric variation in object or environmental contact, but today lack force capacity and fine dexterity. Integrated tactile sensors can provide grasp and task information which can improve dexterity,but should ideally not require object-specific training. The total force vector exerted by a finger provides general information to the internal grasp forces (e.g. for grasp stability) and, when summed over fingers, an estimate of the external force acting on the grasped object (e.g. for task-level control). In this study, we investigate the efficacy of estimating finger force from integrated soft sensors and use it to estimate contact states. We use a neural network for force regression, collecting labelled data with a force/torque sensor and a range of test objects. Subsequently, we apply this model in a plug-in task scenario and demonstrate its validity in estimating contact states.</details> | http://arxiv.org/abs/2410.19684v1 |
| Perception, Control and Hardware for In-Hand Slip-Aware Object   Manipulation with Parallel Grippers | Gabriel Arslan Waltersson; Yiannis Karayiannidis | 2024-10-25 | <details><summary>Click to expand</summary>Dexterous in-hand manipulation offers significant potential to enhance robotic manipulator capabilities. This paper presents a comprehensive study on custom sensors and parallel gripper hardware specifically designed for in-hand slippage control. The gripper features rapid closed-loop, low-level force control, and is equipped with sensors capable of independently measuring contact forces and sliding velocities. Our system can quickly estimate essential object properties during pick-up using only in-hand sensing, without relying on prior object information. We introduce four distinct slippage controllers: gravity-assisted trajectory following for both rotational and linear slippage, a hinge controller that maintains the object's orientation while the gripper rotates, and a slip-avoidance controller. The system is mounted on a robot arm and validated through extensive experiments involving a diverse range of objects, demonstrating its novel capabilities.</details> | http://arxiv.org/abs/2410.19660v1 |
| APRICOT: Active Preference Learning and Constraint-Aware Task Planning   with LLMs | Huaxiaoyue Wang; Nathaniel Chin; Gonzalo Gonzalez-Pumariega; Xiangwan Sun; Neha Sunkara; Maximus Adrian Pace; Jeannette Bohg; Sanjiban Choudhury | 2024-10-25 | <details><summary>Click to expand</summary>Home robots performing personalized tasks must adeptly balance user preferences with environmental affordances. We focus on organization tasks within constrained spaces, such as arranging items into a refrigerator, where preferences for placement collide with physical limitations. The robot must infer user preferences based on a small set of demonstrations, which is easier for users to provide than extensively defining all their requirements. While recent works use Large Language Models (LLMs) to learn preferences from user demonstrations, they encounter two fundamental challenges. First, there is inherent ambiguity in interpreting user actions, as multiple preferences can often explain a single observed behavior. Second, not all user preferences are practically feasible due to geometric constraints in the environment. To address these challenges, we introduce APRICOT, a novel approach that merges LLM-based Bayesian active preference learning with constraint-aware task planning. APRICOT refines its generated preferences by actively querying the user and dynamically adapts its plan to respect environmental constraints. We evaluate APRICOT on a dataset of diverse organization tasks and demonstrate its effectiveness in real-world scenarios, showing significant improvements in both preference satisfaction and plan feasibility. The project website is at https://portal-cornell.github.io/apricot/</details> | http://arxiv.org/abs/2410.19656v1 |
| Equilibrium Adaptation-Based Control for Track Stand of Single-Track   Two-Wheeled Robots | Boyi Wang; Yang Deng; Feilong Jing; Yiyong Sun; Zhang Chen; Bin Liang | 2024-10-25 | <details><summary>Click to expand</summary>Stationary balance control is challenging for single-track two-wheeled (STTW) robots due to the lack of elegant balancing mechanisms and the conflict between the limited attraction domain and external disturbances. To address the absence of balancing mechanisms, we draw inspiration from cyclists and leverage the track stand maneuver, which relies solely on steering and rear-wheel actuation. To achieve accurate tracking in the presence of matched and mismatched disturbances, we propose an equilibrium adaptation-based control (EABC) scheme that can be seamlessly integrated with standard disturbance observers and controllers. This scheme enables adaptation to slow-varying disturbances by utilizing a disturbed equilibrium estimator, effectively handling both matched and mismatched disturbances in a unified manner while ensuring accurate tracking with zero steady-state error. We integrate the EABC scheme with nonlinear model predictive control (MPC) for the track stand of STTW robots and validate its effectiveness through two experimental scenarios. Our method demonstrates significant improvements in tracking accuracy, reducing errors by several orders of magnitude.</details> | http://arxiv.org/abs/2410.19615v1 |
| Shared Control with Black Box Agents using Oracle Queries | Inbal Avraham; Reuth Mirsky | 2024-10-25 | <details><summary>Click to expand</summary>Shared control problems involve a robot learning to collaborate with a human. When learning a shared control policy, short communication between the agents can often significantly reduce running times and improve the system's accuracy. We extend the shared control problem to include the ability to directly query a cooperating agent. We consider two types of potential responses to a query, namely oracles: one that can provide the learner with the best action they should take, even when that action might be myopically wrong, and one with a bounded knowledge limited to its part of the system. Given this additional information channel, this work further presents three heuristics for choosing when to query: reinforcement learning-based, utility-based, and entropy-based. These heuristics aim to reduce a system's overall learning cost. Empirical results on two environments show the benefits of querying to learn a better control policy and the tradeoffs between the proposed heuristics.</details> | http://arxiv.org/abs/2410.19612v1 |
| Multi-modal Motion Prediction using Temporal Ensembling with   Learning-based Aggregation | Kai-Yin Hong; Chieh-Chih Wang; Wen-Chieh Lin | 2024-10-25 | <details><summary>Click to expand</summary>Recent years have seen a shift towards learning-based methods for trajectory prediction, with challenges remaining in addressing uncertainty and capturing multi-modal distributions. This paper introduces Temporal Ensembling with Learning-based Aggregation, a meta-algorithm designed to mitigate the issue of missing behaviors in trajectory prediction, which leads to inconsistent predictions across consecutive frames. Unlike conventional model ensembling, temporal ensembling leverages predictions from nearby frames to enhance spatial coverage and prediction diversity. By confirming predictions from multiple frames, temporal ensembling compensates for occasional errors in individual frame predictions. Furthermore, trajectory-level aggregation, often utilized in model ensembling, is insufficient for temporal ensembling due to a lack of consideration of traffic context and its tendency to assign candidate trajectories with incorrect driving behaviors to final predictions. We further emphasize the necessity of learning-based aggregation by utilizing mode queries within a DETR-like architecture for our temporal ensembling, leveraging the characteristics of predictions from nearby frames. Our method, validated on the Argoverse 2 dataset, shows notable improvements: a 4% reduction in minADE, a 5% decrease in minFDE, and a 1.16% reduction in the miss rate compared to the strongest baseline, QCNet, highlighting its efficacy and potential in autonomous driving.</details> | http://arxiv.org/abs/2410.19606v1 |
| Beyond the Cascade: Juggling Vanilla Siteswap Patterns | Mario Gomez Andreu; Kai Ploeger; Jan Peters | 2024-10-25 | <details><summary>Click to expand</summary>Being widespread in human motor behavior, dynamic movements demonstrate higher efficiency and greater capacity to address a broader range of skill domains compared to their quasi-static counterparts. Among the frequently studied dynamic manipulation problems, robotic juggling tasks stand out due to their inherent ability to scale their difficulty levels to arbitrary extents, making them an excellent subject for investigation. In this study, we explore juggling patterns with mixed throw heights, following the vanilla siteswap juggling notation, which jugglers widely adopted to describe toss juggling patterns. This requires extending our previous analysis of the simpler cascade juggling task by a throw-height sequence planner and further constraints on the end effector trajectory. These are not necessary for cascade patterns but are vital to achieving patterns with mixed throw heights. Using a simulated environment, we demonstrate successful juggling of most common 3-9 ball siteswap patterns up to 9 ball height, transitions between these patterns, and random sequences covering all possible vanilla siteswap patterns with throws between 2 and 9 ball height. https://kai-ploeger.com/beyond-cascades</details> | http://arxiv.org/abs/2410.19591v1 |
| A Field Calibration Approach for Triaxial MEMS Gyroscopes Based on   Gravity and Rotation Consistency | Yaqi Li; Li Wang; Zhitao Wang; Xiangqing Li; Steven W. Su | 2024-10-25 | <details><summary>Click to expand</summary>This paper developed an efficient method for calibrating triaxial MEMS gyroscopes, which can be effectively utilized in the field environment. The core strategy is to utilize the criterion that the dot product of the measured gravity and the rotation speed in a fixed frame remains constant. To eliminate the impact of external acceleration, the calibration process involves separate procedures for measuring local gravity and rotation speed. Moreover, unlike existing approaches for auto calibration of triaxial sensors that often result in nonlinear optimization problems, the proposed method simplifies the estimation of the gyroscope scale factor by employing a linear least squares algorithm. Extensive numerical simulations have been conducted to analyze the proposed method's performance in calibrating the six-parameter triaxial gyroscope model, taking into consideration measurements corrupted by simulated noise. Experimental validation was also carried out using two commercially available MEMS inertial measurement units (LSM9DS1) and a servo motor. The experimental results effectively demonstrate the efficacy of the proposed calibration approach.</details> | http://arxiv.org/abs/2410.19571v1 |
| Robotic Learning in your Backyard: A Neural Simulator from Open Source   Components | Liyou Zhou; Oleg Sinavski; Athanasios Polydoros | 2024-10-25 | <details><summary>Click to expand</summary>The emergence of 3D Gaussian Splatting for fast and high-quality novel view synthesize has opened up the possibility to construct photo-realistic simulations from video for robotic reinforcement learning. While the approach has been demonstrated in several research papers, the software tools used to build such a simulator remain unavailable or proprietary. We present SplatGym, an open source neural simulator for training data-driven robotic control policies. The simulator creates a photorealistic virtual environment from a single video. It supports ego camera view generation, collision detection, and virtual object in-painting. We demonstrate training several visual navigation policies via reinforcement learning. SplatGym represents a notable first step towards an open-source general-purpose neural environment for robotic learning. It broadens the range of applications that can effectively utilise reinforcement learning by providing convenient and unrestricted tooling, and by eliminating the need for the manual development of conventional 3D environments.</details> | http://arxiv.org/abs/2410.19564v1 |
| SODA: a Soft Origami Dynamic utensil for Assisted feeding | Yuxin Ray Song; Shufan Wang | 2024-10-25 | <details><summary>Click to expand</summary>SODA aims to revolutionize assistive feeding systems by designing a multi-purpose utensil using origami-inspired artificial muscles. Traditional utensils, such as forks and spoons,are hard and stiff, causing discomfort and fear among users, especially when operated by autonomous robotic arms. Additionally, these systems require frequent utensil changes to handle different food types. Our innovative utensil design addresses these issues by offering a versatile, adaptive solution that can seamlessly transition between gripping and scooping various foods without the need for manual intervention. Utilizing the flexibility and strength of origami-inspired artificial muscles, the utensil ensures safe and comfortable interactions, enhancing user experience and efficiency. This approach not only simplifies the feeding process but also promotes greater independence for individuals with limited mobility, contributing to the advancement of soft robotics in healthcare applications.</details> | http://arxiv.org/abs/2410.19558v1 |
| PMM-Net: Single-stage Multi-agent Trajectory Prediction with   Patching-based Embedding and Explicit Modal Modulation | Huajian Liu; Wei Dong; Kunpeng Fan; Chao Wang; Yongzhuo Gao | 2024-10-25 | <details><summary>Click to expand</summary>Analyzing and forecasting trajectories of agents like pedestrians plays a pivotal role for embodied intelligent applications. The inherent indeterminacy of human behavior and complex social interaction among a rich variety of agents make this task more challenging than common time-series forecasting. In this letter, we aim to explore a distinct formulation for multi-agent trajectory prediction framework. Specifically, we proposed a patching-based temporal feature extraction module and a graph-based social feature extraction module, enabling effective feature extraction and cross-scenario generalization. Moreover, we reassess the role of social interaction and present a novel method based on explicit modality modulation to integrate temporal and social features, thereby constructing an efficient single-stage inference pipeline. Results on public benchmark datasets demonstrate the superior performance of our model compared with the state-of-the-art methods. The code is available at: github.com/TIB-K330/pmm-net.</details> | http://arxiv.org/abs/2410.19544v1 |
| COR-MP: Conservation of Resources Model for Maneuver Planning | Karim Essalmi; Fernando Garrido; Fawzi Nashashibi | 2024-10-25 | <details><summary>Click to expand</summary>Decision-making for automated driving remains a challenging task. For their integration into real platforms, these algorithms must guarantee passenger safety and comfort while ensuring interpretability and an appropriate computational time. To model and solve this decision-making problem, we have developed a novel approach called COR-MP (Conservation of Resources model for Maneuver Planning). This model is based on the Conservation of Resources theory, a psychological concept applied to human behavior. COR-MP is based on various driving parameters, such as comfort, safety, or energy, and provides in real-time a profit value that enables us to quantify the impact of a decision on the decision-maker. Our method has been tested and validated through closed-loop simulations using RTMaps middleware, and preliminary results have been obtained by testing COR-MP on a real vehicle.</details> | http://arxiv.org/abs/2410.19510v1 |
| A Robust and Efficient Visual-Inertial Initialization with Probabilistic   Normal Epipolar Constraint | Changshi Mu; Daquan Feng; Qi Zheng; Yuan Zhuang | 2024-10-25 | <details><summary>Click to expand</summary>Accurate and robust initialization is essential for Visual-Inertial Odometry (VIO), as poor initialization can severely degrade pose accuracy. During initialization, it is crucial to estimate parameters such as accelerometer bias, gyroscope bias, initial velocity, and gravity, etc. The IMU sensor requires precise estimation of gyroscope bias because gyroscope bias affects rotation, velocity and position. Most existing VIO initialization methods adopt Structure from Motion (SfM) to solve for gyroscope bias. However, SfM is not stable and efficient enough in fast motion or degenerate scenes. To overcome these limitations, we extended the rotation-translation-decoupling framework by adding new uncertainty parameters and optimization modules. First, we adopt a gyroscope bias optimizer that incorporates probabilistic normal epipolar constraints. Second, we fuse IMU and visual measurements to solve for velocity, gravity, and scale efficiently. Finally, we design an additional refinement module that effectively diminishes gravity and scale errors. Extensive initialization tests on the EuRoC dataset show that our method reduces the gyroscope bias and rotation estimation error by an average of 16% and 4% respectively. It also significantly reduces the gravity error, with an average reduction of 29%.</details> | http://arxiv.org/abs/2410.19473v1 |
| Image-Based Visual Servoing for Enhanced Cooperation of Dual-Arm   Manipulation | Zizhe Zhang; Yuan Yang; Wenqiang Zuo; Guangming Song; Aiguo Song; Yang Shi | 2024-10-25 | <details><summary>Click to expand</summary>The cooperation of a pair of robot manipulators is required to manipulate a target object without any fixtures. The conventional control methods coordinate the end-effector pose of each manipulator with that of the other using their kinematics and joint coordinate measurements. Yet, the manipulators' inaccurate kinematics and joint coordinate measurements can cause significant pose synchronization errors in practice. This paper thus proposes an image-based visual servoing approach for enhancing the cooperation of a dual-arm manipulation system. On top of the classical control, the visual servoing controller lets each manipulator use its carried camera to measure the image features of the other's marker and adapt its end-effector pose with the counterpart on the move. Because visual measurements are robust to kinematic errors, the proposed control can reduce the end-effector pose synchronization errors and the fluctuations of the interaction forces of the pair of manipulators on the move. Theoretical analyses have rigorously proven the stability of the closed-loop system. Comparative experiments on real robots have substantiated the effectiveness of the proposed control.</details> | http://arxiv.org/abs/2410.19432v2 |
| Motion Planning for Robotics: A Review for Sampling-based Planners | Liding Zhang; Kuanqi Cai; Zewei Sun; Zhenshan Bing; Chaoqun Wang; Luis Figueredo; Sami Haddadin; Alois Knoll | 2024-10-25 | <details><summary>Click to expand</summary>Recent advancements in robotics have transformed industries such as manufacturing, logistics, surgery, and planetary exploration. A key challenge is developing efficient motion planning algorithms that allow robots to navigate complex environments while avoiding collisions and optimizing metrics like path length, sweep area, execution time, and energy consumption. Among the available algorithms, sampling-based methods have gained the most traction in both research and industry due to their ability to handle complex environments, explore free space, and offer probabilistic completeness along with other formal guarantees. Despite their widespread application, significant challenges still remain. To advance future planning algorithms, it is essential to review the current state-of-the-art solutions and their limitations. In this context, this work aims to shed light on these challenges and assess the development and applicability of sampling-based methods. Furthermore, we aim to provide an in-depth analysis of the design and evaluation of ten of the most popular planners across various scenarios. Our findings highlight the strides made in sampling-based methods while underscoring persistent challenges. This work offers an overview of the important ongoing research in robotic motion planning.</details> | http://arxiv.org/abs/2410.19414v2 |
| Visual Imitation Learning of Non-Prehensile Manipulation Tasks with   Dynamics-Supervised Models | Abdullah Mustafa; Ryo Hanai; Ixchel Ramirez; Floris Erich; Ryoichi Nakajo; Yukiyasu Domae; Tetsuya Ogata | 2024-10-25 | <details><summary>Click to expand</summary>Unlike quasi-static robotic manipulation tasks like pick-and-place, dynamic tasks such as non-prehensile manipulation pose greater challenges, especially for vision-based control. Successful control requires the extraction of features relevant to the target task. In visual imitation learning settings, these features can be learnt by backpropagating the policy loss through the vision backbone. Yet, this approach tends to learn task-specific features with limited generalizability. Alternatively, learning world models can realize more generalizable vision backbones. Utilizing the learnt features, task-specific policies are subsequently trained. Commonly, these models are trained solely to predict the next RGB state from the current state and action taken. But only-RGB prediction might not fully-capture the task-relevant dynamics. In this work, we hypothesize that direct supervision of target dynamic states (Dynamics Mapping) can learn better dynamics-informed world models. Beside the next RGB reconstruction, the world model is also trained to directly predict position, velocity, and acceleration of environment rigid bodies. To verify our hypothesis, we designed a non-prehensile 2D environment tailored to two tasks: "Balance-Reaching" and "Bin-Dropping". When trained on the first task, dynamics mapping enhanced the task performance under different training configurations (Decoupled, Joint, End-to-End) and policy architectures (Feedforward, Recurrent). Notably, its most significant impact was for world model pretraining boosting the success rate from 21% to 85%. Although frozen dynamics-informed world models could generalize well to a task with in-domain dynamics, but poorly to a one with out-of-domain dynamics.</details> | http://arxiv.org/abs/2410.19379v1 |
| Gaze estimation learning architecture as support to affective, social   and cognitive studies in natural human-robot interaction | Maria Lombardi; Elisa Maiettini; Agnieszka Wykowska; Lorenzo Natale | 2024-10-25 | <details><summary>Click to expand</summary>Gaze is a crucial social cue in any interacting scenario and drives many mechanisms of social cognition (joint and shared attention, predicting human intention, coordination tasks). Gaze direction is an indication of social and emotional functions affecting the way the emotions are perceived. Evidence shows that embodied humanoid robots endowing social abilities can be seen as sophisticated stimuli to unravel many mechanisms of human social cognition while increasing engagement and ecological validity. In this context, building a robotic perception system to automatically estimate the human gaze only relying on robot's sensors is still demanding. Main goal of the paper is to propose a learning robotic architecture estimating the human gaze direction in table-top scenarios without any external hardware. Table-top tasks are largely used in many studies in experimental psychology because they are suitable to implement numerous scenarios allowing agents to collaborate while maintaining a face-to-face interaction. Such an architecture can provide a valuable support in studies where external hardware might represent an obstacle to spontaneous human behaviour, especially in environments less controlled than the laboratory (e.g., in clinical settings). A novel dataset was also collected with the humanoid robot iCub, including images annotated from 24 participants in different gaze conditions.</details> | http://arxiv.org/abs/2410.19374v1 |
| An Enhanced Hierarchical Planning Framework for Multi-Robot Autonomous   Exploration | Gengyuan Cai; Luosong Guo; Xiangmao Chang | 2024-10-25 | <details><summary>Click to expand</summary>The autonomous exploration of environments by multi-robot systems is a critical task with broad applications in rescue missions, exploration endeavors, and beyond. Current approaches often rely on either greedy frontier selection or end-to-end deep reinforcement learning (DRL) methods, yet these methods are frequently hampered by limitations such as short-sightedness, overlooking long-term implications, and convergence difficulties stemming from the intricate high-dimensional learning space. To address these challenges, this paper introduces an innovative integration strategy that combines the low-dimensional action space efficiency of frontier-based methods with the far-sightedness and optimality of DRL-based approaches. We propose a three-tiered planning framework that first identifies frontiers in free space, creating a sparse map representation that lightens data transmission burdens and reduces the DRL action space's dimensionality. Subsequently, we develop a multi-graph neural network (mGNN) that incorporates states of potential targets and robots, leveraging policy-based reinforcement learning to compute affinities, thereby superseding traditional heuristic utility values. Lastly, we implement local routing planning through subsequence search, which avoids exhaustive sequence traversal. Extensive validation across diverse scenarios and comprehensive simulation results demonstrate the effectiveness of our proposed method. Compared to baseline approaches, our framework achieves environmental exploration with fewer time steps and a notable reduction of over 30% in data transmission, showcasing its superiority in terms of efficiency and performance.</details> | http://arxiv.org/abs/2410.19373v1 |
| Context-Based Visual-Language Place Recognition | Soojin Woo; Seong-Woo Kim | 2024-10-25 | <details><summary>Click to expand</summary>In vision-based robot localization and SLAM, Visual Place Recognition (VPR) is essential. This paper addresses the problem of VPR, which involves accurately recognizing the location corresponding to a given query image. A popular approach to vision-based place recognition relies on low-level visual features. Despite significant progress in recent years, place recognition based on low-level visual features is challenging when there are changes in scene appearance. To address this, end-to-end training approaches have been proposed to overcome the limitations of hand-crafted features. However, these approaches still fail under drastic changes and require large amounts of labeled data to train models, presenting a significant limitation. Methods that leverage high-level semantic information, such as objects or categories, have been proposed to handle variations in appearance. In this paper, we introduce a novel VPR approach that remains robust to scene changes and does not require additional training. Our method constructs semantic image descriptors by extracting pixel-level embeddings using a zero-shot, language-driven semantic segmentation model. We validate our approach in challenging place recognition scenarios using real-world public dataset. The experiments demonstrate that our method outperforms non-learned image representation techniques and off-the-shelf convolutional neural network (CNN) descriptors. Our code is available at https: //github.com/woo-soojin/context-based-vlpr.</details> | http://arxiv.org/abs/2410.19341v1 |
| Semantics in Robotics: Environmental Data Can't Yield Conventions of   Human Behaviour | Jamie Milton Freestone | 2024-10-25 | <details><summary>Click to expand</summary>The word semantics, in robotics and AI, has no canonical definition. It usually serves to denote additional data provided to autonomous agents to aid HRI. Most researchers seem, implicitly, to understand that such data cannot simply be extracted from environmental data. I try to make explicit why this is so and argue that so-called semantics are best understood as data comprised of conventions of human behaviour. This includes labels, most obviously, but also places, ontologies, and affordances. Object affordances are especially problematic because they require not only semantics that are not in the environmental data (conventions of object use) but also an understanding of physics and object combinations that would, if achieved, constitute artificial superintelligence.</details> | http://arxiv.org/abs/2410.19308v1 |
| In-Simulation Testing of Deep Learning Vision Models in Autonomous   Robotic Manipulators | Dmytro Humeniuk; Houssem Ben Braiek; Thomas Reid; Foutse Khomh | 2024-10-25 | <details><summary>Click to expand</summary>Testing autonomous robotic manipulators is challenging due to the complex software interactions between vision and control components. A crucial element of modern robotic manipulators is the deep learning based object detection model. The creation and assessment of this model requires real world data, which can be hard to label and collect, especially when the hardware setup is not available. The current techniques primarily focus on using synthetic data to train deep neural networks (DDNs) and identifying failures through offline or online simulation-based testing. However, the process of exploiting the identified failures to uncover design flaws early on, and leveraging the optimized DNN within the simulation to accelerate the engineering of the DNN for real-world tasks remains unclear. To address these challenges, we propose the MARTENS (Manipulator Robot Testing and Enhancement in Simulation) framework, which integrates a photorealistic NVIDIA Isaac Sim simulator with evolutionary search to identify critical scenarios aiming at improving the deep learning vision model and uncovering system design flaws. Evaluation of two industrial case studies demonstrated that MARTENS effectively reveals robotic manipulator system failures, detecting 25 % to 50 % more failures with greater diversity compared to random test generation. The model trained and repaired using the MARTENS approach achieved mean average precision (mAP) scores of 0.91 and 0.82 on real-world images with no prior retraining. Further fine-tuning on real-world images for a few epochs (less than 10) increased the mAP to 0.95 and 0.89 for the first and second use cases, respectively. In contrast, a model trained solely on real-world data achieved mAPs of 0.8 and 0.75 for use case 1 and use case 2 after more than 25 epochs.</details> | http://arxiv.org/abs/2410.19277v1 |
| Non-rigid Relative Placement through 3D Dense Diffusion | Eric Cai; Octavian Donca; Ben Eisner; David Held | 2024-10-25 | <details><summary>Click to expand</summary>The task of "relative placement" is to predict the placement of one object in relation to another, e.g. placing a mug onto a mug rack. Through explicit object-centric geometric reasoning, recent methods for relative placement have made tremendous progress towards data-efficient learning for robot manipulation while generalizing to unseen task variations. However, they have yet to represent deformable transformations, despite the ubiquity of non-rigid bodies in real world settings. As a first step towards bridging this gap, we propose ``cross-displacement" - an extension of the principles of relative placement to geometric relationships between deformable objects - and present a novel vision-based method to learn cross-displacement through dense diffusion. To this end, we demonstrate our method's ability to generalize to unseen object instances, out-of-distribution scene configurations, and multimodal goals on multiple highly deformable tasks (both in simulation and in the real world) beyond the scope of prior works. Supplementary information and videos can be found at https://sites.google.com/view/tax3d-corl-2024 .</details> | http://arxiv.org/abs/2410.19247v2 |
| Empirical Study of Ceiling Proximity Effects and Electrostatic Adhesion   for Small-scale Electroaerodynamic Thrusters | C. Luke Nelson; Grant Nations; Daniel S. Drew | 2024-10-25 | <details><summary>Click to expand</summary>Electroaerodynamic propulsion, where force is produced via the momentum-transferring collisions between accelerated ions and neutral air molecules, is a promising alternative mechanism for flight at the micro air vehicle scale due to its silent and solid-state nature. Its relatively low efficiency, however, has thus far precluded its use in a power-autonomous vehicle; leveraging the efficiency benefits of operation close to a fixed surface is a potential solution. While proximity effects like the ground and ceiling effects have been well-investigated for rotorcraft and flapping wing micro air vehicles, they have not been for electroaerodynamically-propelled fliers. In this work, we investigate the change in performance when centimeter-scale thrusters are operated close to a "ceiling" plane about the inlet. We show a surprising and, until now, unreported effect; a major electrostatic attractive component, analogous to electroadhesive pressure but instead mediated by a stable atmospheric plasma. The isolated electrostatic and fluid dynamic components of the ceiling effect are shown for different distances from the plane and for different materials. We further show that a flange attached to the inlet can vastly increase both components of force. A peak efficiency improvement of 600% is shown close to the ceiling. This work points the way towards effective use of the ceiling effect for power autonomous vehicles, extending flight duration, or as a perching mechanism.</details> | http://arxiv.org/abs/2410.19240v1 |
| Learning Diffusion Policies from Demonstrations For Compliant   Contact-rich Manipulation | Malek Aburub; Cristian C. Beltran-Hernandez; Tatsuya Kamijo; Masashi Hamaya | 2024-10-25 | <details><summary>Click to expand</summary>Robots hold great promise for performing repetitive or hazardous tasks, but achieving human-like dexterity, especially in contact-rich and dynamic environments, remains challenging. Rigid robots, which rely on position or velocity control, often struggle with maintaining stable contact and applying consistent force in force-intensive tasks. Learning from Demonstration has emerged as a solution, but tasks requiring intricate maneuvers, such as powder grinding, present unique difficulties. This paper introduces Diffusion Policies For Compliant Manipulation (DIPCOM), a novel diffusion-based framework designed for compliant control tasks. By leveraging generative diffusion models, we develop a policy that predicts Cartesian end-effector poses and adjusts arm stiffness to maintain the necessary force. Our approach enhances force control through multimodal distribution modeling, improves the integration of diffusion policies in compliance control, and extends our previous work by demonstrating its effectiveness in real-world tasks. We present a detailed comparison between our framework and existing methods, highlighting the advantages and best practices for deploying diffusion-based compliance control.</details> | http://arxiv.org/abs/2410.19235v1 |
| Robot Behavior Personalization from Sparse User Feedback | Maithili Patel; Sonia Chernova | 2024-10-25 | <details><summary>Click to expand</summary>As service robots become more general-purpose, they will need to adapt to their users' preferences over a large set of all possible tasks that they can perform. This includes preferences regarding which actions the users prefer to delegate to robots as opposed to doing themselves. Existing personalization approaches require task-specific data for each user. To handle diversity across all household tasks and users, and nuances in user preferences across tasks, we propose to learn a task adaptation function independently, which can be used in tandem with any universal robot policy to customize robot behavior. We create Task Adaptation using Abstract Concepts (TAACo) framework. TAACo can learn to predict the user's preferred manner of assistance with any given task, by mediating reasoning through a representation composed of abstract concepts built based on user feedback. TAACo can generalize to an open set of household tasks from small amount of user feedback and explain its inferences through intuitive concepts. We evaluate our model on a dataset we collected of 5 people's preferences, and show that TAACo outperforms GPT-4 by 16% and a rule-based system by 54%, on prediction accuracy, with 40 samples of user feedback.</details> | http://arxiv.org/abs/2410.19219v1 |
| Collective behavior of "flexicles" | Philipp W. A. Schönhöfer; Sharon C. Glotzer | 2024-10-24 | <details><summary>Click to expand</summary>In recent years the functionality of synthetic active microparticles has edged even closer to that of their biological counterparts. However, we still lack the understanding needed to recreate at the microscale key features of autonomous behavior exhibited by microorganisms or swarms of macroscopic robots. In this study, we propose a model for a three-dimensional deformable cellular composite particle consisting of self-propelled rod-shaped colloids confined within a flexible vesicle - a superstructure we call a "flexicle". Using molecular dynamics simulations, we investigate the collective behavior of dense systems comprised of many flexicles. We show that individual flexicles exhibit shape changes upon collisions with other flexicles that lead to rearrangement of the internal active rods that slow the flexicle motion significantly. This shape deformability gives rise to a diverse set of motility-induced phase separation phenomena and the spontaneous flow of flexicles akin to the migration of cells in dense tissues. Our findings establish a foundation for designing responsive cell-like active particles and developing strategies for controlling swarm migration and other autonomous swarm behaviors at cellular and colloidal scales.</details> | http://arxiv.org/abs/2410.19172v1 |
| SoftSnap: Rapid Prototyping of Untethered Soft Robots Using   Snap-Together Modules | Luyang Zhao; Yitao Jiang; Chun-Yi She; Muhao Chen; Devin Balkcom | 2024-10-24 | <details><summary>Click to expand</summary>Soft robots offer adaptability and safe interaction with complex environments. Rapid prototyping kits that allow soft robots to be assembled easily will allow different geometries to be explored quickly to suit different environments or to mimic the motion of biological organisms. We introduce SoftSnap modules: snap-together components that enable the rapid assembly of a class of untethered soft robots. Each SoftSnap module includes embedded computation, motor-driven string actuation, and a flexible thermoplastic polyurethane (TPU) printed structure capable of deforming into various shapes based on the string configuration. These modules can be easily connected with other SoftSnap modules or customizable connectors. We demonstrate the versatility of the SoftSnap system through four configurations: a starfish-like robot, a brittle star robot, a snake robot, a 3D gripper, and a ring-shaped robot. These configurations highlight the ease of assembly, adaptability, and functional diversity of the SoftSnap modules. The SoftSnap modular system offers a scalable, snap-together approach to simplifying soft robot prototyping, making it easier for researchers to explore untethered soft robotic systems rapidly.</details> | http://arxiv.org/abs/2410.19169v1 |
| Collision Avoidance for Convex Primitives via Differentiable   Optimization Based High-Order Control Barrier Functions | Shiqing Wei; Rooholla Khorrambakht; Prashanth Krishnamurthy; Vinicius Mariano Gonçalves; Farshad Khorrami | 2024-10-24 | <details><summary>Click to expand</summary>Ensuring the safety of dynamical systems is crucial, where collision avoidance is a primary concern. Recently, control barrier functions (CBFs) have emerged as an effective method to integrate safety constraints into control synthesis through optimization techniques. However, challenges persist when dealing with convex primitives and tasks requiring torque control, as well as the occurrence of unintended equilibria. This work addresses these challenges by introducing a high-order CBF (HOCBF) framework for collision avoidance among convex primitives. We transform nonconvex safety constraints into linear constraints by differentiable optimization and prove the high-order continuous differentiability. Then, we employ HOCBFs to accommodate torque control, enabling tasks involving forces or high dynamics. Additionally, we analyze the issue of spurious equilibria in high-order cases and propose a circulation mechanism to prevent the undesired equilibria on the boundary of the safe set. Finally, we validate our framework with three experiments on the Franka Research 3 robotic manipulator, demonstrating successful collision avoidance and the efficacy of the circulation mechanism.</details> | http://arxiv.org/abs/2410.19159v1 |
| Versatile Demonstration Interface: Toward More Flexible Robot   Demonstration Collection | Michael Hagenow; Dimosthenis Kontogiorgos; Yanwei Wang; Julie Shah | 2024-10-24 | <details><summary>Click to expand</summary>Previous methods for Learning from Demonstration leverage several approaches for a human to teach motions to a robot, including teleoperation, kinesthetic teaching, and natural demonstrations. However, little previous work has explored more general interfaces that allow for multiple demonstration types. Given the varied preferences of human demonstrators and task characteristics, a flexible tool that enables multiple demonstration types could be crucial for broader robot skill training. In this work, we propose Versatile Demonstration Interface (VDI), an attachment for collaborative robots that simplifies the collection of three common types of demonstrations. Designed for flexible deployment in industrial settings, our tool requires no additional instrumentation of the environment. Our prototype interface captures human demonstrations through a combination of vision, force sensing, and state tracking (e.g., through the robot proprioception or AprilTag tracking). Through a user study where we deployed our prototype VDI at a local manufacturing innovation center with manufacturing experts, we demonstrated the efficacy of our prototype in representative industrial tasks. Interactions from our study exposed a range of industrial use cases for VDI, clear relationships between demonstration preferences and task criteria, and insights for future tool design.</details> | http://arxiv.org/abs/2410.19141v1 |
| Analyzing Human Perceptions of a MEDEVAC Robot in a Simulated Evacuation   Scenario | Tyson Jordan; Pranav Pandey; Prashant Doshi; Ramviyas Parasuraman; Adam Goodie | 2024-10-24 | <details><summary>Click to expand</summary>The use of autonomous systems in medical evacuation (MEDEVAC) scenarios is promising, but existing implementations overlook key insights from human-robot interaction (HRI) research. Studies on human-machine teams demonstrate that human perceptions of a machine teammate are critical in governing the machine's performance. Here, we present a mixed factorial design to assess human perceptions of a MEDEVAC robot in a simulated evacuation scenario. Participants were assigned to the role of casualty (CAS) or bystander (BYS) and subjected to three within-subjects conditions based on the MEDEVAC robot's operating mode: autonomous-slow (AS), autonomous-fast (AF), and teleoperation (TO). During each trial, a MEDEVAC robot navigated an 11-meter path, acquiring a casualty and transporting them to an ambulance exchange point while avoiding an idle bystander. Following each trial, subjects completed a questionnaire measuring their emotional states, perceived safety, and social compatibility with the robot. Results indicate a consistent main effect of operating mode on reported emotional states and perceived safety. Pairwise analyses suggest that the employment of the AF operating mode negatively impacted perceptions along these dimensions. There were no persistent differences between casualty and bystander responses.</details> | http://arxiv.org/abs/2410.19072v2 |
| Self-Improving Autonomous Underwater Manipulation | Ruoshi Liu; Huy Ha; Mengxue Hou; Shuran Song; Carl Vondrick | 2024-10-24 | <details><summary>Click to expand</summary>Underwater robotic manipulation faces significant challenges due to complex fluid dynamics and unstructured environments, causing most manipulation systems to rely heavily on human teleoperation. In this paper, we introduce AquaBot, a fully autonomous manipulation system that combines behavior cloning from human demonstrations with self-learning optimization to improve beyond human teleoperation performance. With extensive real-world experiments, we demonstrate AquaBot's versatility across diverse manipulation tasks, including object grasping, trash sorting, and rescue retrieval. Our real-world experiments show that AquaBot's self-optimized policy outperforms a human operator by 41% in speed. AquaBot represents a promising step towards autonomous and self-improving underwater manipulation systems. We open-source both hardware and software implementation details.</details> | http://arxiv.org/abs/2410.18969v1 |
| Learning to Look: Seeking Information for Decision Making via Policy   Factorization | Shivin Dass; Jiaheng Hu; Ben Abbatematteo; Peter Stone; Roberto Martín-Martín | 2024-10-24 | <details><summary>Click to expand</summary>Many robot manipulation tasks require active or interactive exploration behavior in order to be performed successfully. Such tasks are ubiquitous in embodied domains, where agents must actively search for the information necessary for each stage of a task, e.g., moving the head of the robot to find information relevant to manipulation, or in multi-robot domains, where one scout robot may search for the information that another robot needs to make informed decisions. We identify these tasks with a new type of problem, factorized Contextual Markov Decision Processes, and propose DISaM, a dual-policy solution composed of an information-seeking policy that explores the environment to find the relevant contextual information and an information-receiving policy that exploits the context to achieve the manipulation goal. This factorization allows us to train both policies separately, using the information-receiving one to provide reward to train the information-seeking policy. At test time, the dual agent balances exploration and exploitation based on the uncertainty the manipulation policy has on what the next best action is. We demonstrate the capabilities of our dual policy solution in five manipulation tasks that require information-seeking behaviors, both in simulation and in the real-world, where DISaM significantly outperforms existing methods. More information at https://robin-lab.cs.utexas.edu/learning2look/.</details> | http://arxiv.org/abs/2410.18964v1 |
| ANAVI: Audio Noise Awareness using Visuals of Indoor environments for   NAVIgation | Vidhi Jain; Rishi Veerapaneni; Yonatan Bisk | 2024-10-24 | <details><summary>Click to expand</summary>We propose Audio Noise Awareness using Visuals of Indoors for NAVIgation for quieter robot path planning. While humans are naturally aware of the noise they make and its impact on those around them, robots currently lack this awareness. A key challenge in achieving audio awareness for robots is estimating how loud will the robot's actions be at a listener's location? Since sound depends upon the geometry and material composition of rooms, we train the robot to passively perceive loudness using visual observations of indoor environments. To this end, we generate data on how loud an 'impulse' sounds at different listener locations in simulated homes, and train our Acoustic Noise Predictor (ANP). Next, we collect acoustic profiles corresponding to different actions for navigation. Unifying ANP with action acoustics, we demonstrate experiments with wheeled (Hello Robot Stretch) and legged (Unitree Go2) robots so that these robots adhere to the noise constraints of the environment. See code and data at https://anavi-corl24.github.io/</details> | http://arxiv.org/abs/2410.18932v1 |
| Swarm manipulation: An efficient and accurate technique for multi-object   manipulation in virtual reality | Xiang Li; Jin-Du Wang; John J. Dudley; Per Ola Kristensson | 2024-10-24 | <details><summary>Click to expand</summary>The theory of swarm control shows promise for controlling multiple objects, however, scalability is hindered by cost constraints, such as hardware and infrastructure. Virtual Reality (VR) can overcome these limitations, but research on swarm interaction in VR is limited. This paper introduces a novel Swarm Manipulation interaction technique and compares it with two baseline techniques: Virtual Hand and Controller (ray-casting). We evaluated these techniques in a user study ($N$ = 12) in three tasks (selection, rotation, and resizing) across five conditions. Our results indicate that Swarm Manipulation yielded superior performance, with significantly faster speeds in most conditions across the three tasks. It notably reduced resizing size deviations but introduced a trade-off between speed and accuracy in the rotation task. Additionally, we conducted a follow-up user study ($N$ = 6) using Swarm Manipulation in two complex VR scenarios and obtained insights through semi-structured interviews, shedding light on optimized swarm control mechanisms and perceptual changes induced by this interaction paradigm. These results demonstrate the potential of the Swarm Manipulation technique to enhance the usability and user experience in VR compared to conventional manipulation techniques. In future studies, we aim to understand and improve swarm interaction via internal swarm particle cooperation.</details> | http://arxiv.org/abs/2410.18924v1 |
| Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling | Mingtong Zhang; Kaifeng Zhang; Yunzhu Li | 2024-10-24 | <details><summary>Click to expand</summary>Videos of robots interacting with objects encode rich information about the objects' dynamics. However, existing video prediction approaches typically do not explicitly account for the 3D information from videos, such as robot actions and objects' 3D states, limiting their use in real-world robotic applications. In this work, we introduce a framework to learn object dynamics directly from multi-view RGB videos by explicitly considering the robot's action trajectories and their effects on scene dynamics. We utilize the 3D Gaussian representation of 3D Gaussian Splatting (3DGS) to train a particle-based dynamics model using Graph Neural Networks. This model operates on sparse control particles downsampled from the densely tracked 3D Gaussian reconstructions. By learning the neural dynamics model on offline robot interaction data, our method can predict object motions under varying initial configurations and unseen robot actions. The 3D transformations of Gaussians can be interpolated from the motions of control particles, enabling the rendering of predicted future object states and achieving action-conditioned video prediction. The dynamics model can also be applied to model-based planning frameworks for object manipulation tasks. We conduct experiments on various kinds of deformable materials, including ropes, clothes, and stuffed animals, demonstrating our framework's ability to model complex shapes and dynamics. Our project page is available at https://gs-dynamics.github.io.</details> | http://arxiv.org/abs/2410.18912v1 |
| SkillMimicGen: Automated Demonstration Generation for Efficient Skill   Learning and Deployment | Caelan Garrett; Ajay Mandlekar; Bowen Wen; Dieter Fox | 2024-10-24 | <details><summary>Click to expand</summary>Imitation learning from human demonstrations is an effective paradigm for robot manipulation, but acquiring large datasets is costly and resource-intensive, especially for long-horizon tasks. To address this issue, we propose SkillMimicGen (SkillGen), an automated system for generating demonstration datasets from a few human demos. SkillGen segments human demos into manipulation skills, adapts these skills to new contexts, and stitches them together through free-space transit and transfer motion. We also propose a Hybrid Skill Policy (HSP) framework for learning skill initiation, control, and termination components from SkillGen datasets, enabling skills to be sequenced using motion planning at test-time. We demonstrate that SkillGen greatly improves data generation and policy learning performance over a state-of-the-art data generation framework, resulting in the capability to produce data for large scene variations, including clutter, and agents that are on average 24% more successful. We demonstrate the efficacy of SkillGen by generating over 24K demonstrations across 18 task variants in simulation from just 60 human demonstrations, and training proficient, often near-perfect, HSP agents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also demonstrate zero-shot sim-to-real transfer on a long-horizon assembly task. Videos, and more at https://skillgen.github.io.</details> | http://arxiv.org/abs/2410.18907v1 |
| Creating and Repairing Robot Programs in Open-World Domains | Claire Schlesinger; Arjun Guha; Joydeep Biswas | 2024-10-24 | <details><summary>Click to expand</summary>Using Large Language Models (LLMs) to produce robot programs from natural language has allowed for robot systems that can complete a higher diversity of tasks. However, LLM-generated programs may be faulty, either due to ambiguity in instructions, misinterpretation of the desired task, or missing information about the world state. As these programs run, the state of the world changes and they gather new information. When a failure occurs, it is important that they recover from the current world state and avoid repeating steps that they they previously completed successfully. We propose RoboRepair, a system which traces the execution of a program up until error, and then runs an LLM-produced recovery program that minimizes repeated actions.   To evaluate the efficacy of our system, we create a benchmark consisting of eleven tasks with various error conditions that require the generation of a recovery program. We compare the efficiency of the recovery program to a plan built with an oracle that has foreknowledge of future errors.</details> | http://arxiv.org/abs/2410.18893v1 |
| Diffusion for Multi-Embodiment Grasping | Roman Freiberg; Alexander Qualmann; Ngo Anh Vien; Gerhard Neumann | 2024-10-24 | <details><summary>Click to expand</summary>Grasping is a fundamental skill in robotics with diverse applications across medical, industrial, and domestic domains. However, current approaches for predicting valid grasps are often tailored to specific grippers, limiting their applicability when gripper designs change. To address this limitation, we explore the transfer of grasping strategies between various gripper designs, enabling the use of data from diverse sources. In this work, we present an approach based on equivariant diffusion that facilitates gripper-agnostic encoding of scenes containing graspable objects and gripper-aware decoding of grasp poses by integrating gripper geometry into the model. We also develop a dataset generation framework that produces cluttered scenes with variable-sized object heaps, improving the training of grasp synthesis methods. Experimental evaluation on diverse object datasets demonstrates the generalizability of our approach across gripper architectures, ranging from simple parallel-jaw grippers to humanoid hands, outperforming both single-gripper and multi-gripper state-of-the-art methods.</details> | http://arxiv.org/abs/2410.18835v1 |
| MazeNet: An Accurate, Fast, and Scalable Deep Learning Solution for   Steiner Minimum Trees | Gabriel Díaz Ramos; Toros Arikan; Richard G. Baraniuk | 2024-10-24 | <details><summary>Click to expand</summary>The Obstacle Avoiding Rectilinear Steiner Minimum Tree (OARSMT) problem, which seeks the shortest interconnection of a given number of terminals in a rectilinear plane while avoiding obstacles, is a critical task in integrated circuit design, network optimization, and robot path planning. Since OARSMT is NP-hard, exact algorithms scale poorly with the number of terminals, leading practical solvers to sacrifice accuracy for large problems. We propose MazeNet, a deep learning-based method that learns to solve the OARSMT from data. MazeNet reframes OARSMT as a maze-solving task that can be addressed with a recurrent convolutional neural network (RCNN). A key hallmark of MazeNet is its scalability: we only need to train the RCNN blocks on mazes with a small number of terminals; larger mazes can be solved by replicating the same pre-trained blocks to create a larger network. Across a wide range of experiments, MazeNet achieves perfect OARSMT-solving accuracy, significantly reduces runtime compared to classical exact algorithms, and can handle more terminals than state-of-the-art approximate algorithms.</details> | http://arxiv.org/abs/2410.18832v1 |
| A generic approach for reactive stateful mitigation of application   failures in distributed robotics systems deployed with Kubernetes | Florian Mirus; Frederik Pasch; Nikhil Singhal; Kay-Ulrich Scholl | 2024-10-24 | <details><summary>Click to expand</summary>Offloading computationally expensive algorithms to the edge or even cloud offers an attractive option to tackle limitations regarding on-board computational and energy resources of robotic systems. In cloud-native applications deployed with the container management system Kubernetes (K8s), one key problem is ensuring resilience against various types of failures. However, complex robotic systems interacting with the physical world pose a very specific set of challenges and requirements that are not yet covered by failure mitigation approaches from the cloud-native domain. In this paper, we therefore propose a novel approach for robotic system monitoring and stateful, reactive failure mitigation for distributed robotic systems deployed using Kubernetes (K8s) and the Robot Operating System (ROS2). By employing the generic substrate of Behaviour Trees, our approach can be applied to any robotic workload and supports arbitrarily complex monitoring and failure mitigation strategies. We demonstrate the effectiveness and application-agnosticism of our approach on two example applications, namely Autonomous Mobile Robot (AMR) navigation and robotic manipulation in a simulated environment.</details> | http://arxiv.org/abs/2410.18825v2 |
| PointPatchRL -- Masked Reconstruction Improves Reinforcement Learning on   Point Clouds | Balázs Gyenes; Nikolai Franke; Philipp Becker; Gerhard Neumann | 2024-10-24 | <details><summary>Click to expand</summary>Perceiving the environment via cameras is crucial for Reinforcement Learning (RL) in robotics. While images are a convenient form of representation, they often complicate extracting important geometric details, especially with varying geometries or deformable objects. In contrast, point clouds naturally represent this geometry and easily integrate color and positional data from multiple camera views. However, while deep learning on point clouds has seen many recent successes, RL on point clouds is under-researched, with only the simplest encoder architecture considered in the literature. We introduce PointPatchRL (PPRL), a method for RL on point clouds that builds on the common paradigm of dividing point clouds into overlapping patches, tokenizing them, and processing the tokens with transformers. PPRL provides significant improvements compared with other point-cloud processing architectures previously used for RL. We then complement PPRL with masked reconstruction for representation learning and show that our method outperforms strong model-free and model-based baselines on image observations in complex manipulation tasks containing deformable objects and variations in target object geometry. Videos and code are available at https://alrhub.github.io/pprl-website</details> | http://arxiv.org/abs/2410.18800v1 |
| Online path planning for kinematic-constrained UAVs in a dynamic   environment based on a Differential Evolution algorithm | Elias J. R. Freitas; Miri Weiss Cohen; Frederico G. Guimarães; Luciano C. A. Pimenta | 2024-10-24 | <details><summary>Click to expand</summary>This research presents an online path planner for Unmanned Aerial Vehicles (UAVs) that can handle dynamic obstacles and UAV motion constraints, including maximum curvature and desired orientations. Our proposed planner uses a NURBS path representation and a Differential Evolution algorithm, incorporating concepts from the Velocity Obstacle approach in a constraint function. Initial results show that our approach is feasible and provides a foundation for future extensions to three-dimensional (3D) environments.</details> | http://arxiv.org/abs/2410.18777v1 |
| Perspectives on the Physics of Late-Type Stars from Beyond Low Earth   Orbit, the Moon and Mars | Savita Mathur; Ângela R. G. Santos | 2024-10-24 | <details><summary>Click to expand</summary>With the new discoveries enabled thanks to the recent space missions, stellar physics is going through a revolution. However, these discoveries opened the door to many new questions that require more observations. The European Space Agency's Human and Robotic Exploration programme provides an excellent opportunity to push forward the limits of our knowledge and better understand stellar structure and dynamics evolution. Long-term observations, Ultra-Violet observations, and a stellar imager are a few highlights of proposed missions for late-type stars that will enhance the already planned space missions.</details> | http://arxiv.org/abs/2410.19026v1 |
| Breaking Down the Barriers: Investigating Non-Expert User Experiences in   Robotic Teleoperation in UK and Japan | Florent P Audonnet; Andrew Hamilton; Yakiyasu Domae; Ixchel G Ramirez-Alpizar; Gerardo Aragon-Camarasa | 2024-10-24 | <details><summary>Click to expand</summary>Robots are being created each year with the goal of integrating them into our daily lives. As such, there is an interest in research in evaluating the trust of humans toward robots. In addition, teleoperating robotic arms can be challenging for non-experts. In order to reduce the strain put on the user, we created TELESIM, a modular and plug-and-play framework that enables direct teleoperation of any robotic arm using a digital twin as the interface between users and the robotic system. However, analysis of the strain put on the user and its ability to trust robots was omitted. This paper addresses these omissions by presenting the additional results of our user survey of 37 participants carried out in UK. In addition, we present the results of an additional user survey, under similar conditions performed in Japan, with the goal of addressing the limitations of our previous approach, by interfacing a VR controller with a UR5e. Our experimental results show that the UR5e has a higher number of towers built. Additionally, the UR5e gives the least amount of cognitive stress, while the combination of Senseglove and UR3 gives the user the highest physical strain and causes the user to feel more frustrated. Finally, Japanese seems more trusting towards robots than British.</details> | http://arxiv.org/abs/2410.18727v1 |
| Rigid Single-Slice-in-Volume registration via rotation-equivariant 2D/3D   feature matching | Stefan Brandstätter; Philipp Seeböck; Christoph Fürböck; Svitlana Pochepnia; Helmut Prosch; Georg Langs | 2024-10-24 | <details><summary>Click to expand</summary>2D to 3D registration is essential in tasks such as diagnosis, surgical navigation, environmental understanding, navigation in robotics, autonomous systems, or augmented reality. In medical imaging, the aim is often to place a 2D image in a 3D volumetric observation to w. Current approaches for rigid single slice in volume registration are limited by requirements such as pose initialization, stacks of adjacent slices, or reliable anatomical landmarks. Here, we propose a self-supervised 2D/3D registration approach to match a single 2D slice to the corresponding 3D volume. The method works in data without anatomical priors such as images of tumors. It addresses the dimensionality disparity and establishes correspondences between 2D in-plane and 3D out-of-plane rotation-equivariant features by using group equivariant CNNs. These rotation-equivariant features are extracted from the 2D query slice and aligned with their 3D counterparts. Results demonstrate the robustness of the proposed slice-in-volume registration on the NSCLC-Radiomics CT and KIRBY21 MRI datasets, attaining an absolute median angle error of less than 2 degrees and a mean-matching feature accuracy of 89% at a tolerance of 3 pixels.</details> | http://arxiv.org/abs/2410.18683v1 |
| Learning dissipative Hamiltonian dynamics with reproducing kernel   Hilbert spaces and random Fourier features | Torbjørn Smith; Olav Egeland | 2024-10-24 | <details><summary>Click to expand</summary>This paper presents a new method for learning dissipative Hamiltonian dynamics from a limited and noisy dataset. The method uses the Helmholtz decomposition to learn a vector field as the sum of a symplectic and a dissipative vector field. The two vector fields are learned using two reproducing kernel Hilbert spaces, defined by a symplectic and a curl-free kernel, where the kernels are specialized to enforce odd symmetry. Random Fourier features are used to approximate the kernels to reduce the dimension of the optimization problem. The performance of the method is validated in simulations for two dissipative Hamiltonian systems, and it is shown that the method improves predictive accuracy significantly compared to a method where a Gaussian separable kernel is used.</details> | http://arxiv.org/abs/2410.18656v1 |
| Data Scaling Laws in Imitation Learning for Robotic Manipulation | Fanqi Lin; Yingdong Hu; Pingyue Sheng; Chuan Wen; Jiacheng You; Yang Gao | 2024-10-24 | <details><summary>Click to expand</summary>Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy's generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90% success rates in novel environments with unseen objects.</details> | http://arxiv.org/abs/2410.18647v1 |
| Moving Object Segmentation in Point Cloud Data using Hidden Markov   Models | Vedant Bhandari; Jasmin James; Tyson Phillips; P. Ross McAree | 2024-10-24 | <details><summary>Click to expand</summary>Autonomous agents require the capability to identify dynamic objects in their environment for safe planning and navigation. Incomplete and erroneous dynamic detections jeopardize the agent's ability to accomplish its task. Dynamic detection is a challenging problem due to the numerous sources of uncertainty inherent in the problem's inputs and the wide variety of applications, which often lead to use-case-tailored solutions. We propose a robust learning-free approach to segment moving objects in point cloud data. The foundation of the approach lies in modelling each voxel using a hidden Markov model (HMM), and probabilistically integrating beliefs into a map using an HMM filter. The proposed approach is tested on benchmark datasets and consistently performs better than or as well as state-of-the-art methods with strong generalized performance across sensor characteristics and environments. The approach is open-sourced at https://github.com/vb44/HMM-MOS.</details> | http://arxiv.org/abs/2410.18638v1 |
| Embodied Manipulation with Past and Future Morphologies through an Open   Parametric Hand Design | Kieran Gilday; Chapa Sirithunge; Fumiya Iida; Josie Hughes | 2024-10-24 | <details><summary>Click to expand</summary>A human-shaped robotic hand offers unparalleled versatility and fine motor skills, enabling it to perform a broad spectrum of tasks with precision, power and robustness. Across the paleontological record and animal kingdom we see a wide range of alternative hand and actuation designs. Understanding the morphological design space and the resulting emergent behaviors can not only aid our understanding of dexterous manipulation and its evolution, but also assist design optimization, achieving, and eventually surpassing human capabilities. Exploration of hand embodiment has to date been limited by inaccessibility of customizable hands in the real-world, and by the reality gap in simulation of complex interactions. We introduce an open parametric design which integrates techniques for simplified customization, fabrication, and control with design features to maximize behavioral diversity. Non-linear rolling joints, anatomical tendon routing, and a low degree-of-freedom, modulating, actuation system, enable rapid production of single-piece 3D printable hands without compromising dexterous behaviors. To demonstrate this, we evaluated the design's low-level behavior range and stability, showing variable stiffness over two orders of magnitude. Additionally, we fabricated three hand designs: human, mirrored human with two thumbs, and aye-aye hands. Manipulation tests evaluate the variation in each hand's proficiency at handling diverse objects, and demonstrate emergent behaviors unique to each design. Overall, we shed light on new possible designs for robotic hands, provide a design space to compare and contrast different hand morphologies and structures, and share a practical and open-source design for exploring embodied manipulation.</details> | http://arxiv.org/abs/2410.18633v1 |
| A Cranial-Feature-Based Registration Scheme for Robotic   Micromanipulation Using a Microscopic Stereo Camera System | Xiaofeng Lin; Saúl Alexis Heredia Pérez; Kanako Harada | 2024-10-24 | <details><summary>Click to expand</summary>Biological specimens exhibit significant variations in size and shape, challenging autonomous robotic manipulation. We focus on the mouse skull window creation task to illustrate these challenges. The study introduces a microscopic stereo camera system (MSCS) enhanced by the linear model for depth perception. Alongside this, a precise registration scheme is developed for the partially exposed mouse cranial surface, employing a CNN-based constrained and colorized registration strategy. These methods are integrated with the MSCS for robotic micromanipulation tasks. The MSCS demonstrated a high precision of 0.10 mm $\pm$ 0.02 mm measured in a step height experiment and real-time performance of 30 FPS in 3D reconstruction. The registration scheme proved its precision, with a translational error of 1.13 mm $\pm$ 0.31 mm and a rotational error of 3.38$^{\circ}$ $\pm$ 0.89$^{\circ}$ tested on 105 continuous frames with an average speed of 1.60 FPS. This study presents the application of a MSCS and a novel registration scheme in enhancing the precision and accuracy of robotic micromanipulation in scientific and surgical settings. The innovations presented here offer automation methodology in handling the challenges of microscopic manipulation, paving the way for more accurate, efficient, and less invasive procedures in various fields of microsurgery and scientific research.</details> | http://arxiv.org/abs/2410.18630v1 |
| Learning Transparent Reward Models via Unsupervised Feature Selection | Daulet Baimukashev; Gokhan Alcan; Kevin Sebastian Luck; Ville Kyrki | 2024-10-24 | <details><summary>Click to expand</summary>In complex real-world tasks such as robotic manipulation and autonomous driving, collecting expert demonstrations is often more straightforward than specifying precise learning objectives and task descriptions. Learning from expert data can be achieved through behavioral cloning or by learning a reward function, i.e., inverse reinforcement learning. The latter allows for training with additional data outside the training distribution, guided by the inferred reward function. We propose a novel approach to construct compact and transparent reward models from automatically selected state features. These inferred rewards have an explicit form and enable the learning of policies that closely match expert behavior by training standard reinforcement learning algorithms from scratch. We validate our method's performance in various robotic environments with continuous and high-dimensional state spaces. Webpage: \url{https://sites.google.com/view/transparent-reward}.</details> | http://arxiv.org/abs/2410.18608v1 |
| AgentStore: Scalable Integration of Heterogeneous Agents As Specialized   Generalist Computer Assistant | Chengyou Jia; Minnan Luo; Zhuohang Dang; Qiushi Sun; Fangzhi Xu; Junlin Hu; Tianbao Xie; Zhiyong Wu | 2024-10-24 | <details><summary>Click to expand</summary>Digital agents capable of automating complex computer tasks have attracted considerable attention due to their immense potential to enhance human-computer interaction. However, existing agent methods exhibit deficiencies in their generalization and specialization capabilities, especially in handling open-ended computer tasks in real-world environments. Inspired by the rich functionality of the App store, we present AgentStore, a scalable platform designed to dynamically integrate heterogeneous agents for automating computer tasks. AgentStore empowers users to integrate third-party agents, allowing the system to continuously enrich its capabilities and adapt to rapidly evolving operating systems. Additionally, we propose a novel core \textbf{MetaAgent} with the \textbf{AgentToken} strategy to efficiently manage diverse agents and utilize their specialized and generalist abilities for both domain-specific and system-wide tasks. Extensive experiments on three challenging benchmarks demonstrate that AgentStore surpasses the limitations of previous systems with narrow capabilities, particularly achieving a significant improvement from 11.21\% to 23.85\% on the OSWorld benchmark, more than doubling the previous results. Comprehensive quantitative and qualitative results further demonstrate AgentStore's ability to enhance agent systems in both generalization and specialization, underscoring its potential for developing the specialized generalist computer assistant. All our codes will be made publicly available in https://chengyou-jia.github.io/AgentStore-Home.</details> | http://arxiv.org/abs/2410.18603v1 |
| On Model-Free Re-ranking for Visual Place Recognition with Deep Learned   Local Features | Tomáš Pivoňka; Libor Přeučil | 2024-10-24 | <details><summary>Click to expand</summary>Re-ranking is the second stage of a visual place recognition task, in which the system chooses the best-matching images from a pre-selected subset of candidates. Model-free approaches compute the image pair similarity based on a spatial comparison of corresponding local visual features, eliminating the need for computationally expensive estimation of a model describing transformation between images. The article focuses on model-free re-ranking based on standard local visual features and their applicability in long-term autonomy systems. It introduces three new model-free re-ranking methods that were designed primarily for deep-learned local visual features. These features evince high robustness to various appearance changes, which stands as a crucial property for use with long-term autonomy systems. All the introduced methods were employed in a new visual place recognition system together with the D2-net feature detector (Dusmanu, 2019) and experimentally tested with diverse, challenging public datasets. The obtained results are on par with current state-of-the-art methods, affirming that model-free approaches are a viable and worthwhile path for long-term visual place recognition.</details> | http://arxiv.org/abs/2410.18573v2 |
| Zero-shot Object Navigation with Vision-Language Models Reasoning | Congcong Wen; Yisiyuan Huang; Hao Huang; Yanjia Huang; Shuaihang Yuan; Yu Hao; Hui Lin; Yu-Shen Liu; Yi Fang | 2024-10-24 | <details><summary>Click to expand</summary>Object navigation is crucial for robots, but traditional methods require substantial training data and cannot be generalized to unknown environments. Zero-shot object navigation (ZSON) aims to address this challenge, allowing robots to interact with unknown objects without specific training data. Language-driven zero-shot object navigation (L-ZSON) is an extension of ZSON that incorporates natural language instructions to guide robot navigation and interaction with objects. In this paper, we propose a novel Vision Language model with a Tree-of-thought Network (VLTNet) for L-ZSON. VLTNet comprises four main modules: vision language model understanding, semantic mapping, tree-of-thought reasoning and exploration, and goal identification. Among these modules, Tree-of-Thought (ToT) reasoning and exploration module serves as a core component, innovatively using the ToT reasoning framework for navigation frontier selection during robot exploration. Compared to conventional frontier selection without reasoning, navigation using ToT reasoning involves multi-path reasoning processes and backtracking when necessary, enabling globally informed decision-making with higher accuracy. Experimental results on PASTURE and RoboTHOR benchmarks demonstrate the outstanding performance of our model in LZSON, particularly in scenarios involving complex natural language as target instructions.</details> | http://arxiv.org/abs/2410.18570v1 |
| Reinforcement Learning Controllers for Soft Robots using Learned   Environments | Uljad Berdica; Matthew Jackson; Niccolò Enrico Veronese; Jakob Foerster; Perla Maiolino | 2024-10-24 | <details><summary>Click to expand</summary>Soft robotic manipulators offer operational advantage due to their compliant and deformable structures. However, their inherently nonlinear dynamics presents substantial challenges. Traditional analytical methods often depend on simplifying assumptions, while learning-based techniques can be computationally demanding and limit the control policies to existing data. This paper introduces a novel approach to soft robotic control, leveraging state-of-the-art policy gradient methods within parallelizable synthetic environments learned from data. We also propose a safety oriented actuation space exploration protocol via cascaded updates and weighted randomness. Specifically, our recurrent forward dynamics model is learned by generating a training dataset from a physically safe \textit{mean reverting} random walk in actuation space to explore the partially-observed state-space. We demonstrate a reinforcement learning approach towards closed-loop control through state-of-the-art actor-critic methods, which efficiently learn high-performance behaviour over long horizons. This approach removes the need for any knowledge regarding the robot's operation or capabilities and sets the stage for a comprehensive benchmarking tool in soft robotics control.</details> | http://arxiv.org/abs/2410.18519v2 |
| Ubiquitous Field Transportation Robots with Robust Wheel-Leg   Transformable Modules | Haoran Wang; Cunxi Dai; Siyuan Wang; Ximan Zhang; Zheng Zhu; Xiaohan Liu; Jianxiang Zhou; Zhengtao Liu; Zhenzhong Jia | 2024-10-24 | <details><summary>Click to expand</summary>This paper introduces two field transportation robots. Both robots are equipped with transformable wheel-leg modules, which can smoothly switch between operation modes and can work in various challenging terrains. SWhegPro, with six S-shaped legs, enables transporting loads in challenging uneven outdoor terrains. SWhegPro3, featuring four three-impeller wheels, has surprising stair-climbing performance in indoor scenarios. Different from ordinary gear-driven transformable mechanisms, the modular wheels we designed driven by self-locking electric push rods can switch modes accurately and stably with high loads, significantly improving the load capacity of the robot in leg mode. This study analyzes the robot's wheel-leg module operation when the terrain parameters change. Through the derivation of mathematical models and calculations based on simplified kinematic models, a method for optimizing the robot parameters and wheel-leg structure parameters is finally proposed.The design and control strategy are then verified through simulations and field experiments in various complex terrains, and the working performance of the two field transportation robots is calculated and analyzed by recording sensor data and proposing evaluation methods.</details> | http://arxiv.org/abs/2410.18507v1 |
| Multi-UAV Behavior-based Formation with Static and Dynamic Obstacles   Avoidance via Reinforcement Learning | Yuqing Xie; Chao Yu; Hongzhi Zang; Feng Gao; Wenhao Tang; Jingyi Huang; Jiayu Chen; Botian Xu; Yi Wu; Yu Wang | 2024-10-24 | <details><summary>Click to expand</summary>Formation control of multiple Unmanned Aerial Vehicles (UAVs) is vital for practical applications. This paper tackles the task of behavior-based UAV formation while avoiding static and dynamic obstacles during directed flight. We present a two-stage reinforcement learning (RL) training pipeline to tackle the challenge of multi-objective optimization, large exploration spaces, and the sim-to-real gap. The first stage searches in a simplified scenario for a linear utility function that balances all task objectives simultaneously, whereas the second stage applies the utility function in complex scenarios, utilizing curriculum learning to navigate large exploration spaces. Additionally, we apply an attention-based observation encoder to enhance formation maintenance and manage varying obstacle quantity. Experiments in simulation and real world demonstrate that our method outperforms planning-based and RL-based baselines regarding collision-free rate and formation maintenance in scenarios with static, dynamic, and mixed obstacles.</details> | http://arxiv.org/abs/2410.18495v1 |
| LLM as a code generator in Agile Model Driven Development | Ahmed R. Sadik; Sebastian Brulin; Markus Olhofer; Antonello Ceravola; Frank Joublin | 2024-10-24 | <details><summary>Click to expand</summary>Leveraging Large Language Models (LLM) like GPT4 in the auto generation of code represents a significant advancement, yet it is not without its challenges. The ambiguity inherent in natural language descriptions of software poses substantial obstacles to generating deployable, structured artifacts. This research champions Model Driven Development (MDD) as a viable strategy to overcome these challenges, proposing an Agile Model Driven Development (AMDD) approach that employs GPT4 as a code generator. This approach enhances the flexibility and scalability of the code auto generation process and offers agility that allows seamless adaptation to changes in models or deployment environments. We illustrate this by modeling a multi agent Unmanned Vehicle Fleet (UVF) system using the Unified Modeling Language (UML), significantly reducing model ambiguity by integrating the Object Constraint Language (OCL) for code structure meta modeling, and the FIPA ontology language for communication semantics meta modeling. Applying GPT4 auto generation capabilities yields Java and Python code that is compatible with the JADE and PADE frameworks, respectively. Our thorough evaluation of the auto generated code verifies its alignment with expected behaviors and identifies enhancements in agent interactions. Structurally, we assessed the complexity of code derived from a model constrained solely by OCL meta models, against that influenced by both OCL and FIPA ontology meta models. The results indicate that the ontology constrained meta model produces inherently more complex code, yet its cyclomatic complexity remains within manageable levels, suggesting that additional meta model constraints can be incorporated without exceeding the high risk threshold for complexity.</details> | http://arxiv.org/abs/2410.18489v1 |
| VECTOR: Velocity-Enhanced GRU Neural Network for Real-Time 3D UAV   Trajectory Prediction | Omer Nacar; Mohamed Abdelkader; Lahouari Ghouti; Kahled Gabr; Abdulrahman S. Al-Batati; Anis Koubaa | 2024-10-24 | <details><summary>Click to expand</summary>This paper tackles the challenge of real-time 3D trajectory prediction for UAVs, which is critical for applications such as aerial surveillance and defense. Existing prediction models that rely primarily on position data struggle with accuracy, especially when UAV movements fall outside the position domain used in training. Our research identifies a gap in utilizing velocity estimates, first-order dynamics, to better capture the dynamics and enhance prediction accuracy and generalizability in any position domain. To bridge this gap, we propose a new trajectory prediction method using Gated Recurrent Units (GRUs) within sequence-based neural networks. Unlike traditional methods that rely on RNNs or transformers, this approach forecasts future velocities and positions based on historical velocity data instead of positions. This is designed to enhance prediction accuracy and scalability, overcoming challenges faced by conventional models in handling complex UAV dynamics. The methodology employs both synthetic and real-world 3D UAV trajectory data, capturing a wide range of flight patterns, speeds, and agility. Synthetic data is generated using the Gazebo simulator and PX4 Autopilot, while real-world data comes from the UZH-FPV and Mid-Air drone racing datasets. The GRU-based models significantly outperform state-of-the-art RNN approaches, with a mean square error (MSE) as low as 2 x 10^-8. Overall, our findings confirm the effectiveness of incorporating velocity data in improving the accuracy of UAV trajectory predictions across both synthetic and real-world scenarios, in and out of position data distributions. Finally, we open-source our 5000 trajectories dataset and a ROS 2 package to facilitate the integration with existing ROS-based UAV systems.</details> | http://arxiv.org/abs/2410.23305v1 |
| Learn 2 Rage: Experiencing The Emotional Roller Coaster That Is   Reinforcement Learning | Lachlan Mares; Stefan Podgorski; Ian Reid | 2024-10-24 | <details><summary>Click to expand</summary>This work presents the experiments and solution outline for our teams winning submission in the Learn To Race Autonomous Racing Virtual Challenge 2022 hosted by AIcrowd. The objective of the Learn-to-Race competition is to push the boundary of autonomous technology, with a focus on achieving the safety benefits of autonomous driving. In the description the competition is framed as a reinforcement learning (RL) challenge. We focused our initial efforts on implementation of Soft Actor Critic (SAC) variants. Our goal was to learn non-trivial control of the race car exclusively from visual and geometric features, directly mapping pixels to control actions. We made suitable modifications to the default reward policy aiming to promote smooth steering and acceleration control. The framework for the competition provided real time simulation, meaning a single episode (learning experience) is measured in minutes. Instead of pursuing parallelisation of episodes we opted to explore a more traditional approach in which the visual perception was processed (via learned operators) and fed into rule-based controllers. Such a system, while not as academically "attractive" as a pixels-to-actions approach, results in a system that requires less training, is more explainable, generalises better and is easily tuned and ultimately out-performed all other agents in the competition by a large margin.</details> | http://arxiv.org/abs/2410.18462v1 |
| SkiLD: Unsupervised Skill Discovery Guided by Factor Interactions | Zizhao Wang; Jiaheng Hu; Caleb Chuck; Stephen Chen; Roberto Martín-Martín; Amy Zhang; Scott Niekum; Peter Stone | 2024-10-24 | <details><summary>Click to expand</summary>Unsupervised skill discovery carries the promise that an intelligent agent can learn reusable skills through autonomous, reward-free environment interaction. Existing unsupervised skill discovery methods learn skills by encouraging distinguishable behaviors that cover diverse states. However, in complex environments with many state factors (e.g., household environments with many objects), learning skills that cover all possible states is impossible, and naively encouraging state diversity often leads to simple skills that are not ideal for solving downstream tasks. This work introduces Skill Discovery from Local Dependencies (Skild), which leverages state factorization as a natural inductive bias to guide the skill learning process. The key intuition guiding Skild is that skills that induce <b>diverse interactions</b> between state factors are often more valuable for solving downstream tasks. To this end, Skild develops a novel skill learning objective that explicitly encourages the mastering of skills that effectively induce different interactions within an environment. We evaluate Skild in several domains with challenging, long-horizon sparse reward tasks including a realistic simulated household robot domain, where Skild successfully learns skills with clear semantic meaning and shows superior performance compared to existing unsupervised reinforcement learning methods that only maximize state coverage.</details> | http://arxiv.org/abs/2410.18416v1 |
| Sensing-Communication-Computing-Control Closed-Loop Optimization for 6G   Unmanned Robotic Systems | Xinran Fang; Chengleyang Lei; Wei Feng; Yunfei Chen; Ming Xiao; Ning Ge; Chengxiang Wang | 2024-10-24 | <details><summary>Click to expand</summary>Rapid advancements in field robots have brought a new kind of cyber physical system (CPS)--unmanned robotic system--under the spotlight. In the upcoming sixth-generation (6G) era, these systems hold great potential to replace humans in hazardous tasks. This paper investigates an unmanned robotic system comprising a multi-functional unmanned aerial vehicle (UAV), sensors, and actuators. The UAV carries communication and computing modules, acting as an edge information hub (EIH) that transfers and processes information. During the task execution, the EIH gathers sensing data, calculates control commands, and transmits commands to actuators--leading to reflex-arc-like sensing-communication-computing-control ($\mathbf{SC}^3$) loops. Unlike existing studies that design $\mathbf{SC}^3$ loop components separately, we take each $\mathbf{SC}^3$ loop as an integrated structure and propose a goal-oriented closed-loop optimization scheme. This scheme jointly optimizes uplink and downlink (UL&DL) communication and computing within and across the $\mathbf{SC}^3$ loops to minimize the total linear quadratic regulator (LQR) cost. We derive optimal closed-form solutions for intra-loop allocation and propose an efficient iterative algorithm for inter-loop optimization. Under the condition of adequate CPU frequency availability, we derive an approximate closed-form solution for inter-loop bandwidth allocation. Simulation results demonstrate that the proposed scheme achieves a two-tier task-level balance within and across $\mathbf{SC}^3$ loops.</details> | http://arxiv.org/abs/2410.18382v1 |
| UGotMe: An Embodied System for Affective Human-Robot Interaction | Peizhen Li; Longbing Cao; Xiao-Ming Wu; Xiaohan Yu; Runze Yang | 2024-10-24 | <details><summary>Click to expand</summary>Equipping humanoid robots with the capability to understand emotional states of human interactants and express emotions appropriately according to situations is essential for affective human-robot interaction. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real-world raises embodiment challenges: addressing the environmental noise issue and meeting real-time requirements. First, in multiparty conversation scenarios, the noises inherited in the visual observation of the robot, which may come from either 1) distracting objects in the scene or 2) inactive speakers appearing in the field of view of the robot, hinder the models from extracting emotional cues from vision inputs. Secondly, realtime response, a desired feature for an interactive system, is also challenging to achieve. To tackle both challenges, we introduce an affective human-robot interaction system called UGotMe designed specifically for multiparty conversations. Two denoising strategies are proposed and incorporated into the system to solve the first issue. Specifically, to filter out distracting objects in the scene, we propose extracting face images of the speakers from the raw images and introduce a customized active face extraction strategy to rule out inactive speakers. As for the second issue, we employ efficient data transmission from the robot to the local server to improve realtime response capability. We deploy UGotMe on a human robot named Ameca to validate its real-time inference capabilities in practical scenarios. Videos demonstrating real-world deployment are available at https://pi3-141592653.github.io/UGotMe/.</details> | http://arxiv.org/abs/2410.18373v1 |
| Thermal Chameleon: Task-Adaptive Tone-mapping for Radiometric   Thermal-Infrared images | Dong-Guw Lee; Jeongyun Kim; Younggun Cho; Ayoung Kim | 2024-10-24 | <details><summary>Click to expand</summary>Thermal Infrared (TIR) imaging provides robust perception for navigating in challenging outdoor environments but faces issues with poor texture and low image contrast due to its 14/16-bit format. Conventional methods utilize various tone-mapping methods to enhance contrast and photometric consistency of TIR images, however, the choice of tone-mapping is largely dependent on knowing the task and temperature dependent priors to work well. In this paper, we present Thermal Chameleon Network (TCNet), a task-adaptive tone-mapping approach for RAW 14-bit TIR images. Given the same image, TCNet tone-maps different representations of TIR images tailored for each specific task, eliminating the heuristic image rescaling preprocessing and reliance on the extensive prior knowledge of the scene temperature or task-specific characteristics. TCNet exhibits improved generalization performance across object detection and monocular depth estimation, with minimal computational overhead and modular integration to existing architectures for various tasks. Project Page: https://github.com/donkeymouse/ThermalChameleon</details> | http://arxiv.org/abs/2410.18340v1 |
| Search-Based Path Planning among Movable Obstacles | Zhongqiang Ren; Bunyod Suvonov; Guofei Chen; Botao He; Yijie Liao; Cornelia Fermuller; Ji Zhang | 2024-10-24 | <details><summary>Click to expand</summary>This paper investigates Path planning Among Movable Obstacles (PAMO), which seeks a minimum cost collision-free path among static obstacles from start to goal while allowing the robot to push away movable obstacles (i.e., objects) along its path when needed. To develop planners that are complete and optimal for PAMO, the planner has to search a giant state space involving both the location of the robot as well as the locations of the objects, which grows exponentially with respect to the number of objects. The main idea in this paper is that, only a small fraction of this giant state space needs to be explored during planning as guided by a heuristic, and most of the objects far away from the robot are intact, which thus leads to runtime efficient algorithms. Based on this idea, this paper introduces two PAMO formulations, i.e., bi-objective and resource constrained problems in an occupancy grid, and develops PAMO*, a search method with completeness and solution optimality guarantees, to solve the two problems. We then further extend PAMO* to hybrid-state PAMO* to plan in continuous spaces with high-fidelity interaction between the robot and the objects. Our results show that, PAMO* can often find optimal solutions within a second in cluttered environments with up to 400 objects.</details> | http://arxiv.org/abs/2410.18333v1 |
| Kenyan Sign Language (KSL) Dataset: Using Artificial Intelligence (AI)   in Bridging Communication Barrier among the Deaf Learners | Lilian Wanzare; Joel Okutoyi; Maurine Kang'ahi; Mildred Ayere | 2024-10-23 | <details><summary>Click to expand</summary>Kenyan Sign Language (KSL) is the primary language used by the deaf community in Kenya. It is the medium of instruction from Pre-primary 1 to university among deaf learners, facilitating their education and academic achievement. Kenyan Sign Language is used for social interaction, expression of needs, making requests and general communication among persons who are deaf in Kenya. However, there exists a language barrier between the deaf and the hearing people in Kenya. Thus, the innovation on AI4KSL is key in eliminating the communication barrier. Artificial intelligence for KSL is a two-year research project (2023-2024) that aims to create a digital open-access AI of spontaneous and elicited data from a representative sample of the Kenyan deaf community. The purpose of this study is to develop AI assistive technology dataset that translates English to KSL as a way of fostering inclusion and bridging language barriers among deaf learners in Kenya. Specific objectives are: Build KSL dataset for spoken English and video recorded Kenyan Sign Language and to build transcriptions of the KSL signs to a phonetic-level interface of the sign language. In this paper, the methodology for building the dataset is described. Data was collected from 48 teachers and tutors of the deaf learners and 400 learners who are Deaf. Participants engaged mainly in sign language elicitation tasks through reading and singing. Findings of the dataset consisted of about 14,000 English sentences with corresponding KSL Gloss derived from a pool of about 4000 words and about 20,000 signed KSL videos that are either signed words or sentences. The second level of data outcomes consisted of 10,000 split and segmented KSL videos. The third outcome of the dataset consists of 4,000 transcribed words into five articulatory parameters according to HamNoSys system.</details> | http://arxiv.org/abs/2410.18295v1 |
| Screw Geometry Meets Bandits: Incremental Acquisition of Demonstrations   to Generate Manipulation Plans | Dibyendu Das; Aditya Patankar; Nilanjan Chakraborty; C. R. Ramakrishnan; I. V. Ramakrishnan | 2024-10-23 | <details><summary>Click to expand</summary>In this paper, we study the problem of methodically obtaining a sufficient set of kinesthetic demonstrations, one at a time, such that a robot can be confident of its ability to perform a complex manipulation task in a given region of its workspace. Although Learning from Demonstrations has been an active area of research, the problems of checking whether a set of demonstrations is sufficient, and systematically seeking additional demonstrations have remained open. We present a novel approach to address these open problems using (i) a screw geometric representation to generate manipulation plans from demonstrations, which makes the sufficiency of a set of demonstrations measurable; (ii) a sampling strategy based on PAC-learning from multi-armed bandit optimization to evaluate the robot's ability to generate manipulation plans in a subregion of its task space; and (iii) a heuristic to seek additional demonstration from areas of weakness. Thus, we present an approach for the robot to incrementally and actively ask for new demonstration examples until the robot can assess with high confidence that it can perform the task successfully. We present experimental results on two example manipulation tasks, namely, pouring and scooping, to illustrate our approach. A short video on the method: https://youtu.be/R-qICICdEos</details> | http://arxiv.org/abs/2410.18275v1 |
| CARLA2Real: a tool for reducing the sim2real gap in CARLA simulator | Stefanos Pasios; Nikos Nikolaidis | 2024-10-23 | <details><summary>Click to expand</summary>Simulators are indispensable for research in autonomous systems such as self-driving cars, autonomous robots and drones. Despite significant progress in various simulation aspects, such as graphical realism, an evident gap persists between the virtual and real-world environments. Since the ultimate goal is to deploy the autonomous systems in the real world, closing the sim2real gap is of utmost importance. In this paper, we employ a state-of-the-art approach to enhance the photorealism of simulated data, aligning them with the visual characteristics of real-world datasets. Based on this, we developed CARLA2Real, an easy-to-use, publicly available tool (plug-in) for the widely used and open-source CARLA simulator. This tool enhances the output of CARLA in near real-time, achieving a frame rate of 13 FPS, translating it to the visual style and realism of real-world datasets such as Cityscapes, KITTI, and Mapillary Vistas. By employing the proposed tool, we generated synthetic datasets from both the simulator and the enhancement model outputs, including their corresponding ground truth annotations for tasks related to autonomous driving. Then, we performed a number of experiments to evaluate the impact of the proposed approach on feature extraction and semantic segmentation methods when trained on the enhanced synthetic data. The results demonstrate that the sim2real gap is significant and can indeed be reduced by the introduced approach.</details> | http://arxiv.org/abs/2410.18238v2 |
| Bayesian optimization for robust robotic grasping using a sensorized   compliant hand | Juan G. Lechuz-Sierra; Ana Elvira H. Martin; Ashok M. Sundaram; Ruben Martinez-Cantin; Máximo A. Roa | 2024-10-23 | <details><summary>Click to expand</summary>One of the first tasks we learn as children is to grasp objects based on our tactile perception. Incorporating such skill in robots will enable multiple applications, such as increasing flexibility in industrial processes or providing assistance to people with physical disabilities. However, the difficulty lies in adapting the grasping strategies to a large variety of tasks and objects, which can often be unknown. The brute-force solution is to learn new grasps by trial and error, which is inefficient and ineffective. In contrast, Bayesian optimization applies active learning by adding information to the approximation of an optimal grasp. This paper proposes the use of Bayesian optimization techniques to safely perform robotic grasping. We analyze different grasp metrics to provide realistic grasp optimization in a real system including tactile sensors. An experimental evaluation in the robotic system shows the usefulness of the method for performing unknown object grasping even in the presence of noise and uncertainty inherent to a real-world environment.</details> | http://arxiv.org/abs/2410.18237v1 |
| Personalized Instance-based Navigation Toward User-Specific Objects in   Realistic Environments | Luca Barsellotti; Roberto Bigazzi; Marcella Cornia; Lorenzo Baraldi; Rita Cucchiara | 2024-10-23 | <details><summary>Click to expand</summary>In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.</details> | http://arxiv.org/abs/2410.18195v1 |
| DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes | Hengwei Bian; Lingdong Kong; Haozhe Xie; Liang Pan; Yu Qiao; Ziwei Liu | 2024-10-23 | <details><summary>Click to expand</summary>LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D LiDAR generation framework capable of generating large-scale, high-quality LiDAR scenes that capture the temporal evolution of dynamic environments. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research.</details> | http://arxiv.org/abs/2410.18084v1 |
| WorldSimBench: Towards Video Generation Models as World Simulators | Yiran Qin; Zhelun Shi; Jiwen Yu; Xijun Wang; Enshen Zhou; Lijun Li; Zhenfei Yin; Xihui Liu; Lu Sheng; Jing Shao; Lei Bai; Wanli Ouyang; Ruimao Zhang | 2024-10-23 | <details><summary>Click to expand</summary>Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulators. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.</details> | http://arxiv.org/abs/2410.18072v1 |
| SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for   Long-Horizon Manipulation | Zihan Zhou; Animesh Garg; Dieter Fox; Caelan Garrett; Ajay Mandlekar | 2024-10-23 | <details><summary>Click to expand</summary>Robot learning has proven to be a general and effective technique for programming manipulators. Imitation learning is able to teach robots solely from human demonstrations but is bottlenecked by the capabilities of the demonstrations. Reinforcement learning uses exploration to discover better behaviors; however, the space of possible improvements can be too large to start from scratch. And for both techniques, the learning difficulty increases proportional to the length of the manipulation task. Accounting for this, we propose SPIRE, a system that first uses Task and Motion Planning (TAMP) to decompose tasks into smaller learning subproblems and second combines imitation and reinforcement learning to maximize their strengths. We develop novel strategies to train learning agents when deployed in the context of a planning system. We evaluate SPIRE on a suite of long-horizon and contact-rich robot manipulation problems. We find that SPIRE outperforms prior approaches that integrate imitation learning, reinforcement learning, and planning by 35% to 50% in average task performance, is 6 times more data efficient in the number of human demonstrations needed to train proficient agents, and learns to complete tasks nearly twice as efficiently. View https://sites.google.com/view/spire-corl-2024 for more details.</details> | http://arxiv.org/abs/2410.18065v1 |
| Selective excitation of work-generating cycles in nonreciprocal living   solids | Yu-Chen Chao; Shreyas Gokhale; Lisa Lin; Alasdair Hastewell; Alexandru Bacanu; Yuchao Chen; Junang Li; Jinghui Liu; Hyunseok Lee; Jorn Dunkel; Nikta Fakhri | 2024-10-23 | <details><summary>Click to expand</summary>Emergent nonreciprocity in active matter drives the formation of self-organized states that transcend the behaviors of equilibrium systems. Integrating experiments, theory and simulations, we demonstrate that active solids composed of living starfish embryos spontaneously transition between stable fluctuating and oscillatory steady states. The nonequilibrium steady states arise from two distinct chiral symmetry breaking mechanisms at the microscopic scale: the spinning of individual embryos resulting in a macroscopic odd elastic response, and the precession of their rotation axis, leading to active gyroelasticity. In the oscillatory state, we observe long-wavelength optical vibrational modes that can be excited through mechanical perturbations. Strikingly, these excitable nonreciprocal solids exhibit nonequilibrium work generation without cycling protocols, due to coupled vibrational modes. Our work introduces a novel class of tunable nonequilibrium processes, offering a framework for designing and controlling soft robotic swarms and adaptive active materials, while opening new possibilities for harnessing nonreciprocal interactions in engineered systems.</details> | http://arxiv.org/abs/2410.18017v1 |
| A Pipeline for Segmenting and Structuring RGB-D Data for Robotics   Applications | Zhiwu Zheng; Lauren Mentzer; Berk Iskender; Michael Price; Colm Prendergast; Audren Cloitre | 2024-10-23 | <details><summary>Click to expand</summary>We introduce a novel pipeline for segmenting and structuring color and depth (RGB-D) data. Existing processing pipelines for RGB-D data have focused on extracting geometric information alone. This approach precludes the development of more advanced robotic navigation and manipulation algorithms, which benefit from a semantic understanding of their environment. Our pipeline can segment RGB-D data into accurate semantic masks. These masks are then used to fuse raw captured point clouds into semantically separated point clouds. We store this information using the Universal Scene Description (USD) file format, a format suitable for easy querying by downstream robotics algorithms, human-friendly visualization, and robotics simulation.</details> | http://arxiv.org/abs/2410.17988v1 |
| Robust Two-View Geometry Estimation with Implicit Differentiation | Vladislav Pyatov; Iaroslav Koshelev; Stamatis Lefkimmiatis | 2024-10-23 | <details><summary>Click to expand</summary>We present a novel two-view geometry estimation framework which is based on a differentiable robust loss function fitting. We propose to treat the robust fundamental matrix estimation as an implicit layer, which allows us to avoid backpropagation through time and significantly improves the numerical stability. To take full advantage of the information from the feature matching stage we incorporate learnable weights that depend on the matching confidences. In this way our solution brings together feature extraction, matching and two-view geometry estimation in a unified end-to-end trainable pipeline. We evaluate our approach on the camera pose estimation task in both outdoor and indoor scenarios. The experiments on several datasets show that the proposed method outperforms both classic and learning-based state-of-the-art methods by a large margin. The project webpage is available at: https://github.com/VladPyatov/ihls</details> | http://arxiv.org/abs/2410.17983v1 |
| Reconfigurable Hydrostatics: Toward Multifunctional and Powerful   Wearable Robotics | Jeff Denis; Frederic Laberge; Jean-Sebastien Plante; Alexandre Girard | 2024-10-23 | <details><summary>Click to expand</summary>Wearable and locomotive robot designers face multiple challenges when choosing actuation. Traditional fully actuated designs using electric motors are multifunctional but oversized and inefficient for bearing conservative loads and for being backdrivable. Alternatively, quasi-passive and underactuated designs reduce the size of motorization and energy storage, but are often designed for specific tasks. Designers of versatile and stronger wearable robots will face these challenges unless future actuators become very torque-dense, backdrivable and efficient.   This paper explores a design paradigm for addressing this issue: reconfigurable hydrostatics. We show that a hydrostatic actuator can integrate a passive force mechanism and a sharing mechanism in the fluid domain and still be multifunctional. First, an analytical study compares how these two mechanisms can relax the motorization requirements in the context of a load-bearing exoskeleton. Then, the hydrostatic concept integrating these two mechanisms using hydraulic components is presented. A case study analysis shows the mass/efficiency/inertia benefits of the concept over a fully actuated one. Then, the feasibility of the concept is partially validated with a proof-of-concept that actuates the knees of an exoskeleton. The experiments show that it can track the vertical ground reaction force (GRF) profiles of walking, running, squatting, and jumping, and that the energy consumption is 6x lower. The transient force behaviors due to switching from one leg to the other are also analyzed along with some mitigation to improve them.</details> | http://arxiv.org/abs/2410.17936v1 |
| Gaussian Process Distance Fields Obstacle and Ground Constraints for   Safe Navigation | Monisha Mushtary Uttsha; Cedric Le Gentil; Lan Wu; Teresa Vidal-Calleja | 2024-10-23 | <details><summary>Click to expand</summary>Navigating cluttered environments is a challenging task for any mobile system. Existing approaches for ground-based mobile systems primarily focus on small wheeled robots, which face minimal constraints with overhanging obstacles and cannot manage steps or stairs, making the problem effectively 2D. However, navigation for legged robots (or even humans) has to consider an extra dimension. This paper proposes a tailored scene representation coupled with an advanced trajectory optimisation algorithm to enable safe navigation. Our 3D navigation approach is suitable for any ground-based mobile robot, whether wheeled or legged, as well as for human assistance. Given a 3D point cloud of the scene and the segmentation of the ground and non-ground points, we formulate two Gaussian Process distance fields to ensure a collision-free path and maintain distance to the ground constraints. Our method adeptly handles uneven terrain, steps, and overhanging objects through an innovative use of a quadtree structure, constructing a multi-resolution map of the free space and its connectivity graph based on a 2D projection of the relevant scene. Evaluations with both synthetic and real-world datasets demonstrate that this approach provides safe and smooth paths, accommodating a wide range of ground-based mobile systems.</details> | http://arxiv.org/abs/2410.17831v1 |
| Optimal Fault-Tolerant Dispersion on Oriented Grids | Rik Banerjee; Manish Kumar; Anisur Rahaman Molla | 2024-10-23 | <details><summary>Click to expand</summary>Dispersion of mobile robots over the nodes of an anonymous graph is an important problem and turns out to be a crucial subroutine for designing efficient algorithms for many fundamental graph problems via mobile robots. In this problem, starting from an arbitrary initial distribution of $n$ robots across the $n$ nodes, the goal is to achieve a final configuration where each node holds at most one robot. This paper investigates the dispersion problem on an oriented grid, considering the possibility of robot failures (crashes) at any time during the algorithm's execution. We present a crash-tolerant dispersion algorithm that solves the dispersion problem on an anonymous oriented grid in $O(\sqrt{n})$ time and using $O(\log n)$ bits of memory per robot. The algorithm is optimal in terms of both time and memory per robot. We further extend this algorithm to deal with weak Byzantine robots. The weak Byzantine fault dispersion algorithm takes optimal $O(\sqrt{n})$ rounds but requires $O(n\log n)$ bits of memory per robot.</details> | http://arxiv.org/abs/2410.17813v2 |
| Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition   Accelerator with Inter-layer and Intra-layer Optimizations | Qijun Zhang; Zhiyao Xie | 2024-10-23 | <details><summary>Click to expand</summary>Point cloud is an important data structure for a wide range of applications, including robotics, AR/VR, and autonomous driving. To process the point cloud, many deep-learning-based point cloud recognition algorithms have been proposed. However, to meet the requirement of applications like autonomous driving, the algorithm must be fast enough, rendering accelerators necessary at the inference stage. But existing point cloud accelerators are still inefficient due to two challenges. First, the multi-layer perceptron (MLP) during feature computation is the performance bottleneck. Second, the feature vector fetching operation incurs heavy DRAM access.   In this paper, we propose Pointer, an efficient Resistive Random Access Memory (ReRAM)-based point cloud recognition accelerator with inter- and intra-layer optimizations. It proposes three techniques for point cloud acceleration. First, Pointer adopts ReRAM-based architecture to significantly accelerate the MLP in feature computation. Second, to reduce DRAM access, Pointer proposes inter-layer coordination. It schedules the next layer to fetch the results of the previous layer as soon as they are available, which allows on-chip fetching thus reduces DRAM access. Third, Pointer proposes topology-aware intra-layer reordering, which improves the execution order for better data locality. Pointer proves to achieve 40x to 393x speedup and 22x to 163x energy efficiency over prior accelerators without any accuracy loss.</details> | http://arxiv.org/abs/2410.17782v1 |
| Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation   Models | Nils Blank; Moritz Reuss; Marcel Rühle; Ömer Erdinç Yağmurlu; Fabian Wenzel; Oier Mees; Rudolf Lioutikov | 2024-10-23 | <details><summary>Click to expand</summary>A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long-horizon robot data at scale in a zero-shot manner without any human intervention. NILS combines pretrained vision-language foundation models in order to detect objects in a scene, detect object-centric changes, segment tasks from large datasets of unlabelled interaction data and ultimately label behavior datasets. Evaluations on BridgeV2, Fractal, and a kitchen play dataset show that NILS can autonomously annotate diverse robot demonstrations of unlabeled and unstructured datasets while alleviating several shortcomings of crowdsourced human annotations, such as low data quality and diversity. We use NILS to label over 115k trajectories obtained from over 430 hours of robot data. We open-source our auto-labeling code and generated annotations on our website: http://robottasklabeling.github.io.</details> | http://arxiv.org/abs/2410.17772v2 |
| VISAGE: Video Synthesis using Action Graphs for Surgery | Yousef Yeganeh; Rachmadio Lazuardi; Amir Shamseddin; Emine Dari; Yash Thirani; Nassir Navab; Azade Farshad | 2024-10-23 | <details><summary>Click to expand</summary>Surgical data science (SDS) is a field that analyzes patient data before, during, and after surgery to improve surgical outcomes and skills. However, surgical data is scarce, heterogeneous, and complex, which limits the applicability of existing machine learning methods. In this work, we introduce the novel task of future video generation in laparoscopic surgery. This task can augment and enrich the existing surgical data and enable various applications, such as simulation, analysis, and robot-aided surgery. Ultimately, it involves not only understanding the current state of the operation but also accurately predicting the dynamic and often unpredictable nature of surgical procedures. Our proposed method, VISAGE (VIdeo Synthesis using Action Graphs for Surgery), leverages the power of action scene graphs to capture the sequential nature of laparoscopic procedures and utilizes diffusion models to synthesize temporally coherent video sequences. VISAGE predicts the future frames given only a single initial frame, and the action graph triplets. By incorporating domain-specific knowledge through the action graph, VISAGE ensures the generated videos adhere to the expected visual and motion patterns observed in real laparoscopic procedures. The results of our experiments demonstrate high-fidelity video generation for laparoscopy procedures, which enables various applications in SDS.</details> | http://arxiv.org/abs/2410.17751v2 |
| Multi-Layered Safety of Redundant Robot Manipulators via Task-Oriented   Planning and Control | Xinyu Jia; Wenxin Wang; Jun Yang; Yongping Pan; Haoyong Yu | 2024-10-23 | <details><summary>Click to expand</summary>Ensuring safety is crucial to promote the application of robot manipulators in open workspace. Factors such as sensor errors or unpredictable collisions make the environment full of uncertainties. In this work, we investigate these potential safety challenges on redundant robot manipulators, and propose a task-oriented planning and control framework to achieve multi-layered safety while maintaining efficient task execution. Our approach consists of two main parts: a task-oriented trajectory planner based on multiple-shooting model predictive control method, and a torque controller that allows safe and efficient collision reaction using only proprioceptive data. Through extensive simulations and real-hardware experiments, we demonstrate that the proposed framework can effectively handle uncertain static or dynamic obstacles, and perform disturbance resistance in manipulation tasks when unforeseen contacts occur. All code will be open-sourced to benefit the community.</details> | http://arxiv.org/abs/2410.17742v1 |
| Towards Safer Planetary Exploration: A Hybrid Architecture for Terrain   Traversability Analysis in Mars Rovers | Achille Chiuchiarelli; Giacomo Franchini; Francesco Messina; Marcello Chiaberge | 2024-10-23 | <details><summary>Click to expand</summary>The field of autonomous navigation for unmanned ground vehicles (UGVs) is in continuous growth and increasing levels of autonomy have been reached in the last few years. However, the task becomes more challenging when the focus is on the exploration of planet surfaces such as Mars. In those situations, UGVs are forced to navigate through unstable and rugged terrains which, inevitably, open the vehicle to more hazards, accidents, and, in extreme cases, complete mission failure. The paper addresses the challenges of autonomous navigation for unmanned ground vehicles in planetary exploration, particularly on Mars, introducing a hybrid architecture for terrain traversability analysis that combines two approaches: appearance-based and geometry-based. The appearance-based method uses semantic segmentation via deep neural networks to classify different terrain types. This is further refined by pixel-level terrain roughness classification obtained from the same RGB image, assigning different costs based on the physical properties of the soil. The geometry-based method complements the appearance-based approach by evaluating the terrain's geometrical features, identifying hazards that may not be detectable by the appearance-based side. The outputs of both methods are combined into a comprehensive hybrid cost map. The proposed architecture was trained on synthetic datasets and developed as a ROS2 application to integrate into broader autonomous navigation systems for harsh environments. Simulations have been performed in Unity, showing the ability of the method to assess online traversability analysis.</details> | http://arxiv.org/abs/2410.17738v1 |
| Markov Potential Game with Final-time Reach-Avoid Objectives | Sarah H. Q. Li; Abraham P. Vinod | 2024-10-23 | <details><summary>Click to expand</summary>We formulate a Markov potential game with final-time reach-avoid objectives by integrating potential game theory with stochastic reach-avoid control. Our focus is on multi-player trajectory planning where players maximize the same multi-player reach-avoid objective: the probability of all participants reaching their designated target states by a specified time, while avoiding collisions with one another. Existing approaches require centralized computation of actions via a global policy, which may have prohibitively expensive communication costs. Instead, we focus on approximations of the global policy via local state feedback policies. First, we adapt the recursive single player reach-avoid value iteration to the multi-player framework with local policies, and show that the same recursion holds on the joint state space. To find each player's optimal local policy, the multi-player reach-avoid value function is projected from the joint state to the local state using the other players' occupancy measures. Then, we propose an iterative best response scheme for the multi-player value iteration to converge to a pure Nash equilibrium. We demonstrate the utility of our approach in finding collision-free policies for multi-player motion planning in simulation.</details> | http://arxiv.org/abs/2410.17690v1 |
| Human-Robot Collaboration System Setup for Weed Harvesting Scenarios in   Aquatic Lakes | Ahmed H. Elsayed; Andrej Lejman; Frederic Stahl | 2024-10-23 | <details><summary>Click to expand</summary>Artificial Water Bodies (AWBs) are human-made and require continuous monitoring due to their artificial biological processes. These systems necessitate regular maintenance to manage their ecosystems effectively. Unmanned Surface Vehicle (USV) offers a collaborative approach for monitoring these environments, working alongside human operators such as boat skippers to identify specific locations. This paper discusses a weed harvesting scenario, demonstrating how human-robot collaboration can be achieved, supported by preliminary results. The USV mainly utilises multibeam SOund NAvigation and Ranging (SONAR) for underwater weed monitoring, showing promising outcomes in these scenarios.</details> | http://arxiv.org/abs/2410.17685v1 |
| Surgical Scene Segmentation by Transformer With Asymmetric Feature   Enhancement | Cheng Yuan; Yutong Ban | 2024-10-23 | <details><summary>Click to expand</summary>Surgical scene segmentation is a fundamental task for robotic-assisted laparoscopic surgery understanding. It often contains various anatomical structures and surgical instruments, where similar local textures and fine-grained structures make the segmentation a difficult task. Vision-specific transformer method is a promising way for surgical scene understanding. However, there are still two main challenges. Firstly, the absence of inner-patch information fusion leads to poor segmentation performance. Secondly, the specific characteristics of anatomy and instruments are not specifically modeled. To tackle the above challenges, we propose a novel Transformer-based framework with an Asymmetric Feature Enhancement module (TAFE), which enhances local information and then actively fuses the improved feature pyramid into the embeddings from transformer encoders by a multi-scale interaction attention strategy. The proposed method outperforms the SOTA methods in several different surgical segmentation tasks and additionally proves its ability of fine-grained structure recognition. Code is available at https://github.com/cyuan-sjtu/ViT-asym.</details> | http://arxiv.org/abs/2410.17642v1 |
| Incremental Learning of Affordances using Markov Logic Networks | George Potter; Gertjan Burghouts; Joris Sijs | 2024-10-23 | <details><summary>Click to expand</summary>Affordances enable robots to have a semantic understanding of their surroundings. This allows them to have more acting flexibility when completing a given task. Capturing object affordances in a machine learning model is a difficult task, because of their dependence on contextual information. Markov Logic Networks (MLN) combine probabilistic reasoning with logic that is able to capture such context. Mobile robots operate in partially known environments wherein unseen object affordances can be observed. This new information must be incorporated into the existing knowledge, without having to retrain the MLN from scratch. We introduce the MLN Cumulative Learning Algorithm (MLN-CLA). MLN-CLA learns new relations in various knowledge domains by retaining knowledge and only updating the changed knowledge, for which the MLN is retrained. We show that MLN-CLA is effective for accumulative learning and zero-shot affordance inference, outperforming strong baselines.</details> | http://arxiv.org/abs/2410.17624v1 |
| ImDy: Human Inverse Dynamics from Imitated Observations | Xinpeng Liu; Junxuan Liang; Zili Lin; Haowen Hou; Yong-Lu Li; Cewu Lu | 2024-10-23 | <details><summary>Click to expand</summary>Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive laboratory setups, restricting its availability. To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-driven manner. The key insight is that the human ID knowledge is implicitly possessed by motion imitators, though not directly applicable. In light of this, we devise an efficient data collection pipeline with state-of-the-art motion imitation algorithms and physics simulators, resulting in a large-scale human inverse dynamics benchmark as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint torque and full-body ground reaction force data. With ImDy, we train a data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised manner, which conducts ID and ground reaction force estimation simultaneously. Experiments on ImDy and real-world data demonstrate the impressive competency of ImDyS in human inverse dynamics and ground reaction force estimation. Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is exhibited with downstream applications. The project page is https://foruck.github.io/ImDy/.</details> | http://arxiv.org/abs/2410.17610v1 |
| Integrating Large Language Models for UAV Control in Simulated   Environments: A Modular Interaction Approach | Abhishek Phadke; Alihan Hadimlioglu; Tianxing Chu; Chandra N Sekharan | 2024-10-23 | <details><summary>Click to expand</summary>The intersection of LLMs (Large Language Models) and UAV (Unoccupied Aerial Vehicles) technology represents a promising field of research with the potential to enhance UAV capabilities significantly. This study explores the application of LLMs in UAV control, focusing on the opportunities for integrating advanced natural language processing into autonomous aerial systems. By enabling UAVs to interpret and respond to natural language commands, LLMs simplify the UAV control and usage, making them accessible to a broader user base and facilitating more intuitive human-machine interactions. The paper discusses several key areas where LLMs can impact UAV technology, including autonomous decision-making, dynamic mission planning, enhanced situational awareness, and improved safety protocols. Through a comprehensive review of current developments and potential future directions, this study aims to highlight how LLMs can transform UAV operations, making them more adaptable, responsive, and efficient in complex environments. A template development framework for integrating LLMs in UAV control is also described. Proof of Concept results that integrate existing LLM models and popular robotic simulation platforms are demonstrated. The findings suggest that while there are substantial technical and ethical challenges to address, integrating LLMs into UAV control holds promising implications for advancing autonomous aerial systems.</details> | http://arxiv.org/abs/2410.17602v1 |
| Energy-Optimal Planning of Waypoint-Based UAV Missions -- Does Minimum   Distance Mean Minimum Energy? | Nicolas Michel; Ayush Patnaik; Zhaodan Kong; Xinfan Lin | 2024-10-23 | <details><summary>Click to expand</summary>Multirotor unmanned aerial vehicle is a prevailing type of aerial robots with wide real-world applications. The energy efficiency of the robot is a critical aspect of its performance, determining the range and duration of the missions that can be performed. This paper studies the energy-optimal planning of the multirotor, which aims at finding the optimal ordering of waypoints with the minimum energy consumption for missions in 3D space. The study is performed based on a previously developed model capturing first-principle energy dynamics of the multirotor. We found that in majority of the cases (up to 95%) the solutions of the energy-optimal planning are different from those of the traditional traveling salesman problem which minimizes the total distance. The difference can be as high as 14.9%, with the average at 1.6%-3.3% and 90th percentile at 3.7%-6.5% depending on the range and number of waypoints in the mission. We then identified and explained the key features of the minimum-energy order by correlating to the underlying flight energy dynamics. It is shown that instead of minimizing the distance, coordination of vertical and horizontal motion to promote aerodynamic efficiency is the key to optimizing energy consumption.</details> | http://arxiv.org/abs/2410.17585v1 |
| Real-time Vehicle-to-Vehicle Communication Based Network Cooperative   Control System through Distributed Database and Multimodal Perception:   Demonstrated in Crossroads | Xinwen Zhu; Zihao Li; Yuxuan Jiang; Jiazhen Xu; Jie Wang; Xuyang Bai | 2024-10-23 | <details><summary>Click to expand</summary>The autonomous driving industry is rapidly advancing, with Vehicle-to-Vehicle (V2V) communication systems highlighting as a key component of enhanced road safety and traffic efficiency. This paper introduces a novel Real-time Vehicle-to-Vehicle Communication Based Network Cooperative Control System (VVCCS), designed to revolutionize macro-scope traffic planning and collision avoidance in autonomous driving. Implemented on Quanser Car (Qcar) hardware platform, our system integrates the distributed databases into individual autonomous vehicles and an optional central server. We also developed a comprehensive multi-modal perception system with multi-objective tracking and radar sensing. Through a demonstration within a physical crossroad environment, our system showcases its potential to be applied in congested and complex urban environments.</details> | http://arxiv.org/abs/2410.17576v1 |
| Multimodal Information Bottleneck for Deep Reinforcement Learning with   Multiple Sensors | Bang You; Huaping Liu | 2024-10-23 | <details><summary>Click to expand</summary>Reinforcement learning has achieved promising results on robotic control tasks but struggles to leverage information effectively from multiple sensory modalities that differ in many characteristics. Recent works construct auxiliary losses based on reconstruction or mutual information to extract joint representations from multiple sensory inputs to improve the sample efficiency and performance of reinforcement learning algorithms. However, the representations learned by these methods could capture information irrelevant to learning a policy and may degrade the performance. We argue that compressing information in the learned joint representations about raw multimodal observations is helpful, and propose a multimodal information bottleneck model to learn task-relevant joint representations from egocentric images and proprioception. Our model compresses and retains the predictive information in multimodal observations for learning a compressed joint representation, which fuses complementary information from visual and proprioceptive feedback and meanwhile filters out task-irrelevant information in raw multimodal observations. We propose to minimize the upper bound of our multimodal information bottleneck objective for computationally tractable optimization. Experimental evaluations on several challenging locomotion tasks with egocentric images and proprioception show that our method achieves better sample efficiency and zero-shot robustness to unseen white noise than leading baselines. We also empirically demonstrate that leveraging information from egocentric images and proprioception is more helpful for learning policies on locomotion tasks than solely using one single modality.</details> | http://arxiv.org/abs/2410.17551v1 |
| Generalizable Motion Planning via Operator Learning | Sharath Matada; Luke Bhan; Yuanyuan Shi; Nikolay Atanasov | 2024-10-23 | <details><summary>Click to expand</summary>In this work, we introduce a planning neural operator (PNO) for predicting the value function of a motion planning problem. We recast value function approximation as learning a single operator from the cost function space to the value function space, which is defined by an Eikonal partial differential equation (PDE). Specifically, we recast computing value functions as learning a single operator across continuous function spaces which prove is equivalent to solving an Eikonal PDE. Through this reformulation, our learned PNO is able to generalize to new motion planning problems without retraining. Therefore, our PNO model, despite being trained with a finite number of samples at coarse resolution, inherits the zero-shot super-resolution property of neural operators. We demonstrate accurate value function approximation at 16 times the training resolution on the MovingAI lab's 2D city dataset and compare with state-of-the-art neural value function predictors on 3D scenes from the iGibson building dataset. Lastly, we investigate employing the value function output of PNO as a heuristic function to accelerate motion planning. We show theoretically that the PNO heuristic is $\epsilon$-consistent by introducing an inductive bias layer that guarantees our value functions satisfy the triangle inequality. With our heuristic, we achieve a 30% decrease in nodes visited while obtaining near optimal path lengths on the MovingAI lab 2D city dataset, compared to classical planning methods (A*, RRT*).</details> | http://arxiv.org/abs/2410.17547v1 |
| Mechanisms and Computational Design of Multi-Modal End-Effector with   Force Sensing using Gated Networks | Yusuke Tanaka; Alvin Zhu; Richard Lin; Ankur Mehta; Dennis Hong | 2024-10-23 | <details><summary>Click to expand</summary>In limbed robotics, end-effectors must serve dual functions, such as both feet for locomotion and grippers for grasping, which presents design challenges. This paper introduces a multi-modal end-effector capable of transitioning between flat and line foot configurations while providing grasping capabilities. MAGPIE integrates 8-axis force sensing using proposed mechanisms with hall effect sensors, enabling both contact and tactile force measurements. We present a computational design framework for our sensing mechanism that accounts for noise and interference, allowing for desired sensitivity and force ranges and generating ideal inverse models. The hardware implementation of MAGPIE is validated through experiments, demonstrating its capability as a foot and verifying the performance of the sensing mechanisms, ideal models, and gated network-based models.</details> | http://arxiv.org/abs/2410.17524v2 |
| X-MOBILITY: End-To-End Generalizable Navigation via World Modeling | Wei Liu; Huihua Zhao; Chenran Li; Joydeep Biswas; Billy Okal; Pulkit Goyal; Yan Chang; Soha Pouya | 2024-10-23 | <details><summary>Click to expand</summary>General-purpose navigation in challenging environments remains a significant problem in robotics, with current state-of-the-art approaches facing myriad limitations. Classical approaches struggle with cluttered settings and require extensive tuning, while learning-based methods face difficulties generalizing to out-of-distribution environments. This paper introduces X-Mobility, an end-to-end generalizable navigation model that overcomes existing challenges by leveraging three key ideas. First, X-Mobility employs an auto-regressive world modeling architecture with a latent state space to capture world dynamics. Second, a diverse set of multi-head decoders enables the model to learn a rich state representation that correlates strongly with effective navigation skills. Third, by decoupling world modeling from action policy, our architecture can train effectively on a variety of data sources, both with and without expert policies: off-policy data allows the model to learn world dynamics, while on-policy data with supervisory control enables optimal action policy learning. Through extensive experiments, we demonstrate that X-Mobility not only generalizes effectively but also surpasses current state-of-the-art navigation approaches. Additionally, X-Mobility also achieves zero-shot Sim2Real transferability and shows strong potential for cross-embodiment generalization.</details> | http://arxiv.org/abs/2410.17491v1 |
| GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion   Policy | Yixuan Wang; Guang Yin; Binghao Huang; Tarik Kelestemur; Jiuguang Wang; Yunzhu Li | 2024-10-23 | <details><summary>Click to expand</summary>Diffusion-based policies have shown remarkable capability in executing complex robotic manipulation tasks but lack explicit characterization of geometry and semantics, which often limits their ability to generalize to unseen objects and layouts. To enhance the generalization capabilities of Diffusion Policy, we introduce a novel framework that incorporates explicit spatial and semantic information via 3D semantic fields. We generate 3D descriptor fields from multi-view RGBD observations with large foundational vision models, then compare these descriptor fields against reference descriptors to obtain semantic fields. The proposed method explicitly considers geometry and semantics, enabling strong generalization capabilities in tasks requiring category-level generalization, resolving geometric ambiguities, and attention to subtle geometric details. We evaluate our method across eight tasks involving articulated objects and instances with varying shapes and textures from multiple object categories. Our method demonstrates its effectiveness by increasing Diffusion Policy's average success rate on unseen instances from 20% to 93%. Additionally, we provide a detailed analysis and visualization to interpret the sources of performance gain and explain how our method can generalize to novel instances.</details> | http://arxiv.org/abs/2410.17488v1 |
| Composing Diffusion Policies for Few-shot Learning of Movement   Trajectories | Omkar Patil; Anant Sah; Nakul Gopalan | 2024-10-22 | <details><summary>Click to expand</summary>Humans can perform various combinations of physical skills without having to relearn skills from scratch every single time. For example, we can swing a bat when walking without having to re-learn such a policy from scratch by composing the individual skills of walking and bat swinging. Enabling robots to combine or compose skills is essential so they can learn novel skills and tasks faster with fewer real world samples. To this end, we propose a novel compositional approach called DSE- Diffusion Score Equilibrium that enables few-shot learning for novel skills by utilizing a combination of base policy priors. Our method is based on probabilistically composing diffusion policies to better model the few-shot demonstration data-distribution than any individual policy. Our goal here is to learn robot motions few-shot and not necessarily goal oriented trajectories. Unfortunately we lack a general purpose metric to evaluate the error between a skill or motion and the provided demonstrations. Hence, we propose a probabilistic measure - Maximum Mean Discrepancy on the Forward Kinematics Kernel (MMD-FK), that is task and action space agnostic. By using our few-shot learning approach DSE, we show that we are able to achieve a reduction of over 30% in MMD-FK across skills and number of demonstrations. Moreover, we show the utility of our approach through real world experiments by teaching novel trajectories to a robot in 5 demonstrations.</details> | http://arxiv.org/abs/2410.17479v1 |
| Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of   Architectural Inductive Biases on Hallucination | Jerry Huang; Prasanna Parthasarathi; Mehdi Rezagholizadeh; Boxing Chen; Sarath Chandar | 2024-10-22 | <details><summary>Click to expand</summary>The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to \textit{hallucinate} false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.</details> | http://arxiv.org/abs/2410.17477v2 |
| Configuração e operação da plataforma Clearpath Husky A200 e   manipulador Cobot UR5 2-finger gripper | Sodre Hiago; Barcelona Sebastian; Sandin Vincent; Moraes Pablo; Peters Christopher; da Silva Angél; Flores Gabriela; Mazondo Ahilen; Fernández Santiago; Assunção Nathalie; de Vargas Bruna; Grando Ricardo; Kelbouscas André | 2024-10-22 | <details><summary>Click to expand</summary>This article presents initial configuration work and use of the robotic platform and manipulator in question. The development of the ideal configuration for using this robot serves as a guide for new users and also validates its functionality for use in projects. Husky is a large payload capacity and power systems robotics development platform that accommodates a wide variety of payloads, customized to meet research needs. Together with the Cobot UR5 Manipulator attached to its base, it expands the application area of its capacity in projects. Advances in robots and mobile manipulators have revolutionized industries by automating tasks that previously required human intervention. These innovations alone increase productivity but also reduce operating costs, which makes the company more competitive in an evolving global market. Therefore, this article investigates the functionalities of this robot to validate its execution in robotics projects.</details> | http://arxiv.org/abs/2410.17453v3 |
| Interação entre robôs humanoides: desenvolvendo a   colaboração e comunicação autônoma | Moraes Pablo; Rodríguez Mónica; Peters Christopher; Sodre Hiago; Mazondo Ahilen; Sandin Vincent; Barcelona Sebastian; Moraes William; Fernández Santiago; Assunção Nathalie; de Vargas Bruna; Dörnbach Tobias; Kelbouscas André; Grando Ricardo | 2024-10-22 | <details><summary>Click to expand</summary>This study investigates the interaction between humanoid robots NAO and Pepper, emphasizing their potential applications in educational settings. NAO, widely used in education, and Pepper, designed for social interactions, of er new opportunities for autonomous communication and collaboration. Through a series of programmed interactions, the robots demonstrated their ability to communicate and coordinate actions autonomously, highlighting their potential as tools for enhancing learning environments. The research also explores the integration of emerging technologies, such as artificial intelligence, into these systems, allowing robots to learn from each other and adapt their behavior. The findings suggest that NAO and Pepper can significantly contribute to both technical learning and the development of social and emotional skills in students, of ering innovative pedagogical approaches through the use of humanoid robotics.</details> | http://arxiv.org/abs/2410.17450v2 |
| Real-time experiment-theory closed-loop interaction for autonomous   materials science | Haotong Liang; Chuangye Wang; Heshan Yu; Dylan Kirsch; Rohit Pant; Austin McDannald; A. Gilad Kusne; Ji-Cheng Zhao; Ichiro Takeuchi | 2024-10-22 | <details><summary>Click to expand</summary>Iterative cycles of theoretical prediction and experimental validation are the cornerstone of the modern scientific method. However, the proverbial "closing of the loop" in experiment-theory cycles in practice are usually ad hoc, often inherently difficult, or impractical to repeat on a systematic basis, beset by the scale or the time constraint of computation or the phenomena under study. Here, we demonstrate Autonomous MAterials Search Engine (AMASE), where we enlist robot science to perform self-driving continuous cyclical interaction of experiments and computational predictions for materials exploration. In particular, we have applied the AMASE formalism to the rapid mapping of a temperature-composition phase diagram, a fundamental task for the search and discovery of new materials. Thermal processing and experimental determination of compositional phase boundaries in thin films are autonomously interspersed with real-time updating of the phase diagram prediction through the minimization of Gibbs free energies. AMASE was able to accurately determine the eutectic phase diagram of the Sn-Bi binary thin-film system on the fly from a self-guided campaign covering just a small fraction of the entire composition - temperature phase space, translating to a 6-fold reduction in the number of necessary experiments. This study demonstrates for the first time the possibility of real-time, autonomous, and iterative interactions of experiments and theory carried out without any human intervention.</details> | http://arxiv.org/abs/2410.17430v1 |
| AG-SLAM: Active Gaussian Splatting SLAM | Wen Jiang; Boshu Lei; Katrina Ashton; Kostas Daniilidis | 2024-10-22 | <details><summary>Click to expand</summary>We present AG-SLAM, the first active SLAM system utilizing 3D Gaussian Splatting (3DGS) for online scene reconstruction. In recent years, radiance field scene representations, including 3DGS have been widely used in SLAM and exploration, but actively planning trajectories for robotic exploration is still unvisited. In particular, many exploration methods assume precise localization and thus do not mitigate the significant risk of constructing a trajectory, which is difficult for a SLAM system to operate on. This can cause camera tracking failure and lead to failures in real-world robotic applications. Our method leverages Fisher Information to balance the dual objectives of maximizing the information gain for the environment while minimizing the cost of localization errors. Experiments conducted on the Gibson and Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the proposed method.</details> | http://arxiv.org/abs/2410.17422v1 |
| Geometric Graph Neural Network Modeling of Human Interactions in Crowded   Environments | Sara Honarvar; Yancy Diaz-Mercado | 2024-10-22 | <details><summary>Click to expand</summary>Modeling human trajectories in crowded environments is challenging due to the complex nature of pedestrian behavior and interactions. This paper proposes a geometric graph neural network (GNN) architecture that integrates domain knowledge from psychological studies to model pedestrian interactions and predict future trajectories. Unlike prior studies using complete graphs, we define interaction neighborhoods using pedestrians' field of view, motion direction, and distance-based kernel functions to construct graph representations of crowds. Evaluations across multiple datasets demonstrate improved prediction accuracy through reduced average and final displacement error metrics. Our findings underscore the importance of integrating domain knowledge with data-driven approaches for effective modeling of human interactions in crowds.</details> | http://arxiv.org/abs/2410.17409v1 |
| Learning Precise, Contact-Rich Manipulation through Uncalibrated Tactile   Skins | Venkatesh Pattabiraman; Yifeng Cao; Siddhant Haldar; Lerrel Pinto; Raunaq Bhirangi | 2024-10-22 | <details><summary>Click to expand</summary>While visuomotor policy learning has advanced robotic manipulation, precisely executing contact-rich tasks remains challenging due to the limitations of vision in reasoning about physical interactions. To address this, recent work has sought to integrate tactile sensing into policy learning. However, many existing approaches rely on optical tactile sensors that are either restricted to recognition tasks or require complex dimensionality reduction steps for policy learning. In this work, we explore learning policies with magnetic skin sensors, which are inherently low-dimensional, highly sensitive, and inexpensive to integrate with robotic platforms. To leverage these sensors effectively, we present the Visuo-Skin (ViSk) framework, a simple approach that uses a transformer-based policy and treats skin sensor data as additional tokens alongside visual information. Evaluated on four complex real-world tasks involving credit card swiping, plug insertion, USB insertion, and bookshelf retrieval, ViSk significantly outperforms both vision-only and optical tactile sensing based policies. Further analysis reveals that combining tactile and visual modalities enhances policy performance and spatial generalization, achieving an average improvement of 27.5% across tasks. https://visuoskin.github.io/</details> | http://arxiv.org/abs/2410.17246v2 |
| Minimum-Violation Temporal Logic Planning for Heterogeneous Robots under   Robot Skill Failures | Samarth Kalluraya; Beichen Zhou; Yiannis Kantaros | 2024-10-22 | <details><summary>Click to expand</summary>In this paper, we consider teams of robots with heterogeneous skills (e.g., sensing and manipulation) tasked with collaborative missions described by Linear Temporal Logic (LTL) formulas. These LTL-encoded tasks require robots to apply their skills to specific regions and objects in a temporal and logical order. While existing temporal logic planning algorithms can synthesize correct-by-construction paths, they typically lack reactivity to unexpected failures of robot skills, which can compromise mission performance. This paper addresses this challenge by proposing a reactive LTL planning algorithm that adapts to unexpected failures during deployment. Specifically, the proposed algorithm reassigns sub-tasks to robots based on their functioning skills and locally revises team plans to accommodate these new assignments and ensure mission completion. The main novelty of the proposed algorithm is its ability to handle cases where mission completion becomes impossible due to limited functioning robots. Instead of reporting mission failure, the algorithm strategically prioritizes the most crucial sub-tasks and locally revises the team's plans, as per user-specified priorities, to minimize mission violations. We provide theoretical conditions under which the proposed framework computes the minimum violation task reassignments and team plans. We provide numerical and hardware experiments to demonstrate the efficiency of the proposed method.</details> | http://arxiv.org/abs/2410.17188v1 |
| DyPNIPP: Predicting Environment Dynamics for RL-based Robust Informative   Path Planning | Srujan Deolasee; Siva Kailas; Wenhao Luo; Katia Sycara; Woojun Kim | 2024-10-22 | <details><summary>Click to expand</summary>Informative path planning (IPP) is an important planning paradigm for various real-world robotic applications such as environment monitoring. IPP involves planning a path that can learn an accurate belief of the quantity of interest, while adhering to planning constraints. Traditional IPP methods typically require high computation time during execution, giving rise to reinforcement learning (RL) based IPP methods. However, the existing RL-based methods do not consider spatio-temporal environments which involve their own challenges due to variations in environment characteristics. In this paper, we propose DyPNIPP, a robust RL-based IPP framework, designed to operate effectively across spatio-temporal environments with varying dynamics. To achieve this, DyPNIPP incorporates domain randomization to train the agent across diverse environments and introduces a dynamics prediction model to capture and adapt the agent actions to specific environment dynamics. Our extensive experiments in a wildfire environment demonstrate that DyPNIPP outperforms existing RL-based IPP algorithms by significantly improving robustness and performing across diverse environment conditions.</details> | http://arxiv.org/abs/2410.17186v1 |
| Risk-Averse Model Predictive Control for Racing in Adverse Conditions | Thomas Lew; Marcus Greiff; Franck Djeumou; Makoto Suminaka; Michael Thompson; John Subosits | 2024-10-22 | <details><summary>Click to expand</summary>Model predictive control (MPC) algorithms can be sensitive to model mismatch when used in challenging nonlinear control tasks. In particular, the performance of MPC for vehicle control at the limits of handling suffers when the underlying model overestimates the vehicle's capabilities. In this work, we propose a risk-averse MPC framework that explicitly accounts for uncertainty over friction limits and tire parameters. Our approach leverages a sample-based approximation of an optimal control problem with a conditional value at risk (CVaR) constraint. This sample-based formulation enables planning with a set of expressive vehicle dynamics models using different tire parameters. Moreover, this formulation enables efficient numerical resolution via sequential quadratic programming and GPU parallelization. Experiments on a Lexus LC 500 show that risk-averse MPC unlocks reliable performance, while a deterministic baseline that plans using a single dynamics model may lose control of the vehicle in adverse road conditions.</details> | http://arxiv.org/abs/2410.17183v1 |
| Impact of 3D LiDAR Resolution in Graph-based SLAM Approaches: A   Comparative Study | J. Jorge; T. Barros; C. Premebida; M. Aleksandrov; D. Goehring; U. J. Nunes | 2024-10-22 | <details><summary>Click to expand</summary>Simultaneous Localization and Mapping (SLAM) is a key component of autonomous systems operating in environments that require a consistent map for reliable localization. SLAM has been a widely studied topic for decades with most of the solutions being camera or LiDAR based. Early LiDAR-based approaches primarily relied on 2D data, whereas more recent frameworks use 3D data. In this work, we survey recent 3D LiDAR-based Graph-SLAM methods in urban environments, aiming to compare their strengths, weaknesses, and limitations. Additionally, we evaluate their robustness regarding the LiDAR resolution namely 64 $vs$ 128 channels. Regarding SLAM methods, we evaluate SC-LeGO-LOAM, SC-LIO-SAM, Cartographer, and HDL-Graph on real-world urban environments using the KITTI odometry dataset (a LiDAR with 64-channels only) and a new dataset (AUTONOMOS-LABS). The latter dataset, collected using instrumented vehicles driving in Berlin suburban area, comprises both 64 and 128 LiDARs. The experimental results are reported in terms of quantitative `metrics' and complemented by qualitative maps.</details> | http://arxiv.org/abs/2410.17171v1 |
| Towards Map-Agnostic Policies for Adaptive Informative Path Planning | Julius Rückin; David Morilla-Cabello; Cyrill Stachniss; Eduardo Montijano; Marija Popović | 2024-10-22 | <details><summary>Click to expand</summary>Robots are frequently tasked to gather relevant sensor data in unknown terrains. A key challenge for classical path planning algorithms used for autonomous information gathering is adaptively replanning paths online as the terrain is explored given limited onboard compute resources. Recently, learning-based approaches emerged that train planning policies offline and enable computationally efficient online replanning performing policy inference. These approaches are designed and trained for terrain monitoring missions assuming a single specific map representation, which limits their applicability to different terrains. To address these issues, we propose a novel formulation of the adaptive informative path planning problem unified across different map representations, enabling training and deploying planning policies in a larger variety of monitoring missions. Experimental results validate that our novel formulation easily integrates with classical non-learning-based planning approaches while maintaining their performance. Our trained planning policy performs similarly to state-of-the-art map-specifically trained policies. We validate our learned policy on unseen real-world terrain datasets.</details> | http://arxiv.org/abs/2410.17166v1 |
| Layered LA-MAPF: a decomposition of large agent MAPF instance to   accelerate solving without compromising solvability | Zhuo Yao | 2024-10-22 | <details><summary>Click to expand</summary>Multi-Agent Path Finding (MAPF) has been widely studied in recent years. However, most existing MAPF algorithms assume that an agent occupies only a single grid in a grid-based map. This assumption limits their applicability in many real-world domains where agents have geometric shapes, rather than being point-like. Such agents, which can occupy multiple cells simultaneously, are referred to as ``large'' agents. When considering the shape and size of agents in MAPF, the computational complexity increases significantly as the number of agents grows, primarily due to the increased overhead in conflict detection between geometric agents. In this paper, we propose two types of subproblems for the LA-MAPF (Large-Agent MAPF) problem: \textbf{cluster} (which has no constraints on the order of solution) and \textbf{level} (which imposes constraints on the solution order). We introduce \textbf{Layered LA-MAPF}, a method that decomposes a MAPF instance involving geometric agents into clusters, and then further decomposes each cluster into levels. This approach aims to reduce time complexity when solving LA-MAPF problems. Our results demonstrate the performance of our method as the number of agents increases across various maps, and how it accelerates LA-MAPF methods, such as LA-CBS and LA-LaCAM. Experiments show that our LA-MAPF method with instance decomposition \textbf{halves the time cost (reducing from an average of 40s to 20s) and triples the success rate (from an average of 0.27 to 0.80)} in finding a solution within 60 seconds. To facilitate further research, we have made the source code for Layered LA-MAPF publicly available at \url{https://github.com/JoeYao-bit/LayeredMAPF/algorithm/LA-MAPF}.</details> | http://arxiv.org/abs/2410.17160v1 |
| Advancing lunar exploration through virtual reality simulations: a   framework for future human missions | Giacomo Franchini; Brenno Tuberga; Marcello Chiaberge | 2024-10-22 | <details><summary>Click to expand</summary>In an era marked by renewed interest in lunar exploration and the prospect of establishing a sustainable human presence on the Moon, innovative approaches supporting mission preparation and astronaut training are imperative. To this end, the advancements in Virtual Reality (VR) technology offer a promising avenue to simulate and optimize future human missions to the Moon. Through VR simulations, tests can be performed quickly, with different environment parameters and a human-centered perspective can be maintained throughout the experiments. This paper presents a comprehensive framework that harnesses VR simulations to replicate the challenges and opportunities of lunar exploration, aiming to enhance astronaut readiness and mission success. Multiple environments with physical and visual characteristics that reflect those found in interesting Moon regions have been modeled and integrated into simulations based on the Unity graphical engine. We exploit VR to allow the user to fully immerse in the simulations and interact with assets in the same way as in real contexts. Different scenarios have been replicated, from upcoming exploration missions where it is possible to deploy scientific payloads, collect samples, and traverse the surrounding environment, to long-term habitation in a futuristic lunar base, performing everyday activities. Moreover, our framework allows us to simulate human-robot collaboration and surveillance directly displaying sensor readings and scheduled tasks of autonomous agents which will be part of future hybrid missions, leveraging the ROS2-Unity bridge. Thus, the entire project can be summarized as a desire to define cornerstones for human-machine design and interaction, astronaut training, and learning of potential weak points in the context of future lunar missions, through targeted operations in a variety of contexts as close to reality as possible.</details> | http://arxiv.org/abs/2410.17132v1 |
| Miniature magneto-oscillatory wireless sensor for magnetic field and   gradient measurements | Felix Fischer; Moonkwang Jeong; Tian Qiu | 2024-10-22 | <details><summary>Click to expand</summary>Magneto-oscillatory devices have been recently developed as very potent wireless miniature position trackers and sensors with an exceptional accuracy and sensing distance for surgical and robotic applications. However, it is still unclear to which extend a mechanically resonating sub-millimeter magnet interacts with external magnetic fields or gradients, which induce frequency shifts of sub-mHz to several Hz and therefore affect the sensing accuracy. Here, we investigate this effect experimentally on a cantilever-based magneto-oscillatory wireless sensor (MOWS) and build an analytical model concerning magnetic and mechanical interactions. The millimeter-scale MOWS is capable to detect magnetic fields with sub-uT resolution to at least +/- 5 mT, and simultaneously detects magnetic field gradients with a resolution of 65 uT/m to at least +/- 50 mT/m. The magnetic field sensitivity allows direct calculation of mechanical device properties, and by rotation, individual contributions of the magnetic field and gradient can be analyzed. The derived model is general and can be applied to other magneto-oscillatory systems interacting with magnetic environments.</details> | http://arxiv.org/abs/2410.17062v1 |
| Optimal gait design for nonlinear soft robotic crawlers | Yenan Shen; Naomi Ehrich Leonard; Bassam Bamieh; Juncal Arbelaiz | 2024-10-22 | <details><summary>Click to expand</summary>Soft robots offer a frontier in robotics with enormous potential for safe human-robot interaction and agility in uncertain environments. A steppingstone towards unlocking the potential of soft robotics is a tailored control theory, including a principled framework for gait design. We analyze the problem of optimal gait design for a soft crawling body, "the crawler". The crawler is an elastic body with the control signal defined as actuation forces between segments of the body. We consider the simplest such crawler: a two-segmented body with a passive mechanical connection modeling the viscoelastic body dynamics and a symmetric control force modeling actuation between the two body segments. The model accounts for the nonlinear asymmetric friction with the ground, which together with the symmetric actuation forces enable the crawler's locomotion. Using a describing-function analysis, we show that when the body is forced sinusoidally, the optimal actuator contraction frequency corresponds to the body's natural frequency when operating with only passive dynamics. We then use the framework of Optimal Periodic Control (OPC) to design optimal force cycles of arbitrary waveform and the corresponding crawling gaits. We provide a hill-climbing algorithm to solve the OPC problem numerically. Our proposed methods and results inform the design of optimal forcing and gaits for more complex and multi-segmented crawling bodies.</details> | http://arxiv.org/abs/2410.17058v1 |
| Magneto-oscillatory localization for small-scale robots | Felix Fischer; Christian Gletter; Moonkwang Jeong; Tian Qiu | 2024-10-22 | <details><summary>Click to expand</summary>Magnetism is widely used for the wireless localization and actuation of robots and devices for medical procedures. However, current static magnetic localization methods suffer from large required magnets and are limited to only five degrees of freedom due to a fundamental constraint of the rotational symmetry around the magnetic axis. We present the small-scale magneto-oscillatory localization (SMOL) method, which is capable of wirelessly localizing a millimeter-scale tracker with full six degrees of freedom in deep biological tissues. The SMOL device uses the temporal oscillation of a mechanically resonant cantilever with a magnetic dipole to break the rotational symmetry, and exploits the frequency-response to achieve a high signal-to-noise ratio with sub-millimeter accuracy over a large distance of up to 12 centimeters and quasi-continuous refresh rates up to 200 Hz. Integration into real-time closed-loop controlled robots and minimally-invasive surgical tools are demonstrated to reveal the vast potential of the SMOL method.</details> | http://arxiv.org/abs/2410.17015v1 |
